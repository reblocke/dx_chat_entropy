{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The % symbol denotes a magic command in Jupyter. It ensures that pip is run in the same Python environment as the Jupyter kernel. This prevents issues that might arise if the pip installed packages into a different environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /opt/anaconda3/lib/python3.12/site-packages (3.3.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (4.49.0.dev0)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (1.6.0)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (1.14.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (0.27.0)\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: srsly in /opt/anaconda3/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /opt/anaconda3/lib/python3.12/site-packages (from srsly) (2.0.10)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers.git\n",
      "  Cloning https://github.com/huggingface/transformers.git to /private/var/folders/kc/jkd3yt1s0436pgb396mzmmn00000gn/T/pip-req-build-eqvuq34w\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /private/var/folders/kc/jkd3yt1s0436pgb396mzmmn00000gn/T/pip-req-build-eqvuq34w\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit 5fa35344755d8d9c29610b57d175efd03776ae9e\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers==4.49.0.dev0) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers==4.49.0.dev0) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers==4.49.0.dev0) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers==4.49.0.dev0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers==4.49.0.dev0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers==4.49.0.dev0) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers==4.49.0.dev0) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.12/site-packages (from transformers==4.49.0.dev0) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers==4.49.0.dev0) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers==4.49.0.dev0) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.49.0.dev0) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.49.0.dev0) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers==4.49.0.dev0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers==4.49.0.dev0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers==4.49.0.dev0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers==4.49.0.dev0) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U sentence-transformers\n",
    "%pip install srsly\n",
    "#%pip install triton - dont' need with mac m2\n",
    "%pip install git+https://github.com/huggingface/transformers.git # install new huggingface transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/metal.html\n",
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (2.5.1)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement torchvision (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for torchvision\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/metal.html\n",
    "\n",
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(f\"Using device: {device}\")\n",
    "else:\n",
    "    print(\"MPS backend not available. Falling back to CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 768)\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Download from the ðŸ¤— Hub\n",
    "model = SentenceTransformer(\"joe32140/ModernBERT-base-msmarco\")\n",
    "# Note - for my medical project, seems like maybe using a different model would be better\n",
    "# also may need to retinker the sentence transformer part\n",
    "\n",
    "# Run inference\n",
    "sentences = [\n",
    "    'what county is hayden in',\n",
    "    \"Hayden is a city in Kootenai County, Idaho, United States. Located in the northern portion of the state, just north of Coeur d'Alene, its population was 13,294 at the 2010 census.\",\n",
    "    \"According to the United States Census Bureau, the city has a total area of 9.61 square miles (24.89 km2), of which 9.60 square miles (24.86 km2) is land and 0.01 square miles (0.03 km2) is water. It lies at the southwestern end of Hayden Lake, and the elevation of the city is 2,287 feet (697 m) above sea level. Hayden is located on U.S. Route 95 at the junction of Route 41. It is also four miles (6 km) north of Interstate 90 and Coeur d'Alene. The Coeur d'Alene airport is northwest of Hayden.\",\n",
    "]\n",
    "\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings.shape)\n",
    "# [3, 768]\n",
    "\n",
    "# Get the similarity scores for the embeddings\n",
    "similarities = model.similarity(embeddings, embeddings)\n",
    "print(similarities.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "825098826bd9452581a85ac6a66bf3ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ad031ec7dd46abab14a8b87083e67f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/870M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ffb9a869ce8456fa7c2ee3a2069e142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.26k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04589695dd214ff982966103e5f6e1d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b099898423b34f0fa4119bc9c9b4a26e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/8.66M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0007c5c3ae954449bae0ffbacab72876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/23.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "646c376f968f41059a064d7131cd855e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/970 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "#!pip install transformers[sentencepiece]\n",
    "from transformers import pipeline\n",
    "\n",
    "text = \"Angela Merkel is a politician in Germany and leader of the CDU\"\n",
    "hypothesis_template = \"This text is about {}\" # Based on this interview, does the patient have {} sign or symptom\n",
    "classes_verbalized = [\"politics\", \"economy\", \"entertainment\", \"environment\"]\n",
    "\n",
    "zeroshot_classifier = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/deberta-v3-large-zeroshot-v2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 272 ms, sys: 135 ms, total: 406 ms\n",
      "Wall time: 2.69 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'Angela Merkel is a politician in Germany and leader of the CDU',\n",
       " 'labels': ['politics', 'economy', 'environment', 'entertainment'],\n",
       " 'scores': [0.9994651079177856,\n",
       "  0.0002065974404104054,\n",
       "  0.0001747352071106434,\n",
       "  0.0001535148185212165]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "zeroshot_classifier(text, classes_verbalized, hypothesis_template=hypothesis_template, multi_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/koaning/arxiv-frontpage/refs/heads/main/data/annot/new-dataset.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import srsly\n",
    "inputs = list(srsly.read_jsonl(\"new-dataset.jsonl\"))\n",
    "texts = [d[\"text\"] for d in inputs]\n",
    "labels = [d[\"cats\"][\"new-dataset\"] for d in inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 22s, sys: 13.6 s, total: 1min 36s\n",
      "Wall time: 2min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "outputs = zeroshot_classifier(texts, [\"new dataset\"], hypothesis_template=hypothesis_template, multi_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.8990536277602523)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "pred_zs = np.array([o[\"scores\"] for o in outputs]).reshape(-1) > 0.5\n",
    "np.mean(pred_zs == labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87b5e079b4f84501b1c2e239b7cf5355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de8e9c8a9619497289130559dce1eb75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/792M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe9334bbf6dd46c68185c284b39f3052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/20.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5152f77f1c0d4edbb6f6f94109a56134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/3.58M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0799871acb0244b6948f4186a61209e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/694 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "zeroshot_classifier = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/ModernBERT-large-zeroshot-v2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 273 ms, sys: 85.5 ms, total: 359 ms\n",
      "Wall time: 1.41 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'Angela Merkel is a politician in Germany and leader of the CDU',\n",
       " 'labels': ['politics', 'economy', 'environment', 'entertainment'],\n",
       " 'scores': [0.9994553327560425,\n",
       "  0.0004048754053656012,\n",
       "  0.00010866236698348075,\n",
       "  3.11371150019113e-05]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "zeroshot_classifier(text, classes_verbalized, hypothesis_template=hypothesis_template, multi_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 28s, sys: 14.1 s, total: 1min 42s\n",
      "Wall time: 1min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "outputs = zeroshot_classifier(texts, [\"new dataset\"], hypothesis_template=hypothesis_template, multi_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.8940964398377648)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_zs = np.array([o[\"scores\"] for o in outputs]).reshape(-1) > 0.5\n",
    "np.mean(pred_zs == labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0965fba46bc24bb686ffef57ed805625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec93a525e3a541418731abf3d405dcfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/299M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "197e830f6ee54b5aa3526c4830523b70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/20.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db4593bb976946cfa54fdebe45889f62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/3.58M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "304fe45c41fb446ea2210c6655fc0896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/694 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "zeroshot_classifier = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/ModernBERT-base-zeroshot-v2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 13s, sys: 10.7 s, total: 1min 23s\n",
      "Wall time: 1min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "outputs = zeroshot_classifier(texts, [\"new dataset\"], hypothesis_template=hypothesis_template, multi_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9062640829202343)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_zs = np.array([o[\"scores\"] for o in outputs]).reshape(-1) > 0.5\n",
    "np.mean(pred_zs == labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST CODE FOR FEATURE EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "SEED_GLOBAL = 42\n",
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"  # force UTF-8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /opt/anaconda3/lib/python3.12/site-packages (3.3.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (4.49.0.dev0)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (1.6.0)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (1.14.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (0.27.0)\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: srsly in /opt/anaconda3/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /opt/anaconda3/lib/python3.12/site-packages (from srsly) (2.0.10)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting git+https://github.com/huggingface/transformers.git\n",
      "  Cloning https://github.com/huggingface/transformers.git to /private/var/folders/kc/jkd3yt1s0436pgb396mzmmn00000gn/T/pip-req-build-hdvr4os6\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /private/var/folders/kc/jkd3yt1s0436pgb396mzmmn00000gn/T/pip-req-build-hdvr4os6\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit 234168c4dc837da10c594408f15807a1d6f3fab4\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers==4.49.0.dev0) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers==4.49.0.dev0) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers==4.49.0.dev0) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers==4.49.0.dev0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers==4.49.0.dev0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers==4.49.0.dev0) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers==4.49.0.dev0) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.12/site-packages (from transformers==4.49.0.dev0) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers==4.49.0.dev0) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers==4.49.0.dev0) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.49.0.dev0) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.49.0.dev0) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers==4.49.0.dev0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers==4.49.0.dev0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers==4.49.0.dev0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers==4.49.0.dev0) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://download.pytorch.org/whl/metal.html\n",
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.12/site-packages (0.20.1)\n",
      "Requirement already satisfied: torchaudio in /opt/anaconda3/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U sentence-transformers\n",
    "%pip install srsly\n",
    "%pip install git+https://github.com/huggingface/transformers.git # install new huggingface transformers\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/metal.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfrom datasets import load_dataset, concatenate_datasets\\nimport re\\nimport time\\n\\nimport tqdm\\nimport torch\\nfrom torch.utils.data import DataLoader\\n\\nimport transformers\\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\\nfrom transformers import TrainingArguments, Trainer\\nfrom datasets import ClassLabel\\nfrom datasets import load_dataset, load_metric, Dataset, DatasetDict, concatenate_datasets, list_metrics\\n\\nfrom sklearn.metrics import balanced_accuracy_score, precision_recall_fscore_support, accuracy_score, classification_report\\n\\nimport gc\\nfrom accelerate.utils import release_memory\\n\\nimport wandb\\nimport json\\nfrom datetime import datetime\\nimport argparse\\n\\nfrom mdutils import MdUtils\\n\\n# suppress unnecessary tokenizer warning https://github.com/huggingface/transformers/issues/14285\\ntransformers.logging.set_verbosity_error()\\n\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(f\"Using device: {device}\")\n",
    "else:\n",
    "    print(\"MPS backend not available. Falling back to CPU.\")\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "import srsly\n",
    "\n",
    "\n",
    "np.random.seed(SEED_GLOBAL)\n",
    "torch.manual_seed(SEED_GLOBAL)\n",
    "random.seed(SEED_GLOBAL)\n",
    "\n",
    "import llm\n",
    "\n",
    "\"\"\"\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import re\n",
    "import time\n",
    "\n",
    "import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import ClassLabel\n",
    "from datasets import load_dataset, load_metric, Dataset, DatasetDict, concatenate_datasets, list_metrics\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score, precision_recall_fscore_support, accuracy_score, classification_report\n",
    "\n",
    "import gc\n",
    "from accelerate.utils import release_memory\n",
    "\n",
    "import wandb\n",
    "import json\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "\n",
    "from mdutils import MdUtils\n",
    "\n",
    "# suppress unnecessary tokenizer warning https://github.com/huggingface/transformers/issues/14285\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio, llm\n",
    "\n",
    "async def run():\n",
    "    model = llm.get_async_model(\"llama3.2:latest\")\n",
    "    response = model.prompt(\"A short poem about tea\")\n",
    "    print(await response.text())\n",
    "\n",
    "asyncio.run(run())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/reblocke/.zshrc\n",
      "OPENAI_API_KEY loaded: None\n"
     ]
    }
   ],
   "source": [
    "# Get OpenAI key from BASH config\n",
    "\n",
    "import re\n",
    "\n",
    "# Path to your zsh configuration file\n",
    "zshrc_path = os.path.expanduser(\"~/.zshrc\")\n",
    "print(zshrc_path)\n",
    "\n",
    "# Read the zshrc file and extract the OPENAI_API_KEY\n",
    "with open(zshrc_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        # Look for the export line for OPENAI_API_KEY\n",
    "        match = re.match(r\"export OPENAI_API_KEY=['\\\"]?([^'\\\"]+)['\\\"]?\", line)\n",
    "        if match:\n",
    "            openai_api_key = match.group(1)\n",
    "            os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "            break\n",
    "\n",
    "# Verify that the key was successfully set\n",
    "print(\"OPENAI_API_KEY loaded:\", os.environ.get(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# verify torch backend\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")\n",
    "\n",
    "# [ ] TODO: troubleshoot - Compiling the model with `torch.compile` and using a `torch.mps` device is not supported. Falling back to non-compiled mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient Case\n",
      "PATIENT DOOR CHART and Learner Instructions\n",
      "- Patient Name: Bettie\n",
      "- Age: 48\n",
      "- Legal Sex: Female\n",
      "- Chief Complaint: Chest pain\n",
      "- Clinical Setting: Outpatient clinic\n",
      "Vital Signs:\n",
      "- Blood Pressure Reading: 130/80 mmHg\n",
      "- Pulse Rate: 75 bpm\n",
      "- Respiratory Rate: 16 breaths/min\n",
      "- Temperature (Celsius): 37.0\n",
      "- SpO2: 97%Learner Tasks:\n",
      "Learner Tasks\n",
      "1. Obtain an appropriately focused and detailed history based upon the chief complaint.\n",
      "2. Perform a pertinent physical examination based upon the chief complaint.\n",
      "3. Discuss your diagnostic impressions and next steps with the patient.\n",
      "4. Place appropriate orders for the patient.\n",
      "5. Review results with the patient and further next steps.\n",
      "6. Answer any questions the patient may have to the best of your ability.Patient Case\n",
      "Clinical Interview:\n",
      "Doctor: can you describe the chest pain?\n",
      "Patient: Chest pain. Tightness.\n",
      "Doctor: location?\n",
      "Patient: Under the middle of my chest.\n",
      "Doctor: have you had this before?\n",
      "Patient: Yes, for about 3 months.\n",
      "Doctor: does anything make it better or worse?\n",
      "Patient: Worse with solid food and exercise. Better after regurgitation.\n",
      "Doctor: have you taken any medications for it?\n",
      "Patient: No, just my usual blood pressure medicine.\n",
      "Doctor: has it been getting worse over the past 3 months? \n",
      "Patient: Yes, the episodes last longer now.Patient Case\n",
      "Doctor: Any palpitations?\n",
      "Patient: No, I haven't noticed any.\n",
      "Doctor: any history of acid reflux?\n",
      "Patient: Yes, heartburn and reflux for over 10 years.\n",
      "Doctor: do you take anything for it? \n",
      "Patient: No, I haven't taken anything specific for it.\n",
      "Doctor: does this feel different than the heartburn? \n",
      "Patient: Yes, it's tighter and lasts longer.\n",
      "Doctor: any radiation of the pain? \n",
      "Patient: No, it doesn't spread anywhere else.\n",
      "Doctor: any fever, chills, cough, runny nose, or sick contacts? \n",
      "Patient: No, none of those.\n",
      "Doctor: any recent change in activity or injuries? Patient Case\n",
      "Patient: I've been less active recently due to stress. No injuries.\n",
      "Doctor: what has been causing stress? \n",
      "Patient: Work stress and recently out of a verbally abusive relationship.\n",
      "Doctor: do you havea. history of anxiety?\n",
      "Patient: Not officially, but I do feel anxious about my health.\n",
      "Doctor: can I get an EKG? \n",
      "Patient: Sure, if you think it's necessary.\n",
      "Doctor: do you feel light headed while exercising? \n",
      "Patient: No, I haven't felt light-headed.\n",
      "Doctor: what medications do you take? \n",
      "Patient: I take Amlodipine 5mg once daily for blood pressure.\n",
      "Doctor: any personal or family history of heart disease?\n",
      "Patient: My father had hypertension and died from a heart attack in his 50s.Patient Case\n",
      "Doctor: is the chest pain that worst right now that it has ever been? \n",
      "Patient: No, it's not the worst it's ever been.\n",
      "Doctor: is it worse at certain times of the day or when you lay down after a meal? \n",
      "Patient: Yes, I've noticed it after large meals or when I lie down.\n",
      "Doctor: It seems likely that you're having worsening of your heartburn but there also might be\n",
      "another problem with your esophagus that I think we should investigate further\n",
      "Patient: Okay, I understand. What should I do next?\n",
      "Doctor: I'd like to refer you to a specialist in throat disorders who can discuss doing further testing\n",
      "and perhaps doing a procedure where they look down into the throat \n",
      "Patient: Alright, I can do that. Thank you for helping me figure this out.\n",
      "Doctor: in the meantime, I think we should get you on stronger heartburn meds \n",
      "Patient: That sounds good. I'll try whatever you recommend.Patient Case\n",
      "Orders:\n",
      "examine heart and lungs, EKG\n",
      "Date and Time of Request: 2024-12-12 15:24:12\n",
      "examine chest wall for tenderness\n",
      "Date and Time of Request: 2024-12-12 15:23:55\n",
      "obtain vital signs\n",
      "Date and Time of Request: 2024-12-12 15:23:33\n",
      "Results:\n",
      "Results for New Orders:\n",
      "Physical Examination Findings:\n",
      "- Heart and Lungs Examination:\n",
      "Heart: Regular rate and rhythm with no murmurs, gallops, or rubs.\n",
      "Lungs: Clear to auscultation bilaterally with no wheezes, crackles, or rhonchi.\n",
      "- Heart: Regular rate and rhythm with no murmurs, gallops, or rubs.\n",
      "- Lungs: Clear to auscultation bilaterally with no wheezes, crackles, or rhonchi.Patient Case\n",
      "- Date and Time of Examination: 2024-12-12 15:34:12\n",
      "Heart and Lungs Examination:\n",
      "- Heart: Regular rate and rhythm with no murmurs, gallops, or rubs.\n",
      "- Lungs: Clear to auscultation bilaterally with no wheezes, crackles, or rhonchi.\n",
      "Date and Time of Examination: 2024-12-12 15:34:12\n",
      "EKG Results:\n",
      "These results reflect the outcome of the specified examination and test orders given the clinical\n",
      "scenario described.\n",
      "Physical Exam Results:\n",
      "Chest Wall Examination Findings:\n",
      "- Tenderness localized over the sternum and anterior chest wall. No swelling or visible abnormalities\n",
      "noted during palpation. \n",
      "Vital Signs:\n"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "# Path to your PDF file\n",
    "pdf_path = '/Users/reblocke/Research/dx_chat_entropy/Chatbot Transcripts/Intermtn MS4 1 Transcript.pdf'\n",
    "\n",
    "# Initialize PDF reader\n",
    "reader = PdfReader(pdf_path)\n",
    "\n",
    "# Extract text from each page\n",
    "all_text = \"\"\n",
    "for page in reader.pages:\n",
    "    all_text += page.extract_text()\n",
    "\n",
    "# Output the text\n",
    "print(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "md = MarkItDown(llm_client=client, llm_model=\"gpt-4o-2024-11-20\")\n",
    "supported_extensions = ('.pptx', '.docx', '.pdf', '.jpg', '.jpeg', '.png')\n",
    "files_to_convert = [f for f in os.listdir('.') if f.lower().endswith(supported_extensions)]\n",
    "for file in files_to_convert:\n",
    "    print(f\"\\nConverting {file}...\")\n",
    "    try:\n",
    "        md_file = os.path.splitext(file)[0] + '.md'\n",
    "        result = md.convert(file)\n",
    "        with open(md_file, 'w') as f:\n",
    "            f.write(result.text_content)\n",
    "        \n",
    "        print(f\"Successfully converted {file} to {md_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting {file}: {str(e)}\")\n",
    "\n",
    "print(\"\\nAll conversions completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis_template = \"This text is about {}\" # Based on this interview, does the patient have {} sign or symptom\n",
    "inputs = list(srsly.read_jsonl(\"new-dataset.jsonl\"))\n",
    "texts = [d[\"text\"] for d in inputs]\n",
    "labels = [d[\"cats\"][\"new-dataset\"] for d in inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry 1:\n",
      "Text: Our code and dataset is available here.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 2:\n",
      "Text: We will release the dataset and code to facilitate future endeavors.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 3:\n",
      "Text: We release our dataset for others to use and build on.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 4:\n",
      "Text: Our dataset is available online.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 5:\n",
      "Text: We release the generated dataset and used prompts to facilitate future research.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 6:\n",
      "Text: Code and dataset will be available.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 7:\n",
      "Text: We demonstrate the value of the created dataset by performing a quantitative and qualitative analysis on the models' results.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 8:\n",
      "Text: We train our model on a new synthetic image dataset, that we release.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 9:\n",
      "Text: The code and new synthetic dataset will be released for better reproducibility of our results.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 10:\n",
      "Text: From this point, we can note the importance of building a new structured dataset to solve the lack of structured data.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 11:\n",
      "Text: Previous datasets are created by either capturing real scenes by event cameras or synthesizing from images with pasted foreground objects.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 12:\n",
      "Text: These datasets included the latest second and third generation deepfake datasets.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 13:\n",
      "Text: To our knowledge, this is the first aligned dataset of its kind and is the largest dataset ever released in the heritage domain.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 14:\n",
      "Text: The code and dataset will be released publicly.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 15:\n",
      "Text: We have released the code and dataset used in the present approach to generate synthetic data.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 16:\n",
      "Text: Our dataset is publicly available and can be freely modified and re-distributed.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 17:\n",
      "Text: The community has recognized the critical role that training datasets play in this context and has developed various techniques to improve dataset curation to overcome this problem.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 18:\n",
      "Text: Our code and dataset will be released at https://github.com/SiyuanYan1/EPVT.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 19:\n",
      "Text: The 3RL dataset was created, which contains approximately 24K images and will be publicly available, to overcome previously available dataset problems.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 20:\n",
      "Text: We use a public dataset for model development.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 21:\n",
      "Text: The related datasets and the source code will be released in the future.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 22:\n",
      "Text: We also collect a new large-scale dataset to serve as the new benchmark for this task.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 23:\n",
      "Text: The complete dataset engineered for this study, referred to as the CIFAKE dataset, is made publicly available to the research community for future work.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 24:\n",
      "Text: The dataset with accompanying code can be downloaded from our website.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 25:\n",
      "Text: We propose new training, validation, and testing splits for the dataset that we make available online.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 26:\n",
      "Text: Our code and unique datasets are available on the project's website.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 27:\n",
      "Text: We make our data available.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 28:\n",
      "Text: To facilitate research in this field, we will share our dataset and code with the community.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 29:\n",
      "Text: The dataset, related codes and models will be publicly available at https://github.com/hitachinsk/THA.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 30:\n",
      "Text: We have developed a systematic method to synthesize such training datasets.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 31:\n",
      "Text: In addition to the dataset itself, we also present some basic analysis of its content, certain temporal features, and its network.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 32:\n",
      "Text: In fact, to date, there is no dataset that we are aware of that addresses this issue.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 33:\n",
      "Text: We are committed to open-sourcing our meticulously curated dataset, as well as a comprehensive toolkit designed to aid researchers in the extensive collection and preprocessing of their own datasets.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 34:\n",
      "Text: Third, we provide a dataset of scenario based on our data generated.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 35:\n",
      "Text: Our source code and dataset will be made publicly available.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 36:\n",
      "Text: To this end, we first collect a new dataset, CAMO-FS, for the benchmark.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 37:\n",
      "Text: In this paper, we propose a framework for enhancing the data quality of original datasets.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 38:\n",
      "Text: We release a demo of our tools at dataportraits.org and call on dataset and model creators to release Data Portraits as a complement to current documentation practices.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 39:\n",
      "Text: The source code and dataset will be public.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 40:\n",
      "Text: To the best of our knowledge, only two datasets are available, with one based on the other.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 41:\n",
      "Text: We validate our method on two widely used datasets.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 42:\n",
      "Text: We introduce the VISION Datasets, a diverse collection of 14 industrial inspection datasets, uniquely poised to meet these challenges.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 43:\n",
      "Text: The dataset is available at https://www.tu-ilmenau.de/neurob/data-sets-code/attach-dataset .\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 44:\n",
      "Text: The dataset and code are available at \\url{https://github.com/littleYaang/HQ-50K}.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 45:\n",
      "Text: The code to reproduce our results, as well as the model and dataset (via a research data use agreement), are available at our Github repo here: https://github.com/som-shahlab/ehrshot-benchmark\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 46:\n",
      "Text: Our SCB-dataset can be downloaded from: https://github.com/Whiffe/SCB-dataset\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 47:\n",
      "Text: The dataset is available for download at: https://ustc-flicar.github.io.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 48:\n",
      "Text: To access our dataset and code, visit our GitHub repository: \\url{https://github.com/styxsys0927/Med-MMHL}.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 49:\n",
      "Text: Code can be downloaded from https://github.com/Zhang-VISLab.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 50:\n",
      "Text: The data products and codes can be downloaded from this https://github.com/sriniraghunathan/cross_ilc_methods_paper.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 51:\n",
      "Text: To download the data please visit https://stanford-tml.github.io/circle_dataset/.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 52:\n",
      "Text: Our code is available at Github.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 53:\n",
      "Text: All code is available on GitHub.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 54:\n",
      "Text: With these new techniques, our proposed \\Ours{} achieves state-of-the-art results on FUNSD and XFUND datasets, outperforming the previous best-performing method by 7.2\\% and 13.2\\% in F1 score, respectively.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 55:\n",
      "Text: We make a python package with the code available to download at https://pypi.org/project/hypertab/\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 56:\n",
      "Text: The air pollution data was downloaded from an online database (UCL).\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 57:\n",
      "Text: The SA3 dataset and scripts (R/Python) to develop these indices have been made available on my GitHub account: https://github.com/lpinzari/homogeneity-location-index\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 58:\n",
      "Text: We make the code available at github.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 59:\n",
      "Text: All the source code is available on Github.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 60:\n",
      "Text: The code has been deposited on GitHub (\\url{https://github.com/hyguozz}).\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 61:\n",
      "Text: The code is on github at https://github.com/IDU-CVLab/COV19D_3rd\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 62:\n",
      "Text: Its features, e.g., no need to download and no installation, have made it popular rapidly.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 63:\n",
      "Text: We outperform all baselines and demonstrate that among the considered methods, ours is the only one that detects label errors of all four types efficiently.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 64:\n",
      "Text: By leveraging this diversity, the collected dataset and the collection system aim to achieve higher recognition accuracy.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 65:\n",
      "Text: The dataset is generated by GPT-3.5-turbo and comprises programs with varying levels of complexity.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 66:\n",
      "Text: Our proposed dataset can support two different computer vision tasks, i.e., semantic segmentation and object detection.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 67:\n",
      "Text: To foster further research on the digitization of statistical graphs, we will make the dataset, code, and models publicly available to the community.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 68:\n",
      "Text: This collection includes a subset of the large-scale instruction dataset known as FLAN, as well as various code-related datasets and conversational datasets derived from ChatGPT/GPT-4.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 69:\n",
      "Text: FLACUNA is publicly available at https://huggingface.co/declare-lab/flacuna-13b-v1.0.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 70:\n",
      "Text: First, we publish a new dataset, EHRSHOT, containing de-identified structured data from the electronic health records (EHRs) of 6,712 patients from Stanford Medicine.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 71:\n",
      "Text: In this paper, we define a unified setting termed as open-set semantic segmentation (O3S), which aims to learn seen and unseen semantics from both visual examples and textual names.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 72:\n",
      "Text: Experimental results, carried out on three sets of the Shape COSEG Dataset, on the human segmentation dataset proposed in Maron et al., 2017 and on the ShapeNet benchmark, show how the proposed approach yields state-of-the-art performance on semantic segmentation of 3D meshes.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 73:\n",
      "Text: Our code and data are available at https://github.com/sergiotasconmorales/locvqa.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 74:\n",
      "Text: The dataset comprises high-resolution RGB-D images and pixel-level annotations of the fruits.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 75:\n",
      "Text: TGB datasets, data loaders, example codes, evaluation setup, and leaderboards are publicly available at https://tgb.complexdatalab.com/ .\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 76:\n",
      "Text: Our code, models, instruction-sets and demo are released at https://github.com/mbzuai-oryx/Video-ChatGPT.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 77:\n",
      "Text: We extensively evaluate SeaLog on two public datasets and an industrial dataset.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 78:\n",
      "Text: Using the COVID-19 Open Research Dataset (CORD-19), we produced two datasets: (1) synCovid, which uses a combination of handwritten prompts and synthetic prompts generated using OpenAI, and (2) real abstracts, which contains abstract and title pairs.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 79:\n",
      "Text: Videos are available at: https://kristery.github.io/edt/\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 80:\n",
      "Text: This paper presents an end-to-end methodology for collecting datasets to recognize handwritten English alphabets by utilizing Inertial Measurement Units (IMUs) and leveraging the diversity present in the Indian writing style.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 81:\n",
      "Text: Videos can be found on the project website https://vis-www.cs.umass.edu/Co-LLM-Agents/.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 82:\n",
      "Text: In this paper, we study linear regression applied to data structured on a manifold.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 83:\n",
      "Text: We assume that the data manifold is smooth and is embedded in a Euclidean space, and our objective is to reveal the impact of the data manifold's extrinsic geometry on the regression.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 84:\n",
      "Text: This research can be extended and contributes to the field of pattern recognition and offers valuable insights for developing improved systems for handwriting recognition, particularly in diverse linguistic and cultural contexts.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 85:\n",
      "Text: To prove these theorems, we revisit William Thurston's results on the calisson tilability of a region $R$.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 86:\n",
      "Text: Large language models trained for safety and harmlessness remain susceptible to adversarial misuse, as evidenced by the prevalence of \"jailbreak\" attacks on early releases of ChatGPT that elicit undesired behavior.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 87:\n",
      "Text: We use these failure modes to guide jailbreak design and then evaluate state-of-the-art models, including OpenAI's GPT-4 and Anthropic's Claude v1.3, against both existing and newly designed attacks.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 88:\n",
      "Text: Given a triangular grid in a hexagon and some given edges of the grid, the problem is to find a calisson tiling such that no input edge is overlapped and calissons adjacent to an input edge have different orientations.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 89:\n",
      "Text: Our work opens up new possibilities for modeling very long sequences, e.g., treating a whole corpus or even the entire Internet as a sequence.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 90:\n",
      "Text: We extend the puzzle to regions $R$ that are not necessarily hexagonal.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 91:\n",
      "Text: By using a natural language generation model to abductively infer a premise given another premise and a conclusion, we can impute missing pieces of evidence needed for the conclusion to be true.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 92:\n",
      "Text: We sample multiple possible outputs for each step to achieve coverage of the search space, at the same time ensuring correctness by filtering low-quality generations with a round-trip validation procedure.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 93:\n",
      "Text: Results on a modified version of the EntailmentBank dataset and a new dataset called Everyday Norms: Why Not? show that abductive generation with validation can recover premises across in- and out-of-domain settings\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 94:\n",
      "Text: To address this issue, this paper presents a systematic and comprehensive study, quantitatively and qualitatively, on training such models.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 95:\n",
      "Text: We implement over 20 variants with controlled settings.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 96:\n",
      "Text: For training data, we investigate the impact of data and sampling strategies.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 97:\n",
      "Text: For benchmarks, we contribute the first, to our best knowledge, comprehensive evaluation set including both image and video tasks through crowd-sourcing.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 98:\n",
      "Text: Here, remote sensing can provide reliable estimates of plastic pollution by regularly monitoring and detecting marine debris in coastal areas.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 99:\n",
      "Text: Medium-resolution satellite data of coastal areas is readily available and can be leveraged to detect aggregations of marine debris containing plastic litter.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 100:\n",
      "Text: In this work, we present a detector for marine debris built on a deep segmentation model that outputs a probability for marine debris at the pixel level.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 101:\n",
      "Text: We train this detector with a combination of annotated datasets of marine debris and evaluate it on specifically selected test sites where it is highly probable that plastic pollution is present in the detected marine debris.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 102:\n",
      "Text: We demonstrate quantitatively and qualitatively that a deep learning model trained on this dataset issued from multiple sources outperforms existing detection models trained on previous datasets by a large margin.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 103:\n",
      "Text: We hope to accelerate advances in the large-scale automated detection of marine debris, which is a step towards quantifying and monitoring marine litter with remote sensing at global scales, and release the model weights and training source code under https://github.com/marccoru/marinedebrisdetector\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 104:\n",
      "Text: This paper proposes a framework called <projektor>, which predicts model performance and supports data selection decisions based on partial samples of prospective data sources.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 105:\n",
      "Text: In the first stage, we leverage the Optimal Transport distance to predict the model's performance for any data mixture ratio within the range of disclosed data sizes.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 106:\n",
      "Text: To facilitate the development of comprehensive scene understanding solutions using multiple camera views, a new dataset called Road Genome (OpenLane-V2) has been released.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 107:\n",
      "Text: We achieve new state-of-the-art performance on the large-scale Waymo Open Dataset.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 108:\n",
      "Text: Finally, we train a detector that generalizes to a wide range of part segmentation datasets while achieving better performance than dataset-specific training.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 109:\n",
      "Text: Finally, we release a large-scale synthetic dataset with 1.4M examples generated using TrueTeacher.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 110:\n",
      "Text: To fully unlock model capabilities for high-quality video generation, we curate a large-scale video dataset called HD-VG-130M. This dataset comprises 130 million text-video pairs from the open-domain, ensuring high-definition, widescreen and watermark-free characters.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 111:\n",
      "Text: Code and datasets are available in https://github.com/zjunlp/DeepKE/tree/main/example/llm.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 112:\n",
      "Text: We evaluate TagGPT on publicly available datasets, i.e., Kuaishou and Food.com, and demonstrate the effectiveness of TagGPT compared to existing hashtags and off-the-shelf taggers.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 113:\n",
      "Text: Additionally, to track new and creative applications for bioinformatics tools such as ChatGPT, we have established a GitHub repository at https://github.com/csbl-br/awesome-compbio-chatgpt.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 114:\n",
      "Text: All data and trained models are publicly available.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 115:\n",
      "Text: We have conducted extensive experiments on 16 public log datasets.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 116:\n",
      "Text: We also release the code and the annotated dataset for replication and future research.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 117:\n",
      "Text: The training data for these models is usually collected from open-source repositories (e.g., GitHub) that contain software faults and security vulnerabilities.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 118:\n",
      "Text: CCUB dataset is curated and our approach is evaluated by people who have a personal relationship with that particular culture.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 119:\n",
      "Text: Project page: https://europe.naverlabs.com/imagenet-sd/\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 120:\n",
      "Text: GitHub repository: https://github.com/ymcui/Chinese-LLaMA-Alpaca\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 121:\n",
      "Text: The resulting \\textbf{C}hinese \\textbf{O}pen \\textbf{I}nstruction \\textbf{G}eneralist (\\textbf{COIG}) corpora are available in Huggingface\\footnote{\\url{https://huggingface.co/datasets/BAAI/COIG}} and Github\\footnote{\\url{https://github.com/FlagOpen/FlagInstruct}}, and will be continuously updated.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 122:\n",
      "Text: We make our model, data, as well as code publicly available.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 123:\n",
      "Text: The data, code, and all model outputs are released in https://github.com/microsoft/AGIEval.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 124:\n",
      "Text: We created a comprehensive dataset including 492.5K samples comprising code-related content produced by ChatGPT, encompassing popular software activities like Q&A (115K), code summarization (126K), and code generation (226.5K).\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 125:\n",
      "Text: The code is publicly available at https://github.com/Vision-CAIR/ChatCaptioner\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 126:\n",
      "Text: Among benchmarks, ChatGPT and GPT-4 do relatively well on well-known datasets like LogiQA and ReClor.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 127:\n",
      "Text: Our dataset and codes are available at https://github.com/XinhaoMei/WavCaps.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 128:\n",
      "Text: Our source code and datasets are available at https://github.com/xinleihe/MGTBench.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 129:\n",
      "Text: The training data, codes, and weights of this project are available at: The training data, codes, and weights of this project are available at: https://github.com/Kent0n-Li/ChatDoctor.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 130:\n",
      "Text: Codes and benchmarking data information are available at https://github.com/yhydhx/ChatGPT-API.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 131:\n",
      "Text: The dataset and code are available at https://github.com/tan92hl/Complex-Question-Answering-Evaluation-of-ChatGPT.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 132:\n",
      "Text: To support further research in related fields, we have made the data generated by ChatGPT publicly available at https://github.com/THU-BPM/chatgpt-sql.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 133:\n",
      "Text: In the second stage, we extrapolate the performance to larger undisclosed data sizes based on a novel parameter-free mapping technique inspired by neural scaling laws.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 134:\n",
      "Text: Also, <projektor> outperforms by a wide margin in data selection effectiveness compared to a range of other off-the-shelf solutions.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 135:\n",
      "Text: Database alignment is a variant of the graph alignment problem: Given a pair of anonymized databases containing separate yet correlated features for a set of users, the problem is to identify the correspondence between the features and align the anonymized user sets based on correlation alone.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 136:\n",
      "Text: We study an instance of the database alignment problem with multivariate Gaussian features and derive results that apply both for database alignment and for planted matching, demonstrating the connection between them.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 137:\n",
      "Text: The maximum likelihood algorithms for both planted matching and database alignment take the form of a linear program and we study relaxations to better understand the significance of various constraints under various conditions and present achievability and converse bounds.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 138:\n",
      "Text: Our analysis and results extend to the unbalanced case where one user set is not fully covered by the alignment.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 139:\n",
      "Text: They only work for in-distribution artifact types generated during training.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 140:\n",
      "Text: In this paper, we analyze the cause and characteristics of the GAN artifacts produced in unseen test data without ground-truths.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 141:\n",
      "Text: We then develop a novel method, namely, DeSRA, to Detect and then Delete those SR Artifacts in practice.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 142:\n",
      "Text: After detecting the artifact regions, we develop a finetune procedure to improve GAN-based SR models with a few samples, so that they can deal with similar types of artifacts in more unseen real data.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 143:\n",
      "Text: The code will be available at https://github.com/TencentARC/DeSRA.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 144:\n",
      "Text: In this work, we review robustness issues of DL and particularly bridge concerns and attempts from approximation theory to statistical learning theory.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 145:\n",
      "Text: Further, we review Bayesian Deep Learning as a means for uncertainty quantification and rigorous explainability.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 146:\n",
      "Text: A new control paradigm using angular momentum and foot placement as state variables in the linear inverted pendulum model has expanded the realm of possibilities for the control of bipedal robots.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 147:\n",
      "Text: This new paradigm, known as the ALIP model, has shown effectiveness in cases where a robot's center of mass height can be assumed to be constant or near constant as well as in cases where there are no non-kinematic restrictions on foot placement.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 148:\n",
      "Text: Our code is available at https://github.com/amazon-science/codetaskcl-pptf\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 149:\n",
      "Text: We also show that it is possible to generate fully-synthetic image-annotation pairs to automatically augment any annotated dataset.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 150:\n",
      "Text: We present and release a new dataset of 50 manually annotated research articles.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 151:\n",
      "Text: The code and trained weights are available at https://github.com/mordecaimalignatius/GAFAR/.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 152:\n",
      "Text: We build on existing tools to computationally analyze data retrieved from publicly available databases.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 153:\n",
      "Text: Among multiple benchmarks on the KITTI dataset, we achieve new state-of-the-art performance.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 154:\n",
      "Text: Codes and models are publicly available at https://github.com/sunlicai/MAE-DFER.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 155:\n",
      "Text: We make the source code available for the 112,000 programs, accompanied by a comprehensive list detailing the vulnerabilities detected in each individual program including location and function name, which makes the dataset ideal to train LLMs and machine learning algorithms.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 156:\n",
      "Text: Our implementation will be publicly available at \\url{https://github.com/ETHRuiGong/PTDiffSeg}.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 157:\n",
      "Text: Our dataset covers 520 images of mathematical graphics collected from 450 documents from different disciplines.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 158:\n",
      "Text: The classification accuracy of the models in the absence and presence of the two attacks are computed on images from the publicly accessible ImageNet dataset.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 159:\n",
      "Text: Furthermore, it facilitates the creation of de-identified datasets for broader 2D image research at major research institutions.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 160:\n",
      "Text: State-of-the-art results are achieved even on more detailed part-segmentation, Pascal-Animals, by only training on coarse-grained datasets.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 161:\n",
      "Text: Our code will be available at the URL: https://github.com/cofly2014/tsa-mlt.git\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 162:\n",
      "Text: KiTS21 is a sequel to its first edition in 2019, and it features a variety of innovations in how the challenge was designed, in addition to a larger dataset.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 163:\n",
      "Text: Additionally, we introduce Tomatopia, a new, large and challenging dataset of greenhouse tomatoes.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 164:\n",
      "Text: TGB datasets are of large scale, spanning years in duration, incorporate both node and edge-level prediction tasks and cover a diverse set of domains including social, trade, transaction, and transportation networks.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 165:\n",
      "Text: The code for this algorithm will be publicly available.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 166:\n",
      "Text: The code and models will be made publicly at \\href{https://github.com/LancasterLi/RefSAM}{github.com/LancasterLi/RefSAM}.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 167:\n",
      "Text: In the fields of Experimental and Computational Aesthetics, numerous image datasets have been created over the last two decades.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 168:\n",
      "Text: Our code is publicly available at https://github.com/withchencheng/ECML_PKDD_23_Real.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 169:\n",
      "Text: We share this visualization and the dataset in the spirit of open science.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 170:\n",
      "Text: covLLM was trained with LLaMA 7B as a baseline model to produce three models trained on (1) the Alpaca and synCovid datasets, (2) the synCovid dataset, and (3) the synCovid and real abstract datasets.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 171:\n",
      "Text: Results demonstrate that training covLLM on the synCovid and abstract pairs datasets performs competitively with ChatGPT and outperforms covLLM trained primarily using the Alpaca dataset.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 172:\n",
      "Text: Using the MIMIC-IT dataset, we train a large VLM named Otter.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 173:\n",
      "Text: The code will be made available.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 174:\n",
      "Text: Project page: https://ba2det.site .\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 175:\n",
      "Text: This dataset allows for the exploration of complex road connections and situations where lane markings may be absent.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 176:\n",
      "Text: Specifically, we benchmark the recognition drop on common detection datasets, where we evaluate both traditional and realistic anonymization for faces and full bodies.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 177:\n",
      "Text: With this CNN, we derived homogeneous atmospheric parameters and abundances for 841300 stars, that remarkably compared to external data-sets.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 178:\n",
      "Text: The final trained model is publicly available at https://github.com/Jesper-Karsten/MBASC\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 179:\n",
      "Text: All resources of PandaLM are released at https://github.com/WeOpenML/PandaLM.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 180:\n",
      "Text: In this paper, we propose a detector with the ability to predict both open-vocabulary objects and their part segmentation.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 181:\n",
      "Text: First, we train the detector on the joint of part-level, object-level and image-level data to build the multi-granularity alignment between language and image.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 182:\n",
      "Text: In light of this, the paper aims to analyze the measurement of the carbon footprint of 1,417 ML models and associated datasets on Hugging Face, which is the most popular repository for pretrained ML models.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 183:\n",
      "Text: Current methods rely on datasets with expensive annotations; multi-view images and their camera parameters.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 184:\n",
      "Text: The WHOW-KG consists of a network of five ontologies and related linked open data, modelled according to those ontologies.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 185:\n",
      "Text: We propose Multi-CrossRE, the broadest multi-lingual dataset for RE, including 26 languages in addition to English, and covering six text domains.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 186:\n",
      "Text: We run a baseline model over the 26 new datasets and--as sanity check--over the 26 back-translations to English.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 187:\n",
      "Text: We also evaluate performance on the MultiMedQA suite of benchmark datasets.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 188:\n",
      "Text: Our model and code are available at https://github.com/microsoft/LMOps.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 189:\n",
      "Text: To train and evaluate our approach, we curated InferredBugs, a novel, metadata-rich dataset of bugs extracted by executing the Infer static analyzer on the change histories of thousands of Java and C# repositories.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 190:\n",
      "Text: We also release codebase for evaluation set extraction.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 191:\n",
      "Text: Dataset, to fight the bias prevalent in giant datasets.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 192:\n",
      "Text: We will make our code and pre-trained models publicly available.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 193:\n",
      "Text: We perform an extensive study across six datasets with eight models from three model families.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 194:\n",
      "Text: For this, we augment standard bug-fixing datasets with bug report discussions.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 195:\n",
      "Text: Resources are made publicly available through GitHub, fostering open research in the Chinese NLP community and beyond.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 196:\n",
      "Text: We release our code and data under fully permissive licenses.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 197:\n",
      "Text: For enabling the combination of ChatGPT and RDF KGs, we present a research prototype, called GPToLODS, which is able to enrich any ChatGPT response with more information from hundreds of RDF KGs.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 198:\n",
      "Text: In particular, it identifies and annotates each entity of the response with statistics and hyperlinks to LODsyndesis KG (which contains integrated data from 400 RDF KGs and over 412 million entities).\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 199:\n",
      "Text: Numerous AIGC detectors have been developed and evaluated on natural language data.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 200:\n",
      "Text: We evaluated six AIGC detectors, including three commercial and three open-source solutions, assessing their performance on this dataset.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 201:\n",
      "Text: We have released a dataset comprised of ChatGPT's responses to support further research in this area.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 202:\n",
      "Text: Both datasets involve scraping through known data sources (through platforms like stack overflow, crowdsourcing, etc.)\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 203:\n",
      "Text: We call the collected dataset the Human ChatGPT Comparison Corpus (HC3).\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 204:\n",
      "Text: The dataset, code, and models are all publicly available at https://github.com/Hello-SimpleAI/chatgpt-comparison-detection.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 205:\n",
      "Text: For datasets originally without shared tokens in label names, we also offer an automated method, using OpenAI's ChatGPT, to generate shared actions and objects.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 206:\n",
      "Text: We make our code, models, and datasets publicly available.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 207:\n",
      "Text: The source code will be released publicly.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 208:\n",
      "Text: We evaluate PaTeCon on two large-scale datasets based on Wikidata and Freebase respectively.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 209:\n",
      "Text: We present a novel synthetic dataset, named Parcel3D, that is based on the Google Scanned Objects (GSO) dataset and consists of more than 13,000 images of parcels with full 3D annotations.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 210:\n",
      "Text: The dataset contains intact, i.e. cuboid-shaped, parcels and damaged parcels, which were generated in simulations.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 211:\n",
      "Text: Dataset and code are available at https://a-nau.github.io/parcel3d.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 212:\n",
      "Text: Datasets, code and results are made publicly available and will be continuously updated at https://github.com/ZhaomingKong/Denoising-Comparison.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 213:\n",
      "Text: Specifically, we collect a dataset of e-textbooks from one of the largest free online publishers in the world.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 214:\n",
      "Text: The dataset and data analysis scripts are available on our open-source repository.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 215:\n",
      "Text: To facilitate progress in this field, we have developed a well-labeled road pothole dataset named Urban Digital Twins Intelligent Road Inspection (UDTIRI) dataset.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 216:\n",
      "Text: Our intention is to employ this dataset for object detection, semantic segmentation, and instance segmentation tasks.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 217:\n",
      "Text: The source code and the proposed dataset are publicly available at https://github.com/fh2019ustc/DocTr-Plus.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 218:\n",
      "Text: Hence, we present a dataset that exclusively consists of healthy and diseased cashew leaves and fruits.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 219:\n",
      "Text: The entire code and models will be open-sourced.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 220:\n",
      "Text: With nationwide coverage, real-world network topology, and rich geospatial features, this data repository can be used for a variety of traffic-related tasks.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 221:\n",
      "Text: The data and code are available on GitHub (https://github.com/baixianghuang/travel).\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 222:\n",
      "Text: Code and models will be accessed at https://github.com/Liuxinyv/SAZS.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 223:\n",
      "Text: BenchMD combines 19 publicly available datasets for 7 medical modalities, including 1D sensor data, 2D images, and 3D volumetric scans.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 224:\n",
      "Text: We introduce the LongForm dataset, which is created by leveraging English corpus examples with augmented instructions.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 225:\n",
      "Text: We publicly release our data and models: https://github.com/akoksal/LongForm.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 226:\n",
      "Text: Code is generated using a commercial tool, SonarCloud.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 227:\n",
      "Text: Our source code will be available at https://github.com/MC-E/DragonDiffusion.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 228:\n",
      "Text: An open-source implementation is available online.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 229:\n",
      "Text: Deep-learning-based object detection and semantic segmentation models have proven to be suitable for this purpose.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 230:\n",
      "Text: We evaluate the effectiveness of this approach by training a semantic segmentation model on a real dataset augmented in two ways: 1) using synthetic images obtained from real masks, and 2) generating images from synthetic semantic masks.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 231:\n",
      "Text: The dataset spans seven sub-topics and is annotated with a materials-science focused multi-label annotation scheme for AZ.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 232:\n",
      "Text: The code is available at \\url{https://github.com/cjw2021/SOV-STG}.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 233:\n",
      "Text: The code and pretrained models will be released under https://github.com/lik1996/iCMFormer.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 234:\n",
      "Text: The data samples are automatically generated from a curated set of reasoning patterns, where the patterns are annotated with inference labels by experts.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 235:\n",
      "Text: The data is obtained from a fleet of gas sensors that measure and track quantities such as oxygen and sound.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 236:\n",
      "Text: With the help of this data, we can detect events such as occupancy in a specific environment.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 237:\n",
      "Text: Extensive experiments and visualizations on four datasets demonstrate the powerful performance of our method.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 238:\n",
      "Text: Codes will be available.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 239:\n",
      "Text: Extensive evaluation on three benchmark datasets using multiple evaluation metrics show the effectiveness of the proposed framework.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 240:\n",
      "Text: Every program is labeled with the vulnerabilities found within the source code, indicating the type, line number, and vulnerable function name.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 241:\n",
      "Text: This property of the dataset makes it suitable for evaluating the effectiveness of various static and dynamic analysis tools.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 242:\n",
      "Text: While prior research demonstrated the high performance of ChatGPT across numerous NLP tasks, open-source LLMs like HugginChat and FLAN are gaining attention for their cost-effectiveness, transparency, reproducibility, and superior data protection.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 243:\n",
      "Text: With extensive experiments on a large-scale real-world dataset, we demonstrate the substantial effectiveness of our approach.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 244:\n",
      "Text: In the query-based FL platform, which is an open model sharing and reusing platform empowered by the community for model mining, we explore a wide range of valuable topics, including the availability of up-to-date model repositories for model querying, legal compliance analysis between different model licenses, and copyright issues and intellectual property protection in model reusing.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 245:\n",
      "Text: However, little is known about how much training data they require, and how this number depends on the data structure.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 246:\n",
      "Text: Our code is available at https://github.com/siyi-wind/MDViT.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 247:\n",
      "Text: Along with the datasets, we also propose corresponding baseline solutions to the three aforementioned tasks.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 248:\n",
      "Text: These attacks are launched on three powerful pre-trained image classifier architectures, ResNet-34, GoogleNet, and DenseNet-161.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 249:\n",
      "Text: This dataset comprises a large number of tasks that demand problem-solving skills.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 250:\n",
      "Text: The implementation of CAME is publicly available.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 251:\n",
      "Text: However, due to privacy restrictions, few public real-world VFL datasets exist for algorithm evaluation, and these represent a limited array of feature distributions.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 252:\n",
      "Text: Additionally, we introduce a real VFL dataset to address the deficit in image-image VFL scenarios.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 253:\n",
      "Text: In this paper, we introduce ScalOTA, an end-to-end scalable OTA software update architecture and secure protocol for modern vehicles.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 254:\n",
      "Text: Our code is available at https://github.com/yuping-wu/PULSAR.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 255:\n",
      "Text: On both \\pascal and \\coco datasets, we conduct extensive experiments to evaluate the framework effectiveness.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 256:\n",
      "Text: Our source code and the related paper list are available on https://github.com/SLDGroup/survey-zero-shot-nas.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 257:\n",
      "Text: Extensive experiments show our method achieves state-of-the-art results on the HMDB51 and UCF101 datasets and a competitive result on the benchmark of Kinetics and something-2-something V2 datasets.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 258:\n",
      "Text: Experimental results on real-world datasets demonstrate that our method achieves better performance compared with several state-of-the-art social bot detection methods.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 259:\n",
      "Text: However, due to the challenges in data collection and network designs, it remains challenging for existing solutions to achieve real-time full-body capture while being accurate in world space.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 260:\n",
      "Text: In this work, we contribute a sequential proxy-to-motion learning scheme together with a proxy dataset of 2D skeleton sequences and 3D rotational motions in world space.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 261:\n",
      "Text: More video results can be found at our project page: https://liuyebin.com/proxycap.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 262:\n",
      "Text: In this technical report, we take an initiative to investigate their capacities of playing text games, in which a player has to understand the environment and respond to situations by having dialogues with the game world.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 263:\n",
      "Text: We address this issue by introducing a new dataset: GHOSTS.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 264:\n",
      "Text: This paper also contributes a new surveillance dataset called NightSuR.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 265:\n",
      "Text: We present in this work a new Universal Morphology dataset for Korean.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 266:\n",
      "Text: The PIKS technique is scalable and can readily ingest new datasets.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 267:\n",
      "Text: Datasets that reflect the diverse array of multimodal, multilingual news sources available online could be used to teach models to benefit from this shift, but existing news video datasets focus on traditional news broadcasts produced for English-speaking audiences.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 268:\n",
      "Text: The MIND dataset is at the moment of writing the most extensive dataset available for the research and development of news recommender systems.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 269:\n",
      "Text: We also introduce a new dataset generated by our classifier that tracks the dynamics of fake news in the Chinese language during the early pandemic.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 270:\n",
      "Text: Our code and data will be released shortly.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 271:\n",
      "Text: This paper introduces a novel dataset\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 272:\n",
      "Text: We present a unique collection of data\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 273:\n",
      "Text: In this study, we unveil our newly created dataset\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 274:\n",
      "Text: This article brings to light a freshly curated dataset\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 275:\n",
      "Text: We reveal an unprecedented dataset in the field of\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 276:\n",
      "Text: In this work, we propose a pioneering dataset\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 277:\n",
      "Text: This research details the compilation of a new dataset\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 278:\n",
      "Text: Our article describes a groundbreaking data collection\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 279:\n",
      "Text: We are pleased to introduce an innovative dataset\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 280:\n",
      "Text: This publication centers around our newly assembled dataset\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 281:\n",
      "Text: Herein, we unfold a freshly minted dataset\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 282:\n",
      "Text: Our research entails the design and creation of a new dataset\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 283:\n",
      "Text: The crux of this paper is the disclosure of a new dataset\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 284:\n",
      "Text: This manuscript details the formulation of a novel data assembly\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 285:\n",
      "Text: We exhibit an inventive dataset for the first time in this paper\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 286:\n",
      "Text: We present an exploratory dataset in this research\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 287:\n",
      "Text: This paper features the unveiling of a hitherto unseen dataset\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 288:\n",
      "Text: In the present study, we showcase an original data compilation\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 289:\n",
      "Text: Our work represents the first public exhibition of a unique dataset\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 290:\n",
      "Text: We draw back the curtains on a state-of-the-art dataset in this research\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 291:\n",
      "Text: We will release our dataset and code for future research.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 292:\n",
      "Text: The real-world datasets will be released.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 293:\n",
      "Text: In an effort to accelerate this research and assist others, we are releasing our dataset and model to the research community.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 294:\n",
      "Text: To promote progress towards this goal, we release OBJECT: a dataset consisting of 400K editing examples created from procedurally generated 3D scenes.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 295:\n",
      "Text: This article presents DataXploreFines, an innovative Shiny application that revolutionizes data exploration, analysis, and visualization.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 296:\n",
      "Text: Users can upload their datasets in popular formats like CSV or Excel, explore the data structure, perform manipulations, and obtain statistical summaries.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 297:\n",
      "Text: DataXploreFines provides a wide range of interactive visualizations, including histograms, scatter plots, bar charts, and line graphs, enabling users to identify patterns and trends.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 298:\n",
      "Text: Experimental results demonstrate that our proposed approach outperforms the state-of-the-art methods on fine-grained datasets under real-world scenarios.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 299:\n",
      "Text: We evaluate our method on two real-world clinical datasets, where the time series contains sequences of (1) high-frequency electrocardiograms and (2) structured data from lab values and vitals signs.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 300:\n",
      "Text: Substantial progress has been made recently in motion data collection technologies and generation methods, laying the foundation for increasing interest in human motion generation.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 301:\n",
      "Text: Additionally, we provide an overview of common datasets and evaluation metrics.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 302:\n",
      "Text: For Amharic, we use our own publicly-available Amharic Speech Emotion Dataset (ASED).\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 303:\n",
      "Text: For English, German and Urdu we use the existing RAVDESS, EMO-DB and URDU datasets.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 304:\n",
      "Text: Our dataset is available at https://github.com/iamazxl/OAVQA.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 305:\n",
      "Text: Concretely, we first summarize the widely-used ST ocean datasets and identify their unique characteristics.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 306:\n",
      "Text: In this paper, we first design various synthetic datasets with custom hardness and noisiness levels for different samples.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 307:\n",
      "Text: To address this, we propose a large-scale SlowTV dataset curated from YouTube, containing an order of magnitude more data than existing automotive datasets.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 308:\n",
      "Text: 13k sentence pairs) and a web-domain corpus (approx.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 309:\n",
      "Text: We release the resulting corpus and our analysis pipeline for future research.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 310:\n",
      "Text: The corpora have been annotated with dependency trees, lemmas, and part-of-speech tags.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 311:\n",
      "Text: Therefore, we propose that LLM-assisted annotation is a promising automated approach for corpus studies.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 312:\n",
      "Text: In this study, we explore the potential of LLMs in assisting corpus-based linguistic studies through automatic annotation of texts with specific categories of linguistic information.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 313:\n",
      "Text: This article proposes a new training model to solve this problem through NLP processing methods.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 314:\n",
      "Text: We also develop tools for automatic extraction of task instances given a constraint structure and a raw text corpus.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 315:\n",
      "Text: Starting with an initial text corpus, our framework employs a large language model to select multiple sentences that describe the same scene from various perspectives.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 316:\n",
      "Text: Experimental results on multiple benchmark datasets demonstrate the effectiveness of our method.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 317:\n",
      "Text: Our proposed methods achieve state-of-the-art results on three popular benchmark datasets, and the source code will be made publicly available shortly.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 318:\n",
      "Text: We perform an exhaustive evaluation in two benchmark datasets.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 319:\n",
      "Text: We conduct experiments on two benchmark datasets.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 320:\n",
      "Text: Extensive experiments conducted on two benchmark datasets show that our approach achieves excellent performance compared to its competitors.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 321:\n",
      "Text: The proposed model is validated through extensive experiments on two benchmark datasets, showcasing superior performance compared to existing methods.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 322:\n",
      "Text: With the above tasks, we benchmark a range of algorithms on our proposed dataset, drawing new insights for further research and practice.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 323:\n",
      "Text: Experimental results indicate that the proposed approach can obtain better performance on benchmark datasets compared with baselines.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 324:\n",
      "Text: We validate our scheme with some of the most popular benchmarking datasets.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 325:\n",
      "Text: Method: We present an evaluation of five open-source and four proprietary tools against a benchmark dataset.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 326:\n",
      "Text: To testify the effectiveness and superiority of the proposed approach, we conduct extensive experiments on benchmark datasets.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 327:\n",
      "Text: We also introduce a new dataset for benchmarking, and the evaluations are performed from four different perspectives including quantitative metrics, visual effects, human ratings and computational cost.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 328:\n",
      "Text: Our method is simple yet it outperforms several state-of-the-art methods on six popular dataset benchmarks.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 329:\n",
      "Text: Finally, we conduct extensive experiments on widely-used benchmark datasets to validate the superiority of our method by comparing it with existing state-of-the-art methods.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 330:\n",
      "Text: We validate the model both qualitatively and quantitatively on four benchmark datasets.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 331:\n",
      "Text: Finally, I provide the first high-quality benchmark dataset in order to fairly compare existing models and aid future model developers.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 332:\n",
      "Text: Finally, we applied our method on two benchmark datasets, STACOM2018, and M\\&Ms 2020 challenges, to show the potency of the proposed model.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 333:\n",
      "Text: Extensive experiments on several benchmark datasets show that our method outperforms existing methods across all datasets while maintaining low computational complexity.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 334:\n",
      "Text: Our data and benchmarking results are available at: https://lmexam.com.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 335:\n",
      "Text: Empirically, we conduct extensive experiments on several benchmark datasets to support our theory.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 336:\n",
      "Text: Extensive experiments are conducted on multiple benchmark data sets and our method establishes a state-of-the-art performance in terms of both performance and trustworthiness.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 337:\n",
      "Text: We evaluate our model on four datasets and achieve state-of-the-art performances.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 338:\n",
      "Text: Experimental results demonstrate the performance and limitations of existing algorithms, and the dataset benchmark has good versatility and effectiveness.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 339:\n",
      "Text: Our extensive experiments on three benchmarks, Lucchi, MitoEM-R and MitoEM-H, reveal the benefits of the proposed contributions achieving state-of-the-art results on all three datasets.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 340:\n",
      "Text: We test our method on two public datasets, our method achieves the best performances on these two datasets.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 341:\n",
      "Text: Finally, the experimental results on several benchmark datasets verify the effectiveness of the proposed method.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 342:\n",
      "Text: Extensive experiments on two real-world datasets show the superior performance of our method.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 343:\n",
      "Text: We have evaluated various baselines on this dataset and benchmarked it with a new neural model, SPOT, which we introduce in this paper.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 344:\n",
      "Text: The benchmark dataset, leaderboard, and baseline methods are released in https://github.com/THU-KEG/KoRC.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 345:\n",
      "Text: Experimental results on real-world and benchmark datasets validate the effectiveness of the proposed method.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 346:\n",
      "Text: Our results improve the state-of-the-art on standard benchmarks.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 347:\n",
      "Text: Experimental evaluations both on our assembled dataset and public benchmark datasets demonstrate the effectiveness of our proposed network.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 348:\n",
      "Text: Our results show that our simple synthetic datasets are sufficient to challenge most of the benchmarked methods.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 349:\n",
      "Text: Additionally, we introduce a novel benchmark based on images from the Open Images Dataset.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 350:\n",
      "Text: As a result, our method TOLD achieves a DER of 10.14% on the CALLHOME dataset, which is a new state-of-the-art result on this benchmark to the best of our knowledge.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 351:\n",
      "Text: We conducted comprehensive experiments on multiple benchmark datasets, demonstrating the superior performance of our proposed SPIFFNet in terms of both quantitative metrics and visual quality when compared to state-of-the-art methods.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 352:\n",
      "Text: We provide a detailed analysis of the dataset.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 353:\n",
      "Text: We evaluate our method on three challenging datasets.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 354:\n",
      "Text: The benchmark results produced by three different deep learning methods are provided.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 355:\n",
      "Text: In addition, we provide extra annotations for used datasets and introduce our new benchmark.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 356:\n",
      "Text: Experimental results on two real-world datasets demonstrate that our method outperforms some state-of-the-art approaches.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 357:\n",
      "Text: Experiments on real-world datasets demonstrate the effectiveness of our approach.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 358:\n",
      "Text: The extensive experiments demonstrated that our proposed method achieves state-of-the-art performance on the newly collected dataset.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 359:\n",
      "Text: We evaluate our method on several datasets and demonstrate its superior performance under heavily occluded scenarios compared to other methods.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 360:\n",
      "Text: The dataset and source code will be released on GitHub soon.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 361:\n",
      "Text: Then this model is trained and evaluated on the new, more extensive dataset to obtain a representative result.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 362:\n",
      "Text: Results show that the performance significantly increases with the dataset size.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 363:\n",
      "Text: Details are available in CSV files provided with the datasets.   \n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 364:\n",
      "Text: Since the manual creation of such datasets is a laborious task, obtaining data from online resources can be a cheap solution to create large-scale datasets.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 365:\n",
      "Text: Data format and usage notes:\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 366:\n",
      "Text: However, online companies also gather user data for more principled purposes, such as improving the user experience and aggregating statistics.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 367:\n",
      "Text: Finally, we provide publicly an open dataset, and online resources with the results.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 368:\n",
      "Text: Besides the traditional data, various types of data, including video, have become available.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 369:\n",
      "Text: Besides, with such an instruction, we can also easily carry out quantitative statistics.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 370:\n",
      "Text: Moreover, limitations related to data sources that change over time (e.g., code bases) and the lack of documentation of extraction processes make it difficult to reproduce datasets over time.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 371:\n",
      "Text: Evaluation datasets and frameworks like the one we present support this line of research.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 372:\n",
      "Text: We then review available datasets, recent approaches and evaluation metrics of the task.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 373:\n",
      "Text: Besides, our extensive analyses verify the high quality of our dataset and the effectiveness of our evaluation metrics.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 374:\n",
      "Text: However, we identify issues with the dataset quality and evaluation metric.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 375:\n",
      "Text: In order to obtain meaningful results, careful partitioning of data into training, validation, and test sets, as well as the selection of suitable evaluation metrics are crucial.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 376:\n",
      "Text: To evaluate and benchmark the models, we propose a comprehensive evaluation scheme (including automatic and manual metrics).\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 377:\n",
      "Text: These metrics have been successful on datasets that leverage the average human perception in limited settings.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 378:\n",
      "Text: We use standard metrics to evaluate the performances of the different training scenarios.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 379:\n",
      "Text: Finally, we report extensive experiments on two real-world datasets to offer insight into the efficiency, scalability, and effectiveness of the proposed framework.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 380:\n",
      "Text: Here we describe Athena 2.0, UCSC's conversational agent for Amazon's Socialbot Grand Challenge 4.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 381:\n",
      "Text: The Alexa Prize Challenge aims to create a socialbot, which allows the user to engage in coherent conversations, on a range of popular topics that will interest the user.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 382:\n",
      "Text: Athena 2.0 also relies on a user model to personalize topic selection and other aspects of the conversation to individual users.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 383:\n",
      "Text: Athena 2.0 utilizes a novel knowledge-grounded discourse model that tracks the entity links that Athena introduces into the dialogue, and uses them to constrain named-entity recognition and linking, and coreference resolution.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 384:\n",
      "Text: Code and data are publicly available on GitHub.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 385:\n",
      "Text: The code is publicly available.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 386:\n",
      "Text: The code and models will be publicly available.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 387:\n",
      "Text: We make the source code and models publicly available.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 388:\n",
      "Text: Our code is publicly available.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 389:\n",
      "Text: Code will be publicly available.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 390:\n",
      "Text: Our data and code are publicly available.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 391:\n",
      "Text: Code is publicly released.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 392:\n",
      "Text: The code will be accessible to the public.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 393:\n",
      "Text: Code and data will be made publicly available.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 394:\n",
      "Text: Code will be made publicly available.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 395:\n",
      "Text: Our data and code are publicly available at: https://pointodyssey.com\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 396:\n",
      "Text: The code will be made publicly available.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 397:\n",
      "Text: The source code and dataset are available at project page.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 398:\n",
      "Text: Project page is https://damo-vilab.github.io/AnyDoor-Page/.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 399:\n",
      "Text: Source code is public, and can be accessed at: https://github.com/ApplicationTechnologyOfMedicalBigData/pFedNet-code.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 400:\n",
      "Text: Our project page is available at the following URL: https://sites.google.com/view/pku-beavertails.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 401:\n",
      "Text: Our code will be publicly available.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 402:\n",
      "Text: In this article, we propose Human-M3, an outdoor multi-modal multi-view multi-person human pose database which includes not only multi-view RGB videos of outdoor scenes but also corresponding pointclouds.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 403:\n",
      "Text: Furthermore, we propose a 3D human pose estimation algorithm based on multi-modal data input, which demonstrates the advantages of multi-modal data input for 3D human pose estimation.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 404:\n",
      "Text: 3D human pose estimation in outdoor environments has garnered increasing attention recently.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 405:\n",
      "Text: We introduce a novel one-stage end-to-end multi-person 2D pose estimation algorithm, known as Joint Coordinate Regression and Association (JCRA), that produces human pose joints and associations without requiring any post-processing.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 406:\n",
      "Text: However, prevalent 3D human pose datasets pertaining to outdoor scenes lack diversity, as they predominantly utilize only one type of modality (RGB image or pointcloud), and often feature only one individual within each scene.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 407:\n",
      "Text: iEDA is publicly available from the project home page http://ieda.oscc.cc.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 408:\n",
      "Text: The codes and data are publicly available at https://github.com/RUCAIBox/iEvaLM-CRS.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 409:\n",
      "Text: Our extensive set of experiments on a variety of standard and few-shot datasets show that our method produces substantially improved performance when compared to the current state of the art methods.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 410:\n",
      "Text: Our benchmark reflects real-world data constraints by evaluating methods across a range of dataset sizes, including challenging few-shot settings that incentivize the use of pretraining.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 411:\n",
      "Text: Despite various attempts to enhance cross-dataset generalization, the problem remains challenging, particularly when testing against common post-processing perturbations, such as video compression or blur.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 412:\n",
      "Text: We demonstrate that our method achieves state-of-the-art results on several challenging benchmark datasets even in comparison to recent supervised methods, and that our method reaches previously unseen cross-dataset generalisation ability.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 413:\n",
      "Text: Extending prior work, our benchmark is designed for zero-shot evaluation only, and hence considers an open-set of similarity conditions.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 414:\n",
      "Text: Traditional zero-shot learning methods are constrained by the training dataset.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 415:\n",
      "Text: Experiments on three different datasets demonstrate that our approach generalizes well beyond the training data, yielding a broad capture range even on unseen anatomies and modality pairs, without the need for specialized retraining.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 416:\n",
      "Text: Doing so, we achieve new state-of-the-art results on low-shot settings and strong experimental results in various evaluation protocols with a training that is at least 3 times faster than prior methods.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 417:\n",
      "Text: Combining both, our method (ProVP-Ref) is evaluated on 11 image benchmark datasets and achieves 7/11 state-of-theart results on both few-shot and base-to-novel settings.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 418:\n",
      "Text: We perform experiments on simulated and retrospective in-vivo data to evaluate the performance of the proposed zero-shot learning method for temporal FSE reconstruction.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 419:\n",
      "Text: Finally, we empirically validate our methods with extensive experiments on three established biomedical benchmarks.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 420:\n",
      "Text: Extensive experiments are conducted on multiple benchmark datasets to validate the effectiveness of our proposed method.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 421:\n",
      "Text: We have quantitatively and qualitatively validated our method on three common benchmark datasets: Human3.6M, MPI-INF-3DHP, and HumanEva.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 422:\n",
      "Text: Extensive experiments on six datasets show substantial improvements to the baseline.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 423:\n",
      "Text: Besides, since medical OSR is still a nascent field, two publicly available benchmark datasets are proposed for comparison.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 424:\n",
      "Text: In this study, we conducted experiments using medical datasets comprising only 100 samples from three medical modalities.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 425:\n",
      "Text: To validate our model design, we conduct extensive experiments on three benchmark datasets: Human3.6M, HumanEva-I, and MPI-INF-3DHP.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 426:\n",
      "Text: We conduct extensive experiments on three public benchmarks (i.e., HumanEval, MBPP, and MBCPP).\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 427:\n",
      "Text: Comprehensive experiments are conducted on public benchmarks and the corresponding experimental results demonstrate the effectiveness of our proposed method.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 428:\n",
      "Text: We have conducted experiments on two real datasets and compare our method with three baselines.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 429:\n",
      "Text: We have theoretically analyzed our method and empirically evaluated it on two real-world medical datasets.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 430:\n",
      "Text: We then describe the dataset and the results of benchmarking.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 431:\n",
      "Text: ModFed is evaluated on three in-vivo datasets.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 432:\n",
      "Text: To this end, we introduce a new data curation pipeline to construct a culturally relevant parallel corpus, enriched with annotations of cultural-specific entities.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 433:\n",
      "Text: Furthermore, while engineers have developed cutting-edge technologies for image classification, there is still a gap in the application of these models in human heritage collections, where data sets usually consist of low-quality pictures of people with diverse ethnicity, gender, and age.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 434:\n",
      "Text: We evaluated the effectiveness of the bias mitigation pipeline on a cultural heritage collection of photographs from the 19th and 20th centuries, and we used the FairFace data set for the transfer learning experiments.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 435:\n",
      "Text: This paper answers the need to support GLAM institutions in facilitating the transition into publishing their digital content and to introduce collections as data services; this will also help their future efficient contribution to data spaces and cultural heritage clouds.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 436:\n",
      "Text: Within the current transition to the emerging data spaces, clouds for cultural heritage and open science, the need to identify practices which support more GLAM institutions to offer datasets becomes a priority, especially within the smaller and medium-sized institutions.   \n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 437:\n",
      "Text: Creating an intelligent search and retrieval system for artwork images, particularly paintings, is crucial for documenting cultural heritage, fostering wider public engagement, and advancing artistic analysis and interpretation.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 438:\n",
      "Text: We will release our annotation scheme, the corpus, and codes to the research community to alleviate the scarcity of labeled data in this domain.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 439:\n",
      "Text: Digital preservation of Cultural Heritage (CH) sites is crucial to protect them against damage from natural disasters or human activities.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 440:\n",
      "Text: Our work aims to provide a dataset of crowdsourced images and 3D reconstructions for research in computer vision, heritage conservation, and related domains.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 441:\n",
      "Text: Our second contribution is to conduct a comprehensive analysis of the annotations, focusing on how different demographic groups are represented.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 442:\n",
      "Text: Existing benchmarks often resort to synthetic datasets, derived from arbitrary feature splits from a global set, which only capture a subset of feature distributions, leading to inadequate algorithm performance assessment.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 443:\n",
      "Text: Moreover, our method demonstrates high generalization capability in cross-dataset experiments, even when the training and test sets have different characteristics.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 444:\n",
      "Text: Our model shows strong performance improvements on the Winoground and VL-checklist datasets with only a mild degradation in zero-shot performance.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 445:\n",
      "Text: We conduct comprehensive experiments on multiple datasets, including CIFAR10, CIFAR100 and ImageNet.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 446:\n",
      "Text: We also systematically benchmark sample efficiency, zero-shot generalization, and adaptability to changes in data distributions of various models.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 447:\n",
      "Text: This challenge is especially prominent in the few-shot learning scenario, where the data in the target domain is generally much scarcer and of lowered quality.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 448:\n",
      "Text: Experiments on two datasets demonstrate that our proposed method outperforms the baselines and achieves new state-of-the-art performance.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 449:\n",
      "Text: Our results show a significant improvement for all the language pairs investigated, better cross-lingual inference, and a superior performance in zero-shot learning scenarios as compared to state-of-the-art baselines.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 450:\n",
      "Text: Cross-dataset evaluation further shows the strong generalization ability of our approach.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 451:\n",
      "Text: We also tested our model in a zero-shot setting on an independent test site without any additional fine-tuning.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 452:\n",
      "Text: Also, it demonstrates robust zero-shot capability when trained exclusively on reasoning-free datasets.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 453:\n",
      "Text: Extensive experiments on real-world degraded datasets demonstrate the effectiveness of our proposed method.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 454:\n",
      "Text: We propose a new approach, CLIPPR (CLIP with Priors), which adapts zero-shot models for regression and classification on unlabelled datasets.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 455:\n",
      "Text: Extensive experiments show that our model has great cross-dataset generalization.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 456:\n",
      "Text: Without resorting to any complex techniques, such as image translation, augmentation, or rare-class sampling, we set a new state-of-the-art on all benchmarks.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 457:\n",
      "Text: In addition, our method also surprisingly helps improve the generalization ability of the models under zero-shot settings.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 458:\n",
      "Text: We demonstrate the effectiveness of our method by conducting experiments on benchmark datasets for few-shot learning as well as in the real world for robot perception.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 459:\n",
      "Text: Existing methods show impressive results on individual datasets but lack the ability to generalize to unseen scenarios.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 460:\n",
      "Text: Empirically, our method achieves better performance than all baselines on multiple datasets.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 461:\n",
      "Text: Our technique functions is in a zero-shot fashion, as it only utilizes data from a single scan of highly undersampled time series images.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 462:\n",
      "Text: As a result, current methods often resort to random sampling from supervised datasets to create \"few-data\" setups and employ inconsistent training strategies during evaluations, which poses a challenge in accurately comparing recent progress.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 463:\n",
      "Text: In comparison to existing zero-shot methods, our approach is universally applicable to any retriever without additional adaptation or indexing.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 464:\n",
      "Text: However, due to the computational demands associated with training these models, their applications often rely on zero-shot settings.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 465:\n",
      "Text: The model can capture more context information from multiple scales and better fuse the local and global information to achieve high-quality segmentation.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 466:\n",
      "Text: When benchmarking against popular existing segmentation models across three datasets, our proposed model demonstrates a substantial leap in performance.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 467:\n",
      "Text: Extensive experiments on the PASCAL VOC 2012 and Cityscapes datasets demonstrate the superiority and scalability of the proposed method, outperforming the current state-of-the-art approaches.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 468:\n",
      "Text: In this paper, we present a framework to enhance depth by leveraging semantic segmentation to guide the network to jump out of the local minimum.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 469:\n",
      "Text: The DNMP provides a new paradigm for urban-level scene representation with appealing properties: $(1)$ High-quality rendering.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 470:\n",
      "Text: Results on the Cityscapes and Mapillary datasets show the proposed approach achieves significantly more controllability and improved image quality than previous approaches on urban scenes and is on par with general-purpose non-controllable generative models (like StyleGAN2) in terms of quality.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 471:\n",
      "Text: To facilitate a unified comparison between novel segmentation algorithms, we propose a standardized evaluation strategy for our dataset.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 472:\n",
      "Text: Experimental results on the nuScenes dataset demonstrate that our framework is highly compatible with various map segmentation and detection architectures and considerably strengthens map prediction performance, even under adverse weather conditions and across longer horizons.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 473:\n",
      "Text: For the high-resolution Cityscapes and Mapillary Vistas datasets, we achieve improvements of up to +2.5 on the Panoptic Quality for thing classes, and even more considerable gains of up to +5.8 on both the pixel accuracy and pixel precision, which we identify as better metrics to capture the confusion problem.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 474:\n",
      "Text: Our experimental results demonstrate the effectiveness of our approach in overcoming the challenges posed by urban traffic scenes.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 475:\n",
      "Text: To develop and evaluate our approach, a large urban driving dataset dubbed AV Breadcrumbs is automatically labeled by leveraging vector map representations and projective geometry to annotate over 900,000 images.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 476:\n",
      "Text: Depth completion, which aims to generate high-quality dense depth maps from sparse depth maps, has attracted increasing attention in recent years.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 477:\n",
      "Text: Extensive experiments show the effectiveness of our method.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 478:\n",
      "Text: Extensive experiments demonstrate the effectiveness of our method.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 479:\n",
      "Text: We finally conduct extensive analyses to understand the effectiveness of our method.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 480:\n",
      "Text: Extensive experiments show the effectiveness of our approach.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 481:\n",
      "Text: A series of experiments demonstrate the effectiveness of our methods.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 482:\n",
      "Text: Our method is effective and presents a significant improvement over the original model.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 483:\n",
      "Text: We show that our method produces highly accurate results.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 484:\n",
      "Text: Extensive experiments demonstrate that our method outperforms existing works significantly.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 485:\n",
      "Text: Our approach has several advantages over many previous methods.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 486:\n",
      "Text: Additionally, we employ the self-training strategy to improve the performance of our method further.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 487:\n",
      "Text: our method has three advantages.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 488:\n",
      "Text: Extensive experiments are conducted, and the results demonstrate the effectiveness and superiority of our method.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 489:\n",
      "Text: Comparison experiments demonstrate the effectiveness of our method.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 490:\n",
      "Text: Extensive experimental results confirm the effectiveness of our method.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 491:\n",
      "Text: Our method consistently outperforms previous approaches across a range of tasks.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 492:\n",
      "Text: Our method enjoys three key technical parts.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 493:\n",
      "Text: Our approach yields excellent results, with accuracies that are comparable to or better than those of existing state-of-the-art approaches.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 494:\n",
      "Text: Empirical experiments demonstrate the efficacy of our approach.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 495:\n",
      "Text: Extensive experiments are conducted to demonstrate the effectiveness of our proposed method.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 496:\n",
      "Text: Our experimental results demonstrate the effectiveness of our proposed model, outperforming existing methods on three datasets.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 497:\n",
      "Text: The method proposed here showed improvement from our precious results on the same dataset.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 498:\n",
      "Text: Experiments show that the proposed method achieves state-of-the-art results on the present dataset.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 499:\n",
      "Text: Experiments conducted on two dedicated datasets demonstrate the superior performance of the proposed method over recent studies in the literature.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 500:\n",
      "Text: Sufficient experiments demonstrate that the proposed method achieves results beyond the state-of-the-art methods on various datasets.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 501:\n",
      "Text: Extensive experiments demonstrate that benefiting from the proposed components, we achieve new state-of-the-art performance compared to previous methods on several public datasets.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 502:\n",
      "Text: Furthermore, on various datasets, we make competitive achievement results with other previous state-of-the-art methods.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 503:\n",
      "Text: Overall, our work carefully studies the effectiveness of popular scoring functions in realistic settings and helps to better understand their limitations.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 504:\n",
      "Text: We have evaluated our method on a set of supervised-learning datasets.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 505:\n",
      "Text: Our method yields superior performance when dealing with scales outside of those covered by the training dataset.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 506:\n",
      "Text: The experimental results demonstrate that the proposed method can bring significant improvements in BLEU scores on two datasets.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 507:\n",
      "Text: Compared to a variety of baselines, our method achieves superior results.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 508:\n",
      "Text: Experiments on real-world datasets prove the significant effectiveness and generalization ability of the proposed method.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 509:\n",
      "Text: Extensive evaluations conducted on public datasets and comparison with other methods further confirm the effectiveness of the proposed framework.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 510:\n",
      "Text: Experiments on five challenging large-scale public datasets demonstrate that our proposed method is effective and outperforms the state-of-the-art methods.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 511:\n",
      "Text: The experimental results on two datasets show the superiority of our methods.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 512:\n",
      "Text: Extensive experiments are conducted on seven widely used datasets to demonstrate the effectiveness of the proposed approach.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 513:\n",
      "Text: We validate our method on five datasets, empirically demonstrating that it outperforms the baseline methods in most cases and is valid over a wider range of training budgets.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 514:\n",
      "Text: Extensive experiments have been implemented on three real life datasets to demonstrate the effectiveness of our proposed algorithm.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 515:\n",
      "Text: Extensive experiments are conducted to demonstrate the effectiveness of all the proposed components, and results show that our approach outperforms the state-of-the-art (SOTA) in four representative datasets, both qualitatively and quantitatively.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 516:\n",
      "Text: Experimental results on 11 datasets demonstrate the consistent superiority of our method over previous alternatives.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 517:\n",
      "Text: Extensive experiments with multiple datasets are conducted to demonstrate the effectiveness of our unified solution to all the three tasks and the generalized case.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 518:\n",
      "Text: Extensive experiments conducted on three popular public datasets (CASIA-B, OU-MVLP, and GREW) demonstrate that our proposed method achieves state-of-the-art performance on multiple benchmark tests.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 519:\n",
      "Text: We demonstrate that our method surpasses previous attempts in qualitative and quantitative results through extensive experiments conducted on benchmark datasets such as Set5, Set14, Urban100, BSD100, and Manga109.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 520:\n",
      "Text: Through extensive experiments on three real datasets, we demonstrate the efficiency and effectiveness of our proposed algorithms.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 521:\n",
      "Text: We show that our method performs superior to state of-the-art alternatives on various datasets.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 522:\n",
      "Text: The statistical and experimental results indicate that our dataset provides the basis for the future improvements of existing methods.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 523:\n",
      "Text: Finally, extensive experimental results on three public datasets clearly demonstrate the effectiveness of the proposed model.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 524:\n",
      "Text: Experiment results on real-world datasets MLQA demonstrate that the proposed method can improve the performance by a large margin, outperforming the baseline method by 13.18%/12.00% F1/EM on average.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 525:\n",
      "Text: Extensive experiments demonstrate that our method outperforms baselines on 16 out of 20 datasets, underlining its effectiveness and superiority in alleviating the heterophily problem.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 526:\n",
      "Text: Our methods are complementary to the existing pre-training or data mining approaches and can be used in a variety of settings.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 527:\n",
      "Text: Experimental results on two authoritative public datasets demonstrate that our proposed method boosts state-of-the-art performance by a large margin.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 528:\n",
      "Text: Upon testing this framework on four distinct real datasets, we find that our synthetic training data are able to yield high-quality results also on real images-even more so if fine-tune on a few real images was done.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 529:\n",
      "Text: We show that our framework significantly improves the quality of the resulting synthetic images and is adaptable to unseen data with fine-tuning.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 530:\n",
      "Text: To this end, we generate two synthetic datasets and then develop an end-to-end pipeline and model that is tested on both benchmarks.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 531:\n",
      "Text: The results on the kgbench dataset with three different embedding methods show promising results.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 532:\n",
      "Text: We present our findings that these embeddings are useful beyond the noise prediction task, as they contain discriminative information and can also be leveraged for classification.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 533:\n",
      "Text: Though this approach is more discriminative, for large-scale datasets the computational cost is prohibitive since pre-computation of candidate embeddings is no longer possible.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 534:\n",
      "Text: Our results show that the inherent data bias that persists in KG can be altered by specific algorithm bias as incorporated by KG embedding learning algorithms.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 535:\n",
      "Text: To test our pipeline and determine the performance of our models, we first evaluate both word embedding models on an English dataset (SEMEVAL-CCOHA).\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 536:\n",
      "Text: Most existing graph embedding methods fall short of reaching high data scalability.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 537:\n",
      "Text: In this paper, we present a large-scale human-labeled dataset for Russian text recognition in-the-wild.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 538:\n",
      "Text: Experiments are conducted on two commonly used KGET datasets to show that the performance of KGE methods on the KGET task can be substantially improved by the proposed multiple auxiliary relations and asynchronous embedding learning.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 539:\n",
      "Text: Existing methods commonly pre-compute image embeddings over the entire corpus and compare these to a reference image embedding modified by the query text at test time.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 540:\n",
      "Text: However, classic approaches, which employ a fixed distance metric as a similarity function between two embeddings, may lead to suboptimal performance for capturing the complex data distribution.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 541:\n",
      "Text: External Knowledge Injection:} By fine-tuning special token embeddings for each category via Textual Inversion, performance improves across 17 datasets, except when dealing with low-resolution reference images.   \n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 542:\n",
      "Text: However, existing embedding architectures suffer from two limitations: (1) limited discriminability of synthetic features' embedding without considering fine-grained cluster structures; (2) inflexible optimization due to restricted scaling mechanisms on existing contrastive embedding networks, leading to overlapped representations in the embedding space.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 543:\n",
      "Text: Recent works in this domain have achieved significant improvements by the representation learning paradigm, e.g., embedding-based retrieval (EBR) and collaborative filtering (CF).\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 544:\n",
      "Text: We compare these embeddings to those generated by competing architectures and pre-trainings for classification tasks.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 545:\n",
      "Text: While there are many high-quality datasets for English text recognition; there are no available datasets for Russian language.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 546:\n",
      "Text: In this paper, we analyze different strategies to create static and contextual word embedding models, i.e., Word2Vec and ELMo, on real-world English and Romanian datasets.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 547:\n",
      "Text: Moreover, we compared the 3RL dataset with other famous state-of-the-art datasets (FER dataset, CK+ dataset), and we applied the most commonly used algorithms in previous works, SVM and CNN.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 548:\n",
      "Text: Our dataset includes 11,248 labels and 4,003 images, with a focus on hand-raising behavior.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 549:\n",
      "Text: Especially for video segmentation, adjacent frames contain rich contextual information beneficial in cognizing noisy labels.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 550:\n",
      "Text: Extracting image semantics effectively and assigning corresponding labels to multiple objects or attributes for natural images is challenging due to the complex scene contents and confusing label dependencies.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 551:\n",
      "Text: Subsequently, semantically similar target images are clustered while the source model provides their corresponding weak pseudo-labels with unreliable confidence.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 552:\n",
      "Text: Multi-label image classification is a prediction task that aims to identify more than one label from a given image.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 553:\n",
      "Text: To that end, we introduce a new dataset for this problem, Doppelgangers, which includes image pairs of similar structures with ground truth labels.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 554:\n",
      "Text: Finally, the discriminative label-level features related to the image content are accurately captured by the joint training of the above three parts.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 555:\n",
      "Text: Based on this dataset, we propose FUnet (Frame-Unet), which achieves state-of-the-art performance by utilizing inter-frame information and self-attention mechanisms.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 556:\n",
      "Text: However, the paired images and layouts in the existing dataset are collected by inpainting and annotating posters, respectively.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 557:\n",
      "Text: The dataset also contains 2D image labels that are used in the supervised training of an ink detection model.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 558:\n",
      "Text: We study algorithms to automatically detect such annotation errors, in particular methods to score label quality, such that the images with the lowest scores are least likely to be correctly labeled.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 559:\n",
      "Text: We focus especially on Task-C and propose a novel LLMs cooperation system named a doctor-patient loop to generate high-quality conversation data sets.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 560:\n",
      "Text: This analysis also investigates the potential of utilizing cooperation LLMs to generate high-quality datasets.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 561:\n",
      "Text: This paper presents UMASS_BioNLP team participation in the MEDIQA-Chat 2023 shared task for Task-A and Task-C.  \n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 562:\n",
      "Text: The experiment results demonstrate that our approaches yield reasonable performance as evaluated by automatic metrics such as ROUGE, medical concept recall, BLEU, and Self-BLEU.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 563:\n",
      "Text: Furthermore, we conducted a comparative analysis between our proposed method and ChatGPT and GPT-4.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 564:\n",
      "Text: We present working notes on transfer learning with semi-supervised dataset annotation for the BirdCLEF 2023 competition, focused on identifying African bird species in recorded soundscapes\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 565:\n",
      "Text: We explore the embedding space learned by BirdNET and propose a process to derive an annotated dataset for supervised learning\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 566:\n",
      "Text: Our approach utilizes existing off-the-shelf models, BirdNET and MixIT, to address representation and labeling challenges in the competition.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 567:\n",
      "Text: We explore the embedding space learned by BirdNET and propose a process to derive an annotated dataset for supervised learning.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 568:\n",
      "Text: Our experiments involve various models and feature engineerih in classifying bird species and highlight the potential of transfer learning and semi-supervised dataset annotation in similar tasks.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 569:\n",
      "Text: Our data generator is capable of generating large-scale datasets of human activities\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 570:\n",
      "Text: We release M3Act3D, an 87.6-hour 3D motion dataset of human activities with larger group sizes and higher complexity of inter-person interactions than previous multi-person datasets\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 571:\n",
      "Text: The understanding of complex human interactions and group activities has garnered attention in human-centric computer vision.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 572:\n",
      "Text: However, the advancement of the related tasks is hindered due to the difficulty of obtaining large-scale labeled real-world datasets.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 573:\n",
      "Text: To mitigate the issue, we propose M3Act, a multi-view multi-group multi-person human atomic action and group activity data generator.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 574:\n",
      "Text: Powered by the Unity engine, M3Act contains simulation-ready 3D scenes and human assets, configurable lighting and camera systems, highly parameterized modular group activities, and a large degree of domain randomization during the data generation process.  \n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 575:\n",
      "Text: with multiple viewpoints, modalities (RGB images, 2D poses, 3D motions), and high-quality annotations for individual persons and multi-person groups (2D bounding boxes, instance segmentation masks, individual actions and group activity categories).\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 576:\n",
      "Text: Using M3Act, we perform synthetic data pre-training for 2D skeleton-based group activity recognition and RGB-based multi-person pose tracking.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 577:\n",
      "Text: The results indicate that learning from our synthetic datasets largely improves the model performances on real-world datasets, with the highest gain of 5.59% and 7.32% respectively in group and person recognition accuracy on CAD2, as well as an improvement of 6.63 in MOTP on HiEve.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 578:\n",
      "Text: Pre-training with our synthetic data also leads to faster model convergence on downstream tasks (up to 6.8% faster).\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 579:\n",
      "Text: Moreover, M3Act opens new research problems for 3D group activity generation.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 580:\n",
      "Text: We release M3Act3D, an 87.6-hour 3D motion dataset of human activities with larger g\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 581:\n",
      "Text: Large language models typically undergo two training stages, pretraining and finetuning.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 582:\n",
      "Text: Despite that large-scale pretraining endows the model with strong capabilities to generate natural language responses, these pretrained models can still fail to understand human instructions at times.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 583:\n",
      "Text: To enhance language models' ability of interpreting and responding to instructions, instruction finetuning has emerged as a critical method in this area.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 584:\n",
      "Text: Recent studies found that large language models can be finetuned to perform well even with a small amount of high-quality instruction-following data.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 585:\n",
      "Text: However, the selection of high-quality datasets for finetuning language models still lacks clear guidelines to follow.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 586:\n",
      "Text: In this paper, we propose InstructMining, a linear rule for evaluating instruction-following data quality.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 587:\n",
      "Text: We formulate InstructMining using specific natural language indicators.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 588:\n",
      "Text: To investigate the relationship between data quality and these indicators, we further conduct extensive finetuning experiments.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 589:\n",
      "Text: The experiment results are then applied to estimating parameters in InstructMining.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 590:\n",
      "Text: To further investigate its performance, we use InstructMining to select high-quality data from unseen datasets.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 591:\n",
      "Text: Results demonstrate that InstructMining can help select relatively high-quality samples from various instruction-following datasets.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 592:\n",
      "Text: Compared to models finetuned on unfiltered datasets, models finetuned on InstructMining selected datasets perform better on 42.5% cases.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 593:\n",
      "Text: Therefore, we introduce the Infrastructural Multi-Person Trajectory and Context Dataset (IMPTC).\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 594:\n",
      "Text: The resulting dataset consists of eight hours of measurement data\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 595:\n",
      "Text: In addition, to enable the entire stack of research capabilities, the dataset includes all data, starting from the sensor-, calibration- and detection data until trajectory and context data.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 596:\n",
      "Text: The dataset is continuously expanded and is available online for non-commercial research at https://github.com/kav-institute/imptc-dataset.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 597:\n",
      "Text: Inner-city intersections are among the most critical traffic areas for injury and fatal accidents.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 598:\n",
      "Text: Automated vehicles struggle with the complex and hectic everyday life within those areas.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 599:\n",
      "Text: Sensor-equipped smart infrastructures, which can cooperate with vehicles, can benefit automated traffic by extending the perception capabilities of drivers and vehicle perception systems.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 600:\n",
      "Text: Additionally, they offer the opportunity to gather reproducible and precise data of a holistic scene understanding, including context information as a basis for training algorithms for various applications in automated traffic.  \n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 601:\n",
      "Text: We use an intelligent public inner-city intersection in Germany with visual sensor technology.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 602:\n",
      "Text: A multi-view camera and LiDAR system perceives traffic situations and road users' behavior.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 603:\n",
      "Text: Additional sensors monitor contextual information like weather, lighting, and traffic light signal status.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 604:\n",
      "Text: The data acquisition system focuses on Vulnerable Road Users (VRUs) and multi-agent interaction.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 605:\n",
      "Text: The resulting dataset consists of eight hours of measurement data.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 606:\n",
      "Text: It contains over 2,500 VRU trrollers, and wheelchair users, and over 20,000 vehicle trajectories at different day times, weather conditions, and seasons.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 607:\n",
      "Text: In addition, to enable the entire stack of research capabilities, the dataset includes all data, starting from the sensor-, calibration- and detection data until\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 608:\n",
      "Text: Noisy label problems are inevitably in existence within medical image segmentation causing severe performance degradation.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 609:\n",
      "Text: Previous segmentation methods for noisy label problems only utilize a single image while the potential of leveraging the correlation between images has been overlooked.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 610:\n",
      "Text: Based on two insights, we propose a Multi-Scale Temporal Feature Affinity Learning (MS-TFAL) framework to resolve noisy-labeled medical video segmentation issues.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 611:\n",
      "Text: First, we argue the sequential prior of videos is an effective reference, i.e., pixel-level features from adjacent frames are close in distance for the same class and far in distance otherwise.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 612:\n",
      "Text: Therefore, Temporal Feature Affinity Learning (TFAL) is devised to indicate possible noisy labels by evaluating the affinity between pixels in two adjacent frames.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 613:\n",
      "Text: We also notice that the noise distribution exhibits considerable variations across video, image, and pixel levels.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 614:\n",
      "Text: In this way, we introduce Multi-Scale Supervision (MSS) to supervise the network from three different perspectives by re-weighting and refining the samples.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 615:\n",
      "Text: This design enables the network to concentrate on clean samples in a coarse-to-fine manner.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 616:\n",
      "Text: Experiments with both synthetic and real-world label noise demonstrate that our method outperforms recent state-of-the-art robust segmentation approaches.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 617:\n",
      "Text: Code is available at https://github.com/BeileiCui/MS-TFAL.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 618:\n",
      "Text: This paper proposes a data-efficient detection method for deep neural networks against backdoor attacks under a black-box scenario.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 619:\n",
      "Text: The proposed approach is motivated by the intuition that features corresponding to triggers have a higher influence in determining the backdoored network output than any other benign features.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 620:\n",
      "Text: To quantitatively measure the effects of triggers and benign features on determining the backdoored network output, we introduce five metrics.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 621:\n",
      "Text: To calculate the five-metric values for a given input, we first generate several synthetic samples by injecting the input's partial contents into clean validation samples.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 622:\n",
      "Text: Then, the five metrics are computed by using the output labels of the corresponding synthetic samples.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 623:\n",
      "Text: One contribution of this work is the use of a tiny clean validation dataset.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 624:\n",
      "Text: Having the computed five metrics, five novelty detectors are trained from the validation dataset.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 625:\n",
      "Text: A meta novelty detector fuses the output of the five trained novelty detectors to generate a meta confidence score.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 626:\n",
      "Text: During online testing, our method determines if online samples are poisoned or not via assessing their meta confidence scores output by the meta novelty detector.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 627:\n",
      "Text: We show the efficacy of our methodology through a broad range of backdoor attacks, including ablation studies and comparison to existing approaches.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 628:\n",
      "Text: Our methodology is promising since the proposed five metrics quantify the inherent differences between clean and poisoned samples.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 629:\n",
      "Text: Additionally, our detection method can be incrementally improved by appending more metrics that may be proposed to address future advanced attacks.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 630:\n",
      "Text: To address this limitation, we have developed a synthetic dataset for short-term trajectory prediction tasks using the CARLA simulator.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 631:\n",
      "Text: This dataset is extensive and incorporates what is considered complex scenarios - pedestrians crossing the road, vehicles overtaking - and comprises 6000 perspective view images with corresponding IMU and odometry information for each frame.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 632:\n",
      "Text: Our datasets are publicly available on https://github.com/navigatinguncertainty.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 633:\n",
      "Text: Autonomous vehicles require accurate and reliable short-term trajectory predictions for safe and efficient driving.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 634:\n",
      "Text: While most commercial automated vehicles currently use state machine-based algorithms for trajectory forecasting, recent efforts have focused on end-to-end data-driven systems.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 635:\n",
      "Text: Often, the design of these models is limited by the availability of datasets, which are typically restricted to generic scenarios.  \n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 636:\n",
      "Text: This dataset is extensive and incorporates what is considered complex scenarios - pedestrians crossing the road, vehicles overtaking -  (LSTM) networks has also been developed.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 637:\n",
      "Text: This model can handle corner cases, such as slowing down near zebra crossings and stopping when pedestrians cross the road, without the need for explicit encoding of the surrounding environment.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 638:\n",
      "Text: Prompt-tuning has become an increasingly popular parameter-efficient method for adapting large pretrained language models to downstream tasks.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 639:\n",
      "Text: However, both discrete prompting and continuous prompting assume fixed prompts for all data samples within a task, neglecting the fact that inputs vary greatly in some tasks such as open-domain dialogue generation.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 640:\n",
      "Text: In this paper, we present a novel, instance-specific prompt-tuning algorithm for dialogue generation.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 641:\n",
      "Text: Specifically, we generate prompts based on instance-level control code, rather than the conversation history, to explore their impact on controlled dialogue generation.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 642:\n",
      "Text: Experiments on popular open-domain dialogue datasets, evaluated on both automated metrics and human evaluation, demonstrate that our method is superior to prompting baselines and comparable to fine-tuning with only 5%-6% of total parameters.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 643:\n",
      "Text: Code and datasets are available on GitHub https://github.com/neurospin-projects/2023_rlouiset_sepvae.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 644:\n",
      "Text: Contrastive Analysis VAE (CA-VAEs) is a family of Variational auto-encoders (VAEs) that aims at separating the common factors of variation between a background dataset (BG) (i.e., healthy subjects) and a target dataset (TG) (i.e., patients) from the ones that only exist in the target dataset.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 645:\n",
      "Text: To do so, these methods separate the latent space into a set of salient features (i.e., proper to the target dataset) and a set of common features (i.e., exist in both datasets).\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 646:\n",
      "Text: Currently, all models fail to prevent the sharing of information between latent spaces effectively and to capture all salient factors of variation.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 647:\n",
      "Text: To this end, we introduce two crucial regularization losses: a disentangling term between common and salient representations and a classification term between background and target samples in the salient space.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 648:\n",
      "Text: We show a better performance than previous CA-VAEs methods on three medical applications and a natural images dataset (CelebA).\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 649:\n",
      "Text: The code and dataset are open source at https://github.com/junjun-yan/ATL-PINN.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 650:\n",
      "Text: Physics-informed neural networks (PINNs) have emerged as promising surrogate modes for solving partial differential equations (PDEs).\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 651:\n",
      "Text: Their effectiveness lies in the ability to capture solution-related features through neural networks.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 652:\n",
      "Text: However, original PINNs often suffer from bottlenecks, such as low accuracy and non-convergence, limiting their applicability in complex physical contexts.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 653:\n",
      "Text: To alleviate these issues, we proposed auxiliary-task learning-based physics-informed neural networks (ATL-PINNs), which provide four different auxiliary-task learning modes and investigate their performance compared with original PINNs.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 654:\n",
      "Text: We also employ the gradient cosine similarity algorithm to integrate auxiliary problem loss with the primary problem loss in ATL-PINNs, which aims to enhance the effectiveness of the auxiliary-task learning modes.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 655:\n",
      "Text: To the best of our knowledge, this is the first study to introduce auxiliary-task learning modes in the context of physics-informed learning.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 656:\n",
      "Text: We conduct experiments on three PDE problems across different fields and scenarios.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 657:\n",
      "Text: Our findings demonstrate that the proposed auxiliary-task learning modes can significantly improve solution accuracy, achieving a maximum performance boost of 96.62% (averaging 28.23%) compared to the original single-task PINNs.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 658:\n",
      "Text: To facilitate the investigation, we introduce WikiTiLo, a well-curated image dataset compromising images with rich socio-cultural cues\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 659:\n",
      "Text: We will release our dataset and codes to facilitate future studies.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 660:\n",
      "Text: Vision-Language Models (VLMs) are expected to be capable of reasoning with commonsense knowledge as human beings.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 661:\n",
      "Text: One example is that humans can reason where and when an image is taken based on their knowledge.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 662:\n",
      "Text: This makes us wonder if, based on visual cues, Vision-Language Models that are pre-trained with large-scale image-text resources can achieve and even outperform human's capability in reasoning times and location.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 663:\n",
      "Text: To address this question, we propose a two-stage \\recognition\\space and \\reasoning\\space probing task, applied to discriminative and generative VLMs to uncover whether VLMs can recognize times and location-relevant features and further reason about it. .\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 664:\n",
      "Text: In the extensive experimental studies, we find that although VLMs can effectively retain relevant features in visual encoders, they still fail to make perfect reasoning.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 665:\n",
      "Text: Our model was evaluated on two benchmark tree counting datasets, Jiangsu, and Yosemite, as well as a new dataset, KCL-London, created by ourselves\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 666:\n",
      "Text: The codes and datasets are available at https://github.com/HAAClassic/TreeFormer.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 667:\n",
      "Text: Automatic tree density estimation and counting using single aerial and satellite images is a challenging task in photogrammetry and remote sensing, yet has an important role in forest management.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 668:\n",
      "Text: In this paper, we propose the first semisupervised transformer-based framework for tree counting which reduces the expensive tree annotations for remote sensing images.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 669:\n",
      "Text: Our method, termed as TreeFormer, first develops a pyramid tree representation module based on transformer blocks to extract multi-scale features during the encoding stage.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 670:\n",
      "Text: Contextual attention-based feature fusion and tree density regressor modules are further designed to utilize the robust features from the encoder to estimate tree density maps in the decoder.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 671:\n",
      "Text: Moreover, we propose a pyramid learning strategy that includes local tree density consistency and local tree count ranking losses to utilize unlabeled images into the training process.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 672:\n",
      "Text: Finally, the tree counter token is introduced to regulate the network by computing the global tree counts for both labeled and unlabeled images. .\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 673:\n",
      "Text: Our TreeFormer outperforms the state of the art semi-supervised methods under the same setting and exceeds the fully-supervised methods using the same number of labeled images.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 674:\n",
      "Text: Despite recent advancements in speech emotion recognition (SER) models, state-of-the-art deep learning (DL) approaches face the challenge of the limited availability of annotated data.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 675:\n",
      "Text: Large language models (LLMs) have revolutionised our understanding of natural language, introducing emergent properties that broaden comprehension in language, speech, and vision.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 676:\n",
      "Text: This paper examines the potential of LLMs to annotate abundant speech data, aiming to enhance the state-of-the-art in SER.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 677:\n",
      "Text: We evaluate this capability across various settings using publicly available speech emotion classification datasets.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 678:\n",
      "Text: Leveraging ChatGPT, we experimentally demonstrate the promising role of LLMs in speech emotion data annotation.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 679:\n",
      "Text: Our evaluation encompasses single-shot and few-shots scenarios, revealing performance variability in SER.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 680:\n",
      "Text: Notably, we achieve improved results through data augmentation, incorporating ChatGPT-annotated samples into existing datasets.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 681:\n",
      "Text: Our work uncovers new frontiers in speech emotion classification, highlighting the increasing significance of LLMs in this field moving forward.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 682:\n",
      "Text: We have taken millions of images in the sorghum fields, manually selected 5,447 images that contain aphids, and annotated each aphid cluster in the image.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 683:\n",
      "Text: To use these images for machine learning models, we crop the images into patches and created a labeled dataset with over 151,000 image patches.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 684:\n",
      "Text: Aphids are one of the main threats to crops, rural families, and global food security.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 685:\n",
      "Text: Chemical pest control is a necessary component of crop production for maximizing yields, however, it is unnecessary to apply the chemical approaches to the entire fields in consideration of the environmental pollution and the cost.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 686:\n",
      "Text: Thus, accurately localizing the aphid and estimating the infestation level is crucial to the precise local application of pesticides.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 687:\n",
      "Text: Aphid detection is very challenging as each individual aphid is really small and all aphids are crowded together as clusters.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 688:\n",
      "Text: In this paper, we propose to estimate the infection level by detecting aphid clusters.  \n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 689:\n",
      "Text: Then, we i\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 690:\n",
      "Text: Given a set of calibrated images of a scene, we present an approach that produces a simple, compact, and actionable 3D world representation by means of 3D primitives.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 691:\n",
      "Text: While many approaches focus on recovering high-fidelity 3D scenes, we focus on parsing a scene into mid-level 3D representations made of a small set of textured primitives.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 692:\n",
      "Text: Such representations are interpretable, easy to manipulate and suited for physics-based simulations.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 693:\n",
      "Text: Moreover, unlike existing primitive decomposition methods that rely on 3D input data, our approach operates directly on images through differentiable rendering.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 694:\n",
      "Text: Specifically, we model primitives as textured superquadric meshes and optimize their parameters from scratch with an image rendering loss.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 695:\n",
      "Text: We highlight the importance of modeling transparency for each primitive, which is critical for optimization and also enables handling varying numbers of primitives.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 696:\n",
      "Text: We show that the resulting textured primitives faithfully reconstruct the input images and accurately model the visible 3D points, while providing amodal shape completions of unseen object regions.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 697:\n",
      "Text: We compare our approach to the state of the art on diverse scenes from DTU, and demonstrate its robustness on real-life captures from BlendedMVS and Nerfstudio.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 698:\n",
      "Text: We also showcase how our results can be used to effortlessly edit a scene or perform physical simulations.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 699:\n",
      "Text: Code and video results are available at https://www.tmonnier.com/DBW .\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 700:\n",
      "Text: This resource paper introduces ISLTranslate, a translation dataset for continuous Indian Sign Language (ISL) consisting of 31k ISL-English sentence/phrase pairs\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 701:\n",
      "Text: To the best of our knowledge, it is the largest translation dataset for continuous Indian Sign Language\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 702:\n",
      "Text: To validate the performance of existing end-to-end Sign language to spoken language translation systems, we benchmark the created dataset with a transformer-based model for ISL translation\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 703:\n",
      "Text: Sign languages are the primary means of communication for many hard-of-hearing people worldwide.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 704:\n",
      "Text: Recently, to bridge the communication gap between the hard-of-hearing community and the rest of the population, several sign language translation datasets have been proposed to enable the development of statistical sign language translation systems.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 705:\n",
      "Text: However, there is a dearth of sign language resources for the Indian sign language. .\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 706:\n",
      "Text: To the best of our knowledge, it is the largest translation dataset for continuous Indian Sign Language.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 707:\n",
      "Text: To validchmark the created dataset with a transformer-based model for ISL translation.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 708:\n",
      "Text: This paper introduces the Life Scapes Reasoning Benchmark (LSR-Benchmark), a novel dataset targeting real-life scenario reasoning\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 709:\n",
      "Text: The dataset consists of 2,162 questions collected from open-source online sources and is manually annotated to improve its quality.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 710:\n",
      "Text: , aiming to close the gap in artificial neural networks' ability to reason in everyday contexts.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 711:\n",
      "Text: In contrast to domain knowledge reasoning datasets, LSR-Benchmark comprises free-text formatted questions with rich information on real-life scenarios, human behaviors, and character roles.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 712:\n",
      "Text: The dataset consists of 2,162 questions collected from open-source online sources and is manually annotated to improve its qualitto test the performance in LSR-Benchmark.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 713:\n",
      "Text: The results reveal that humans outperform these models significantly, indicating a persisting challenge for machine learning models in comprehending daily human life.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 714:\n",
      "Text: The smarty4covid dataset contains audio signals of cough (4,676), regular breathing (4,665), deep breathing (4,695) and voice (4,291) as recorded by means of mobile devices following a crowd-sourcing approach\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 715:\n",
      "Text: Other self reported information is also included (e.g. COVID-19 virus tests), thus providing a comprehensive dataset for the development of COVID-19 risk detection models.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 716:\n",
      "Text: The smarty4covid dataset is released in the form of a web-ontology language (OWL) knowledge base enabling data consolidation from other relevant datasets, complex queries and reasoning\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 717:\n",
      "Text: Harnessing the power of Artificial Intelligence (AI) and m-health towards detecting new bio-markers indicative of the onset and progress of respiratory abnormalities/conditions has greatly attracted the scientific and research interest especially during COVID-19 pandemic. .\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 718:\n",
      "Text: The smarty4covid dataset is releasedtowards the development of models able to: (i) extract clinically informative respiratory indicators from regular breathing records, and (ii) identify cough, breath and voirisk detection models is proposed and validated.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 719:\n",
      "Text: This dataset collects nighttime images with different properties of nighttime environments, such as flare and extreme darkness\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 720:\n",
      "Text: Nighttime surveillance suffers from degradation due to poor illumination and arduous human annotations.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 721:\n",
      "Text: It is challengable and remains a security risk at night.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 722:\n",
      "Text: Existing methods rely on multi-spectral images to perceive objects in the dark, which are troubled by low resolution and color absence.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 723:\n",
      "Text: We argue that the ultimate solution for nighttime surveillance is night-to-day translation, or Night2Day, which aims to translate a surveillance scene from nighttime to the daytime while maintaining semantic consistency.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 724:\n",
      "Text: To achieve this, this paper presents a Disentangled Contrastive (DiCo) learning method.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 725:\n",
      "Text: Specifically, to address the poor and complex illumination in the nighttime scenes, we propose a learnable physical prior, i.e., the color invariant, which provides a stable perception of a highly dynamic night environment and can be incorporated into the learning pipeline of neural networks.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 726:\n",
      "Text: Targeting the surveillance scenes, we develop a disentangled representation, which is an auxiliary pretext task that separates surveillance scenes into the foreground and background with contrastive learning.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 727:\n",
      "Text: Such a strategy can extract the semantics without supervision and boost our model to achieve instance-aware translation.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 728:\n",
      "Text: Finally, we incorporate all the modules above into generative adversarial networks and achieve high-fidelity translation.  \n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 729:\n",
      "Text: It includes six scenes to support the study on nighttime surveillance.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 730:\n",
      "Text: This dataset collects nighttime images with different properties of nigg works significantly.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 731:\n",
      "Text: In this paper, we present TRansPose, the first large-scale multispectral dataset that combines stereo RGB-D, thermal infrared (TIR) images, and object poses to promote transparent object research\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 732:\n",
      "Text: The dataset includes 99 transparent objects, encompassing 43 household items, 27 recyclable trashes, 29 chemical laboratory equivalents, and 12 non-transparent objects\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 733:\n",
      "Text: It comprises a vast collection of 333,819 images and 4,000,056 annotations, providing instance-level segmentation masks, ground-truth poses, and completed depth information\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 734:\n",
      "Text: The data was acquired using a FLIR A65 thermal infrared (TIR) camera, two Intel RealSense L515 RGB-D cameras, and a Franka Emika Panda robot manipulator\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 735:\n",
      "Text: TRansPose dataset can be accessed from the following link: https://sites.google.com/view/transpose-dataset\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 736:\n",
      "Text: Transparent objects are encountered frequently in our daily lives, yet recognizing them poses challenges for conventional vision sensors due to their unique material properties, not being well perceived from RGB or depth cameras.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 737:\n",
      "Text: Overcoming this limitation, thermal infrared cameras have emerged as a solution, offering improved visibility and shape information for transparent objects. .\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 738:\n",
      "Text: The dataset includes 99 transparent objects, encompassing 43 household items, 27 recyclable trashes, 29 chemical laboratory equivalents, and 12 non-transparent objects.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 739:\n",
      "Text: It comprises a vast colleced using a FLIR A65 thermal infrared (TIR) camera, two Intel RealSense L515 RGB-D cameras, and a Franka Emika Panda robot manipulator.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 740:\n",
      "Text: Spanning 87 sequences, TRansPose cbjects in plastic bags, and multi-stacked objects.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 741:\n",
      "Text: this paper, we introduce the BeaverTails dataset, aimed at fostering research on safety alignment in large language models (LLMs).\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 742:\n",
      "Text: In total, we have compiled safety meta-labels for 30,207 question-answer (QA) pairs and gathered 30,144 pairs of expert comparison data for both the helpfulness and harmlessness metrics\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 743:\n",
      "Text: We believe this dataset provides vital resources for the community, contributing towards the safe development and deployment of LLMs.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 744:\n",
      "Text: In  This dataset uniquely separates annotations of helpfulness and harmlessness for question-answering pairs, thus offering distinct perspectives on these crucial attributes.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 745:\n",
      "Text: In total, we have compiled safety meta-labels for 30,207 question-answer (QA) pairs and gathered 30,144 pairs of expert comparisonhasizing its potential for practical safety measures in LLMs.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 746:\n",
      "Text: We propose the In-context Autoencoder (ICAE) for context compression in a large language model (LLM).\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 747:\n",
      "Text: The ICAE has two modules: a learnable encoder adapted with LoRA from an LLM for compressing a long context into a limited number of memory slots, and a fixed decoder which is the target LLM that can condition on the memory slots for various purposes.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 748:\n",
      "Text: We first pretrain the ICAE using both autoencoding and language modeling objectives on massive text data, enabling it to generate memory slots that accurately and comprehensively represent the original context.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 749:\n",
      "Text: Then, we fine-tune the pretrained ICAE on a small amount of instruct data to enhance its interaction with various prompts for producing desirable responses.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 750:\n",
      "Text: Our experimental results demonstrate that the ICAE learned with our proposed pretraining and fine-tuning paradigm can effectively produce memory slots with $4\\times$ context compression, which can be well conditioned on by the target LLM to respond to various prompts.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 751:\n",
      "Text: The promising results demonstrate significant implications of the ICAE for its novel approach to the long context problem and its potential to reduce computation and memory overheads for LLM inference in practice, suggesting further research effort in context management for an LLM.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 752:\n",
      "Text: This paper introduces InternVid, a large-scale video-centric multimodal dataset that enables learning powerful and transferable video-text representations for multimodal understanding and generation.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 753:\n",
      "Text: The InternVid dataset contains over 7 million videos lasting nearly 760K hours, yielding 234M video clips accompanied by detailed descriptions of total 4.1B words.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 754:\n",
      "Text:  The InternVid dataset contains over 7 million videos lasting nearly 760K hours, yielding 234M video clips accompanied by detailed descriptions of total 4.1B words.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 755:\n",
      "Text: Our core contribution is to developnguage representation at scale.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 756:\n",
      "Text: Specifically, we utilize a multi-scale approach to generate video-related descriptions.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 757:\n",
      "Text: Furthermore, we introduce ViCLIP, a video-text representation learning model based on ViT-L. Learned on InternVid via contrastive learning, this model demonstrates leading zero-shot action recognition and competitive video retrieval performance.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 758:\n",
      "Text: Beyond basic video understanding tasks like recognition and retrieval, our dataset and model have broad applications.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 759:\n",
      "Text: They are particularly beneficial for generating interleaved video-text data for learning a video-centric dialogue system, advancing video-to-text and text-to-video generation research.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 760:\n",
      "Text: These proposed resources provide a tool for researchers and practitioners interested in multimodal video understanding and generation.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 761:\n",
      "Text: We make our data and code publicly available in https://github.com/AI21Labs/factor.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 762:\n",
      "Text: Before deploying a language model (LM) within a given domain, it is important to measure its tendency to generate factually incorrect information in that domain.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 763:\n",
      "Text: Existing factual generation evaluation methods focus on facts sampled from the LM itself, and thus do not control the set of evaluated facts and might under-represent rare and unlikely facts.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 764:\n",
      "Text: We propose FACTOR:\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 765:\n",
      "Text: Factual Assessment via Corpus TransfORmation, a scalable approach for evaluating LM factuality.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 766:\n",
      "Text: FACTOR automatically transforms a factual corpus of interest into a benchmark evaluating an LM's propensity to generate true facts from the corpus vs. similar but incorrect statements.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 767:\n",
      "Text: We use our framework to create two benchmarks: Wiki-FACTOR and News-FACTOR.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 768:\n",
      "Text: We show that: (i) our benchmark scores increase with model size and improve when the LM is augmented with retrieval; (ii) benchmark score correlates with perplexity, but the two metrics do not always agree on model ranking; and (iii) when perplexity and benchmark score disagree, the latter better reflects factuality in open-ended generation, as measured by human annotators.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 769:\n",
      "Text: LCC provided an anonymised dataset comprising 14360 records of young people under the age of 18.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 770:\n",
      "Text: Local authorities in England, such as Leicestershire County Council (LCC), provide Early Help services that can be offered at any point in a young person's life when they experience difficulties that cannot be supported by universal services alone, such as schools.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 771:\n",
      "Text: This paper investigates the utilisation of machine learning (ML) to assist experts in identifying families that may need to be referred for Early Help assessment and support.  \n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 772:\n",
      "Text: The dataset was pre-processed, machine learning models were build, and experiments were conducted to validate and test the performance of the models.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 773:\n",
      "Text: Bias mitigation techniques were applied to improve the fairness of these models.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 774:\n",
      "Text: During testing, while the models demonstrated the capability to identify young people requiring intervention or early help, they also produced a significant number of false positives, especially when constructed with imbalanced data, incorrectly identifying individuals who most likely did not need an Early Help referral.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 775:\n",
      "Text: This paper empirically explores the suitability of data-driven ML models for identifying young people who may require Early Help services and discusses their appropriateness and limitations for this task.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 776:\n",
      "Text: Experiments on a new dataset of real images show that adding RePoGen data to the COCO surpasses previous attempts to top-view pose estimation and significantly improves performance on the bottom-view dataset\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 777:\n",
      "Text: The code and the datasets are available on the project website.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 778:\n",
      "Text: Human Pose Estimation is a thoroughly researched problem; however, most datasets focus on the side and front-view scenarios.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 779:\n",
      "Text: We address the limitation by proposing a novel approach that tackles the challenges posed by extreme viewpoints and poses.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 780:\n",
      "Text: We introduce a new method for synthetic data generation - RePoGen, RarE POses GENerator - with comprehensive control over pose and view to augment the COCO dataset. .\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 781:\n",
      "Text: Through an extensive ablation study on both the top and bottom view data, we elucidate the contributions of methodological choices and demonstrate improved performance.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 782:\n",
      "Text: We propose IntelliGraphs, a set of five new Knowledge Graph datasets.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 783:\n",
      "Text: We also present the dataset generator that produced the synthetic datasets.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 784:\n",
      "Text: Knowledge Graph Embedding (KGE) models are used to learn continuous representations of entities and relations.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 785:\n",
      "Text: A key task in the literature is predicting missing links between entities.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 786:\n",
      "Text: However, Knowledge Graphs are not just sets of links but also have semantics underlying their structure.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 787:\n",
      "Text: Semantics is crucial in several downstream tasks, such as query answering or reasoning.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 788:\n",
      "Text: We introduce the subgraph inference task, where a model has to generate likely and semantically valid subgraphs.  \n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 789:\n",
      "Text: The IntelliGraphs datasets contain subgraphs with semantics expressed in logical rules for evaluating subgraph inference.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 790:\n",
      "Text: We also present the dataset generator that produced the synthetic datased on traditional KGEs.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 791:\n",
      "Text: We evaluate their expressiveness and show that these models cannot capture the semantics.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 792:\n",
      "Text: We believe this benchmark will encourage the development of machine learning models that emphasize semantic understanding.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 793:\n",
      "Text: In the realm of Tiny AI, we introduce \"You Only Look at Interested Cells\" (YOLIC), an efficient method for object localization and classification on edge devices.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 794:\n",
      "Text: Seamlessly blending the strengths of semantic segmentation and object detection, YOLIC offers superior computational efficiency and precision.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 795:\n",
      "Text: By adopting Cells of Interest for classification instead of individual pixels, YOLIC encapsulates relevant information, reduces computational load, and enables rough object shape inference.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 796:\n",
      "Text: Importantly, the need for bounding box regression is obviated, as YOLIC capitalizes on the predetermined cell configuration that provides information about potential object location, size, and shape.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 797:\n",
      "Text: To tackle the issue of single-label classification limitations, a multi-label classification approach is applied to each cell, effectively recognizing overlapping or closely situated objects.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 798:\n",
      "Text: This paper presents extensive experiments on multiple datasets, demonstrating that YOLIC achieves detection performance comparable to the state-of-the-art YOLO algorithms while surpassing in speed, exceeding 30fps on a Raspberry Pi 4B CPU.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 799:\n",
      "Text: All resources related to this study, including datasets, cell designer, image annotation tool, and source code, have been made publicly available on our project website at https://kai3316.github.io/yolic.github.io\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 800:\n",
      "Text: Most of the existing LiDAR-inertial navigation systems are based on frame-to-map registrations, leading to inconsistency in state estimation.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 801:\n",
      "Text: The newest solid-state LiDAR with a non-repetitive scanning pattern makes it possible to achieve a consistent LiDAR-inertial estimator by employing a frame-to-frame data association.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 802:\n",
      "Text: In this letter, we propose a robust and consistent frame-to-frame LiDAR-inertial navigation system (FF-LINS) for solid-state LiDARs.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 803:\n",
      "Text: With the INS-centric LiDAR frame processing, the keyframe point-cloud map is built using the accumulated point clouds to construct the frame-to-frame data association.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 804:\n",
      "Text: The LiDAR frame-to-frame and the inertial measurement unit (IMU) preintegration measurements are tightly integrated using the factor graph optimization, with online calibration of the LiDAR-IMU extrinsic and time-delay parameters.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 805:\n",
      "Text: The experiments on the public and private datasets demonstrate that the proposed FF-LINS achieves superior accuracy and robustness than the state-of-the-art systems.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 806:\n",
      "Text: Besides, the LiDAR-IMU extrinsic and time-delay parameters are estimated effectively, and the online calibration notably improves the pose accuracy.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 807:\n",
      "Text: The proposed FF-LINS and the employed datasets are open-sourced on GitHub (https://github.com/i2Nav-WHU/FF-LINS).\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 808:\n",
      "Text: Considering these limitations, we introduce the first video-based retinal dataset by employing handheld devices for data acquisition.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 809:\n",
      "Text: The dataset comprises 635 smartphone-based fundus videos collected from four different clinics, involving 415 patients from 50 to 75 years old.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 810:\n",
      "Text: Specifically, the dataset provides three levels of spatial annotations\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 811:\n",
      "Text: In addition, the dataset offers temporal annotations\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 812:\n",
      "Text: We hope this challenging dataset would significantly contribute to the development of eye disease diagnosis and early prevention.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 813:\n",
      "Text: Retinal vessel segmentation is generally grounded in image-based datasets collected with bench-top devices.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 814:\n",
      "Text: The static images naturally lose the dynamic characteristics of retina fluctuation, resulting in diminished dataset richness, and the usage of bench-top devices further restricts dataset scalability due to its limited accessibility.  \n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 815:\n",
      "Text: The dataset comprises 635 smartphone-based fundus videos collected from four different clinics, involving 415 patients from 50 to 75 he landscape of vasculature segmentation.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 816:\n",
      "Text: Specifically, the dataset provides three levels of spatial annotations: binary vessel masks for overall retinal structure delineation, general vein-artery masks for distinguishing the vein and artery, and fine-grained vein-artery masks for further characterizing the granulari annotations that capture the vessel pulsation characteristics, assisting in detecting ocular diseases that require fine-grained recognition of hemodynamic fluctuation.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 817:\n",
      "Text: In application, our dataset exhibits a significant domain shift with respect to data captured by bench-top devices, thus posing great chaprovide evaluation metrics and benchmark results on our dataset, reflecting both the potential and challenges it offers for vessel segmentation tasks.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 818:\n",
      "Text: This study addressed the complex task of sentiment analysis on a dataset of 119,988 original tweets from Weibo using a Convolutional Neural Network (CNN), offering a new approach to Natural Language Processing (NLP).\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 819:\n",
      "Text: The data, sourced from Baidu's PaddlePaddle AI platform, were meticulously preprocessed, tokenized, and categorized based on sentiment labels.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 820:\n",
      "Text: A CNN-based model was utilized, leveraging word embeddings for feature extraction, and trained to perform sentiment classification.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 821:\n",
      "Text: The model achieved a macro-average F1-score of approximately 0.73 on the test set, showing balanced performance across positive, neutral, and negative sentiments.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 822:\n",
      "Text: The findings underscore the effectiveness of CNNs for sentiment analysis tasks, with implications for practical applications in social media analysis, market research, and policy studies.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 823:\n",
      "Text: The complete experimental content and code have been made publicly available on the Kaggle data platform for further research and development.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 824:\n",
      "Text: Future work may involve exploring different architectures, such as Recurrent Neural Networks (RNN) or transformers, or using more complex pre-trained models like BERT, to further improve the model's ability to understand linguistic nuances and context.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 825:\n",
      "Text: To address the scarcity of annotated corpora with realistic multi-issue negotiation dialogues, we use GPT-3 to build GPT-Negochat, a synthesized dataset that we make publicly available.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 826:\n",
      "Text: Automated negotiation support systems aim to help human negotiators reach more favorable outcomes in multi-issue negotiations (e.g., an employer and a candidate negotiating over issues such as salary, hours, and promotions before a job offer).\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 827:\n",
      "Text: To be successful, these systems must accurately track agreements reached by participants in real-time.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 828:\n",
      "Text: Existing approaches either focus on task-oriented dialogues or produce unstructured outputs, rendering them unsuitable for this objective.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 829:\n",
      "Text: Our work introduces the novel task of agreement tracking for two-party multi-issue negotiations, which requires continuous monitoring of agreements within a structured state space.  \n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 830:\n",
      "Text: We present a strong initial baseline for our task by transfer-learning a T5 model trained on the MultiWOZ 2.4 corpus.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 831:\n",
      "Text: Pre-training T5-small and T5-base on MultiWOZ 2.4's DST task enhances results by 21% and 9% respectively over training solely on GPT-Negochat.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 832:\n",
      "Text: We validate our method's sample-efficiency via smaller training subset experiments.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 833:\n",
      "Text: By releasing GPT-Negochat and our baseline models, we aim to encourage further research in multi-issue negotiation dialogue agreement tracking.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 834:\n",
      "Text: This work presents WaterScenes, the first multi-task 4D radar-camera fusion dataset for autonomous driving on water surfaces\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 835:\n",
      "Text: Focusing on typical static and dynamic objects on water surfaces, we label the camera images and radar point clouds at pixel-level and point-level, respectively\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 836:\n",
      "Text: WaterScenes dataset is public on https://waterscenes.github.io.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 837:\n",
      "Text: Autonomous driving on water surfaces plays an essential role in executing hazardous and time-consuming missions, such as maritime surveillance, survivors rescue, environmental monitoring, hydrography mapping and waste cleaning. .\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 838:\n",
      "Text: Equipped with a 4D radar and a monocular camera, our Unmanned Surface Vehicle (USV) proffers all-weather solutions for discerning object-related information, including color, shape, texture, range, velocity, azimuth, and elevation.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 839:\n",
      "Text: Focusing on typical static and dynamic objects on water surfaces, we label the camera images and radar point clouds at pixelprovide annotations for free-space segmentation and waterline segmentation.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 840:\n",
      "Text: Leveraging the multi-task and multi-modal data, we conduct numerous experiments on the single modality of radar and camera, as well as the fused modalities.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 841:\n",
      "Text: Results demonstrate that 4D radar-camera fusion can considerably enhance the robustness of perception on water surfaces, especially in adverse lighting and weather conditions.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 842:\n",
      "Text: This paper presents the introduction of a framework called \\textit{Ashaar} https://github.com/ARBML/Ashaar, which encompasses a collection of datasets and pre-trained models designed specifically for the analysis and generation of Arabic poetry.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 843:\n",
      "Text: Furthermore, as part of this endeavor, we provide four datasets: one for poetry generation, another for diacritization, and two for Arudi-style prediction.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 844:\n",
      "Text: These datasets aim to facilitate research and development in the field of Arabic poetry by enabling researchers and enthusiasts to delve into the nuances of this rich literary tradition.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 845:\n",
      "Text: Poetry holds immense significance within the cultural and traditional fabric of any nation.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 846:\n",
      "Text: It serves as a vehicle for poets to articulate their emotions, preserve customs, and convey the essence of their culture.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 847:\n",
      "Text: Arabic poetry is no exception, having played a cherished role in the heritage of the Arabic community throughout history and maintaining its relevance in the present era.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 848:\n",
      "Text: Typically, comprehending Arabic poetry necessitates the expertise of a linguist who can analyze its content and assess its quality.  \n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 849:\n",
      "Text: The pipeline established within our proposed approach encompasses various aspects of poetry, such as meter, theme, and era classification.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 850:\n",
      "Text: It also incorporates automatic poetry diacritization, enabling more intricate analyses like automated extraction of the \\textit{Arudi} style.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 851:\n",
      "Text: Additionally, we explore the feasibility of generating conditional poetry through the pre-training of a character-based GPT model.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 852:\n",
      "Text: These datasets aim to facilitate research and development in the field of Arabic poetry b\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 853:\n",
      "Text: We build LogBench, the first logging statement generation dataset.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 854:\n",
      "Text: Automated logging statement generation techniques facilitate developers in writing appropriate logging statements that document software behaviors.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 855:\n",
      "Text: Current retrieval-based and learning-based logging methods fail to provide accurate logging statements in complex software.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 856:\n",
      "Text: Although existing large language models (LLMs) might be a good fit for the task due to their great success in natural language generation and programming language comprehension, their effectiveness and generalization capabilities have not been explored.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 857:\n",
      "Text: To this end, this paper performs the first extensive study on applying LLMs for logging statement generation.  \n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 858:\n",
      "Text: On LogBench, we evaluate the effectiveness and generalization capabilities of eight state-of-the-art LLMs, which include general-purpose and code-specific models ranging from 60M to 175B in size.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 859:\n",
      "Text: Specifically, we evaluate LLM's logging effectiveness by studying 1) their ability to decide logging ingredients, 2) the impact of the internal characteristics of LLMs, and 3) the influence of external factors.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 860:\n",
      "Text: We further evaluate LLM's logging generalization capabilities using unseen data derived from code transformation techniques.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 861:\n",
      "Text: Our study demonstrates that existing LLMs fall short of practical requirements for generating proper logging statement texts.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 862:\n",
      "Text: We also disclose the impact of internal characteristics and external factors for LLMs in automated logging.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 863:\n",
      "Text: In addition, we observe that existing LLMs cannot generalize to logging unseen code, revealing their unsatisfactory generalization capabilities.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 864:\n",
      "Text: Based on our findings, we further discuss three implications that can enhance logging statement generation in the future, such as developing a unified metric for logging quality, incorporating shareable code knowledge into LLMs, and devising suitable prompts.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 865:\n",
      "Text: To address this gap, we introduce the Brazilian Leading Universities Entrance eXams (BLUEX), a dataset of entrance exams from the two leading universities in Brazil: UNICAMP and USP\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 866:\n",
      "Text: The dataset includes annotated metadata for evaluating the performance of NLP models on a variety of subjects\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 867:\n",
      "Text: The dataset is also annotated to indicate the position of images in each question, providing a valuable resource for advancing the state-of-the-art in multimodal language understanding and reasoning.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 868:\n",
      "Text: The data and relevant code can be found at https://github.com/Portuguese-Benchmark-Datasets/BLUEX\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 869:\n",
      "Text: One common trend in recent studies of language models (LMs) is the use of standardized tests for evaluation.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 870:\n",
      "Text: However, despite being the fifth most spoken language worldwide, few such evaluations have been conducted in Portuguese.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 871:\n",
      "Text: This is mainly due to the lack of high-quality datasets available to the community for carrying out evaluations in Portuguese. .\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 872:\n",
      "Text: The dataset includes annotated metadata for evaluating the performance of NLP models on a variety of subjects.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 873:\n",
      "Text: Furthermore, BLUEX includes a collection of recently administered examnnotated to indicate the position of images in each question, providing a valuable resource for advancing the state-of-the-art in multimodal language understanding and reasoning.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 874:\n",
      "Text: We describe the creation and characteristics of BLUEX and establish a benchmark through exund at https://github.com/Portuguese-Benchmark-Datasets/BLUEX\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 875:\n",
      "Text: We produce 1,304 sentence pairs by modifying 15 examples from the SICK dataset (Marelli et al., 2014).\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 876:\n",
      "Text: We introduce a synthetic dataset called Sentences Involving Complex Compositional Knowledge (SICCK) and a novel analysis that investigates the performance of Natural Language Inference (NLI) models to understand compositionality in logic.  \n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 877:\n",
      "Text: To this end, we modify the original texts using a set of phrases - modifiers that correspond to universal quantifiers, existential quantifiers, negation, and other concept modifiers in Natural Logic (NL) (MacCartney, 2009).\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 878:\n",
      "Text: We use these phrases to modify the subject, verb, and object parts of the premise and hypothesis.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 879:\n",
      "Text: Lastly, we annotate these modified texts with the corresponding entailment labels following NL rules.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 880:\n",
      "Text: We conduct a preliminary verification of how well the change in the structural and semantic composition is captured by neural NLI models, in both zero-shot and fine-tuned scenarios.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 881:\n",
      "Text: We found that the performance of NLI models under the zero-shot setting is poor, especially for modified sentences with negation and existential quantifiers.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 882:\n",
      "Text: After fine-tuning this dataset, we observe that models continue to perform poorly over negation, existential and universal modifiers.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 883:\n",
      "Text: Age and gender recognition in the wild is a highly challenging task: apart from the variability of conditions, pose complexities, and varying image quality, there are cases where the face is partially or completely occluded.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 884:\n",
      "Text: We present MiVOLO (Multi Input VOLO), a straightforward approach for age and gender estimation using the latest vision transformer.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 885:\n",
      "Text: Our method integrates both tasks into a unified dual input/output model, leveraging not only facial information but also person image data.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 886:\n",
      "Text: This improves the generalization ability of our model and enables it to deliver satisfactory results even when the face is not visible in the image.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 887:\n",
      "Text: To evaluate our proposed model, we conduct experiments on four popular benchmarks and achieve state-of-the-art performance, while demonstrating real-time processing capabilities.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 888:\n",
      "Text: The ground truth annotations for this benchmark have been meticulously generated by human annotators, resulting in high accuracy answers due to the smart aggregation of votes.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 889:\n",
      "Text: Furthermore, we compare our model's age recognition performance with human-level accuracy and demonstrate that it significantly outperforms humans across a majority of age ranges.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 890:\n",
      "Text: Finally, we grant public access to our models, along with the code for validation and inference.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 891:\n",
      "Text: To bridge this gap, we introduce a dataset, SAGC-A68, which comprises access graphs automatically generated from 68 digital 3D models of space layouts of apartment buildings\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 892:\n",
      "Text: This graph-based dataset is well-suited for developing GDL models for space function and space element classification\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 893:\n",
      "Text: The dataset and code used in the experiment are available online.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 894:\n",
      "Text: The analysis of building models for usable area, building safety, and energy use requires accurate classification data of spaces and space elements.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 895:\n",
      "Text: To reduce input model preparation effort and errors, automated classification of spaces and space elements is desirable.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 896:\n",
      "Text: A barrier hindering the utilization of Graph Deep Learning (GDL) methods to space function and space element classification is a lack of suitable datasets. .\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 897:\n",
      "Text: This graph-based dataset is well-suited for developing GDL models for space function and space element classification.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 898:\n",
      "Text: To demonstrate the potential of the dataset, we employ.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 899:\n",
      "Text: https://doi.org/10.5281/zenodo.7805872, https://github.com/A2Amir/SAGC-A68.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 900:\n",
      "Text: We position the contribution of this work in two folds: (i)-we collect and curate nearly 2k high-quality tabular datasets, each of which is guaranteed to possess clear semantics, clean labels, and other necessary meta information.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 901:\n",
      "Text: Tabular data -- also known as structured data -- is one of the most common data forms in existence, thanks to the stable development and scaled deployment of database systems in the last few decades.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 902:\n",
      "Text: At present however, despite the blast brought by large pre-trained models in other domains such as ChatGPT or SAM, how can we extract common knowledge across tables at a scale that may eventually lead to generalizable representation for tabular data remains a full blank.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 903:\n",
      "Text: Indeed, there have been a few works around this topic.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 904:\n",
      "Text: Most (if not all) of them are limited in the scope of a single table or fixed form of a schema.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 905:\n",
      "Text: In this work, we first identify the crucial research challenges behind tabular data pre-training, particularly towards the cross-table scenario.  \n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 906:\n",
      "Text: (ii)-we propose a novel framework that allows cross-table pre-training dubbed as CT-BERT.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 907:\n",
      "Text: Noticeably, in light of pioneering the scaled cross-table training, CT-BERT is fully compatible with both supervised and self-supervised schemes, where the specific instantiation of CT-BERT is very much dependent on the downstream tasks.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 908:\n",
      "Text: We further propose and implement a contrastive-learning-based and masked table modeling (MTM) objective into CT-BERT, that is inspired from computer vision and natural language processing communities but sophistically tailored to tables.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 909:\n",
      "Text: The extensive empirical results on 15 datasets demonstrate CT-BERT's state-of-the-art performance, where both its supervised and self-supervised setups significantly outperform the prior approaches.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 910:\n",
      "Text: The dataset is publicly available at: https://huggingface.co/datasets/Soyoung/HistRED under CC BY-NC-ND 4.0 license.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 911:\n",
      "Text: Despite the extensive applications of relation extraction (RE) tasks in various domains, little has been explored in the historical context, which contains promising data across hundreds and thousands of years.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 912:\n",
      "Text: To promote the historical RE research, we present HistRED constructed from Yeonhaengnok.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 913:\n",
      "Text: Yeonhaengnok is a collection of records originally written in Hanja, the classical Chinese writing, which has later been translated into Korean.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 914:\n",
      "Text: HistRED provides bilingual annotations such that RE can be performed on Korean and Hanja texts.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 915:\n",
      "Text: In addition, HistRED supports various self-contained subtexts with different lengths, from a sentence level to a document level, supporting diverse context settings for researchers to evaluate the robustness of their RE models.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 916:\n",
      "Text: To demonstrate the usefulness of our dataset, we propose a bilingual RE model that leverages both Korean and Hanja contexts to predict relations between entities.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 917:\n",
      "Text: Our model outperforms monolingual baselines on HistRED, showing that employing multiple language contexts supplements the RE predictions.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 918:\n",
      "Text: This research paper introduces a new curated dataset and a deep learning-based approach to solve these problems using convolutional neural networks (CNNs) and comprehensive data processing techniques.\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 919:\n",
      "Text: Our dataset includes curated images and diverse channel bands from Sentinel, Landsat, VIIRS, and MODIS satellites.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 920:\n",
      "Text: We design the dataset considering different spatial and temporal resolution requirements\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 921:\n",
      "Text: Our code, models and dataset are open source: https://github.com/h2oai/cvpr-multiearth-deforestation-segmentation\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 922:\n",
      "Text: Deforestation estimation and fire detection in the Amazon forest poses a significant challenge due to the vast size of the area and the limited accessibility.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 923:\n",
      "Text: However, these are crucial problems that lead to severe environmental consequences, including climate change, global warming, and biodiversity loss.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 924:\n",
      "Text: To effectively address this problem, multimodal satellite imagery and remote sensing offer a promising solution for estimating deforestation and detecting wildfire in the Amazonia region.  \n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 925:\n",
      "Text: We design the dataset considering different spatial and temporal resolution requiremeimages from the region.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 926:\n",
      "Text: Our code, models and dataset are open source: https://github.com/h2oai/cvpr-multiearth-defo\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 927:\n",
      "Text: We address this limitation by constructing MultiVENT, a dataset of multilingual, event-centric videos grounded in text documents across five target languages\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 928:\n",
      "Text: Everyday news coverage has shifted from traditional broadcasts towards a wide range of presentation formats such as first-hand, unedited video footage.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 929:\n",
      "Text: Datasets that reflect the diverse array of multimodal, multilingual news sources available online could be used to teach models to benefit from this shift, but existing news video datasets focus on traditional news broadcasts produced for English-speaking audiences. .\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 930:\n",
      "Text: MultiVENT includes both news broadcast videos and non-professional event footage, which we use to analyze the state of online news videos and how they can be leveraged to build robust, factually accurate models.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 931:\n",
      "Text: Finally, we provide a model for complex, multilingual video retrieval to serve as a baseline for information retrieval using MultiVENT.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 932:\n",
      "Text: To address this, we establish a large-scale dataset, namely the Tuberculosis X-ray (TBX11K) dataset, which contains 11,200 chest X-ray (CXR) images with corresponding bounding box annotations for TB areas\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 933:\n",
      "Text: This dataset enables the training of sophisticated detectors for high-quality CTD\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 934:\n",
      "Text: The data, code, and models will be released.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 935:\n",
      "Text: Tuberculosis (TB) is a major global health threat, causing millions of deaths annually.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 936:\n",
      "Text: Although early diagnosis and treatment can greatly improve the chances of survival, it remains a major challenge, especially in developing countries.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 937:\n",
      "Text: Recently, computer-aided tuberculosis diagnosis (CTD) using deep learning has shown promise, but progress is hindered by limited training data. .\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 938:\n",
      "Text: This dataset enables the training of sophisticated detectors for high-quality CTD.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 939:\n",
      "Text: Furthermore, we propose a strong baseline, SymFormer, for simultaneous CXR image classification and TB infection area dete the bilateral symmetry property of CXR images for learning discriminative features.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 940:\n",
      "Text: Since CXR images may not strictly adhere to the bilateral symmetry property, we also propose Symmetric Positional Encoding (SPE) to facilitate SymAttention through feature recalibration.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 941:\n",
      "Text: To promote future research on CTD, we build a benchmark by introducing evaluation metrics, evaluating baseline models reformed from existing detectors, and running an online challenge.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 942:\n",
      "Text: Experiments show that SymFormer achieves state-of-the-art performance on the TBX11K dataset.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 943:\n",
      "Text: This paper presents an end-to-end methodology for collecting datasets to recognize handwritten English alphabets by utilizing Inertial Measurement Units (IMUs) and leveraging the diversity present in the Indian writing style\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 944:\n",
      "Text: The IMUs are utilized to capture the dynamic movement patterns associated with handwriting, enabling more accurate recognition of alphabets.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 945:\n",
      "Text: The Indian context introduces various challenges due to the heterogeneity in writing styles across different regions and languages.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 946:\n",
      "Text: Some preliminary experimental results demonstrate the effectiveness of the dataset in accurately recogpattern recognition and offers valuable insights for developing improved systems for handwriting recognition, particularly in diverse linguistic and cultural contexts.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 947:\n",
      "Text: We fill this gap by semi-automatically creating an NLI dataset for spatial reasoning, called SpaceNLI\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 948:\n",
      "Text: While many natural language inference (NLI) datasets target certain semantic phenomena, e.g., negation, tense & aspect, monotonicity, and presupposition, to the best of our knowledge, there is no NLI dataset that involves diverse types of spatial expressions and reasoning. .\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 949:\n",
      "Text: We test several SOTA NLI systems on SpaceNLI to gauge the complexity of the dataset and the system's capacity for spatial reasoning.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 950:\n",
      "Text: Moreover, we introduce a Pattern Accuracy and argue that it is a more reliable and stricter measure than the accuracy for evaluating a system's performance on pattern-based generated data samples.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 951:\n",
      "Text: Based on the evaluation results we find that the systems obtain moderate results on the spatial NLI problems but lack consistency per inference pattern.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 952:\n",
      "Text: The results also reveal that non-projective spatial inferences (especially due to the \"between\" preposition) are the most challenging ones.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 953:\n",
      "Text: This paper presents the FormAI dataset, a large collection of 112,000 AI-generated compilable and independent C programs with vulnerability classification\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 954:\n",
      "Text: We make the source code available for the 112,000 programs, accompanied by a comprehensive list detailing the vulnerabilities detected in each individual program\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 955:\n",
      "Text: We introduce a dynamic zero-shot prompting technique, constructed to spawn a diverse set of programs utilizing Large Language Models (LLMs).\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 956:\n",
      "Text: Some programs handle complicated tasks such as networkion.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 957:\n",
      "Text: This is accomplished by employing a formal verification method using the Efficient SMT-based Bounded Model Checker (ESBMC), which performs model checking, abstract interpretation, constraint programming, and satisfiability modulo theories, to reason over safety/security properties in programs.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 958:\n",
      "Text: This approach definitively detects vulnerabilities and offers a formal model known as a counterexample, thus eliminating the possibility of generating false positive reports.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 959:\n",
      "Text: Furthermore, we have associated the identified vulnerabilities with relevant Common Weakness Enumeration (CWE) numbers.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 960:\n",
      "Text: We make the source code available for the 112,000 programs, accompanied by a comprehensive list detailing the vulnerabilities detected in each individual program including location and function name, which makes the dataset ideal to train LLMs and machi\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 961:\n",
      "Text: In this work, we construct two datasets to address this issue\n",
      "Label: 1\n",
      "--------------------------------------------------\n",
      "Entry 962:\n",
      "Text: We introduce a new conversation head generation benchmark for synthesizing behaviors of a single interlocutor in a face-to-face conversation.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 963:\n",
      "Text: The capability to automatically synthesize interlocutors which can participate in long and multi-turn conversations is vital and offer benefits for various applications, including digital humans, virtual agents, and social robots.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 964:\n",
      "Text: While existing research primarily focuses on talking head generation (one-way interaction), hindering the ability to create a digital human for conversation (two-way) interaction due to the absence of listening and interaction parts.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 965:\n",
      "Text: , ``ViCo'' for independent talking and listening head generation tasks at the sentence level, and ``ViCo-X'', for synthesizing interlocutors in multi-turn conversational scenarios.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 966:\n",
      "Text: Based on ViCo and ViCo-X, we define three novel tasks targeting the interaction modeling during the face-to-face conversation: 1) responsive listening head generation making listeners respond actively to the speaker with non-verbal signals, 2) expressive talking head generation guiding speakers to be aware of listeners' behaviors, and 3) conversational head generation to integrate the talking/listening ability in one interlocutor.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 967:\n",
      "Text: Along with the datasets, we also propose corresponding baselierate responsive and vivid agents that can collaborate with real person to fulfil the whole conversation.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 968:\n",
      "Text: Project page: https://vico.solutions/.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 969:\n",
      "Text: The digitization of documents allows for wider accessibility and reproducibility.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 970:\n",
      "Text: While automatic digitization of document layout and text content has been a long-standing focus of research, this problem in regard to graphical elements, such as statistical plots, has been under-explored.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 971:\n",
      "Text: In this paper, we introduce the task of fine-grained visual understanding of mathematical graphics and present the Line Graphics (LG) dataset, which includes pixel-wise annotations of 5 coarse and 10 fine-grained categories.  \n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 972:\n",
      "Text: To benchmark our LG dataset, we explore 7 state-of-the-art models.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 973:\n",
      "Text: To foster further research on the digitization of statistical graphs, we will make the dataset, code, and model\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 974:\n",
      "Text: While the general machine learning (ML) community has benefited from public datasets, tasks, and models, the progress of ML in healthcare has been hampered by a lack of such shared assets.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 975:\n",
      "Text: The success of foundation models creates new challenges for healthcare ML by requiring access to shared pretrained models to validate performance benefits.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 976:\n",
      "Text: We help address these challenges through three contributions.  \n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 977:\n",
      "Text: Unlike MIMIC-III/IV and other popular EHR datasets, EHRSHOT is longitudinal and not restricted to ICU/ED patients.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 978:\n",
      "Text: Second, we publish the weights of a 141M parameter clinical foundation model pretrained on the structured EHR data of 2.57M patients.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 979:\n",
      "Text: We are one of the first to fully release such a model for coded EHR data; in contrast, most prior models released for clinical data (e.g. GatorTron, ClinicalBERT) only work with unstructured text and cannot process the rich, structured data within an EHR.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 980:\n",
      "Text: We provide an end-to-end pipeline for the community to validate and build upon its performance.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 981:\n",
      "Text: Third, we define 15 few-shot clinical prediction tasks, enabling evaluation of foundation models on benefits such as sample efficiency and task adaption.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 982:\n",
      "Text: The code to reproduce our results, as well as the model and dataset (via a research data use agreement), are available at our Github repo here: https://github.com/som\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 983:\n",
      "Text: Generative latent diffusion models have been established as state-of-the-art in data generation.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 984:\n",
      "Text: One promising application is generation of realistic synthetic medical imaging data for open data sharing without compromising patient privacy.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 985:\n",
      "Text: Despite the promise, the capacity of such models to memorize sensitive patient training data and synthesize samples showing high resemblance to training data samples is relatively unexplored.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 986:\n",
      "Text: Here, we assess the memorization capacity of 3D latent diffusion models on photon-counting coronary computed tomography angiography and knee magnetic resonance imaging datasets.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 987:\n",
      "Text: To detect potential memorization of training samples, we utilize self-supervised models based on contrastive learning.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 988:\n",
      "Text: Our results suggest that such latent diffusion models indeed memorize training data, and there is a dire need for devising strategies to mitigate memorization.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 989:\n",
      "Text: Visual Question Answering (VQA) models aim to answer natural language questions about given images.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 990:\n",
      "Text: Due to its ability to ask questions that differ from those used when training the model, medical VQA has received substantial attention in recent years.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 991:\n",
      "Text: However, existing medical VQA models typically focus on answering questions that refer to an entire image rather than where the relevant content may be located in the image.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 992:\n",
      "Text: Consequently, VQA models are limited in their interpretability power and the possibility to probe the model about specific image regions.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 993:\n",
      "Text: This paper proposes a novel approach for medical VQA that addresses this limitation by developing a model that can answer questions about image regions while considering the context necessary to answer the questions.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 994:\n",
      "Text: Artificial intelligence applications enable farmers to optimize crop growth and production while reducing costs and environmental impact.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 995:\n",
      "Text: Computer vision-based algorithms in particular, are commonly used for fruit segmentation, enabling in-depth analysis of the harvest quality and accurate yield estimation.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 996:\n",
      "Text: In this paper, we propose TomatoDIFF, a novel diffusion-based model for semantic segmentation of on-plant tomatoes.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 997:\n",
      "Text: When evaluated against other competitive methods, our model demonstrates state-of-the-art (SOTA) performance, even in challenging environments with highly occluded fruits.  \n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 998:\n",
      "Text: Large language models~(LLMs) obtain instruction-following capability through instruction-finetuning (IFT) on supervised instruction/response data.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 999:\n",
      "Text: However, widely used IFT datasets (e.g., Alpaca's 52k data) surprisingly contain many low-quality instances with incorrect or irrelevant responses, which are misleading and detrimental to IFT.\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Entry 1000:\n",
      "Text: In this paper, we propose a simple and effective data selection strategy that automatically identifies and removes low-quality data using a strong LLM (e.g., ChatGPT).\n",
      "Label: 0\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Print the first 1000 entries of texts and labels\n",
    "for i, (text, label) in enumerate(zip(texts[:1000], labels[:1000]), start=1):\n",
    "    print(f\"Entry {i}:\")\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Label: {label}\")\n",
    "    print(\"-\" * 50)  # Separator for better readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "zeroshot_classifier = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/ModernBERT-large-zeroshot-v2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling the model with `torch.compile` and using a `torch.mps` device is not supported. Falling back to non-compiled mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 30s, sys: 14.1 s, total: 1min 44s\n",
      "Wall time: 1min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "outputs = zeroshot_classifier(texts, [\"new dataset\"], hypothesis_template=hypothesis_template, multi_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.8940964398377648)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_zs = np.array([o[\"scores\"] for o in outputs]).reshape(-1) > 0.5\n",
    "np.mean(pred_zs == labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
