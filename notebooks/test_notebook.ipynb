{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Repository setup for portable, repo-relative paths\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "def _find_repo_root(start: Path | None = None) -> Path:\n",
    "    start = (start or Path.cwd()).resolve()\n",
    "    for candidate in [start, *start.parents]:\n",
    "        if (candidate / \"pyproject.toml\").exists():\n",
    "            return candidate\n",
    "    return start\n",
    "\n",
    "REPO_ROOT = _find_repo_root()\n",
    "if str(REPO_ROOT / \"src\") not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_ROOT / \"src\"))\n",
    "\n",
    "from dx_chat_entropy.paths import get_paths\n",
    "PATHS = get_paths(REPO_ROOT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ModerBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U sentence-transformers\n",
    "%pip install srsly\n",
    "#%pip install triton - dont' need with mac m2\n",
    "%pip install git+https://github.com/huggingface/transformers.git # install new huggingface transformers\n",
    "# [ ] At some point - the need for the newest transformers won't be required once modernBERT is integrated into the main release\n",
    "\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/metal.html\n",
    "\n",
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(f\"Using device: {device}\")\n",
    "else:\n",
    "    print(\"MPS backend not available. Falling back to CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Download from the ðŸ¤— Hub\n",
    "model = SentenceTransformer(\"joe32140/ModernBERT-base-msmarco\")\n",
    "# Note - for my medical project, seems like maybe using a different model would be better\n",
    "# also may need to retinker the sentence transformer part\n",
    "\n",
    "# Run inference\n",
    "sentences = [\n",
    "    'what county is hayden in',\n",
    "    \"Hayden is a city in Kootenai County, Idaho, United States. Located in the northern portion of the state, just north of Coeur d'Alene, its population was 13,294 at the 2010 census.\",\n",
    "    \"According to the United States Census Bureau, the city has a total area of 9.61 square miles (24.89 km2), of which 9.60 square miles (24.86 km2) is land and 0.01 square miles (0.03 km2) is water. It lies at the southwestern end of Hayden Lake, and the elevation of the city is 2,287 feet (697 m) above sea level. Hayden is located on U.S. Route 95 at the junction of Route 41. It is also four miles (6 km) north of Interstate 90 and Coeur d'Alene. The Coeur d'Alene airport is northwest of Hayden.\",\n",
    "]\n",
    "\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings.shape)\n",
    "# [3, 768]\n",
    "\n",
    "# Get the similarity scores for the embeddings\n",
    "similarities = model.similarity(embeddings, embeddings)\n",
    "print(similarities.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers[sentencepiece]\n",
    "from transformers import pipeline\n",
    "\n",
    "text = \"Angela Merkel is a politician in Germany and leader of the CDU\"\n",
    "hypothesis_template = \"This text is about {}\" # Based on this interview, does the patient have {} sign or symptom\n",
    "classes_verbalized = [\"politics\", \"economy\", \"entertainment\", \"environment\"]\n",
    "\n",
    "zeroshot_classifier = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/deberta-v3-large-zeroshot-v2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "zeroshot_classifier(text, classes_verbalized, hypothesis_template=hypothesis_template, multi_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/koaning/arxiv-frontpage/refs/heads/main/data/annot/new-dataset.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import srsly\n",
    "inputs = list(srsly.read_jsonl(\"new-dataset.jsonl\"))\n",
    "texts = [d[\"text\"] for d in inputs]\n",
    "labels = [d[\"cats\"][\"new-dataset\"] for d in inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "outputs = zeroshot_classifier(texts, [\"new dataset\"], hypothesis_template=hypothesis_template, multi_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pred_zs = np.array([o[\"scores\"] for o in outputs]).reshape(-1) > 0.5\n",
    "np.mean(pred_zs == labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroshot_classifier = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/ModernBERT-large-zeroshot-v2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "zeroshot_classifier(text, classes_verbalized, hypothesis_template=hypothesis_template, multi_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "outputs = zeroshot_classifier(texts, [\"new dataset\"], hypothesis_template=hypothesis_template, multi_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_zs = np.array([o[\"scores\"] for o in outputs]).reshape(-1) > 0.5\n",
    "np.mean(pred_zs == labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroshot_classifier = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/ModernBERT-base-zeroshot-v2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "outputs = zeroshot_classifier(texts, [\"new dataset\"], hypothesis_template=hypothesis_template, multi_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_zs = np.array([o[\"scores\"] for o in outputs]).reshape(-1) > 0.5\n",
    "np.mean(pred_zs == labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED_GLOBAL = 42\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(f\"Using device: {device}\")\n",
    "else:\n",
    "    print(\"MPS backend not available. Falling back to CPU.\")\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "import srsly\n",
    "\n",
    "\n",
    "np.random.seed(SEED_GLOBAL)\n",
    "torch.manual_seed(SEED_GLOBAL)\n",
    "random.seed(SEED_GLOBAL)\n",
    "\n",
    "import llm\n",
    "\n",
    "#Anothe\n",
    "\"\"\"\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import re\n",
    "import time\n",
    "\n",
    "import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import ClassLabel\n",
    "from datasets import load_dataset, load_metric, Dataset, DatasetDict, concatenate_datasets, list_metrics\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score, precision_recall_fscore_support, accuracy_score, classification_report\n",
    "\n",
    "import gc\n",
    "from accelerate.utils import release_memory\n",
    "\n",
    "import wandb\n",
    "import json\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "\n",
    "from mdutils import MdUtils\n",
    "\n",
    "# suppress unnecessary tokenizer warning https://github.com/huggingface/transformers/issues/14285\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API key from environment only (do not read shell config or print secrets)\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY is not set in environment.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify torch backend\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "# Path to your PDF file\n",
    "pdf_path = str(PATHS.raw / 'chatbot_transcripts' / 'Intermtn MS4 1 Transcript.pdf')\n",
    "\n",
    "# Initialize PDF reader\n",
    "reader = PdfReader(pdf_path)\n",
    "\n",
    "# Extract text from each page\n",
    "all_text = \"\"\n",
    "for page in reader.pages:\n",
    "    all_text += page.extract_text()\n",
    "\n",
    "# Output the text\n",
    "print(all_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison to using OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from markitdown import MarkItDown\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "md = MarkItDown(llm_client=client, llm_model=\"gpt-4o-mini\")\n",
    "supported_extensions = ('.pptx', '.docx', '.pdf', '.jpg', '.jpeg', '.png')\n",
    "files_to_convert = [f for f in os.listdir('.') if f.lower().endswith(supported_extensions)]\n",
    "for file in files_to_convert:\n",
    "    print(f\"\\nConverting {file}...\")\n",
    "    try:\n",
    "        md_file = os.path.splitext(file)[0] + '.md'\n",
    "        result = md.convert(file)\n",
    "        with open(md_file, 'w') as f:\n",
    "            f.write(result.text_content)\n",
    "        \n",
    "        print(f\"Successfully converted {file} to {md_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting {file}: {str(e)}\")\n",
    "\n",
    "print(\"\\nAll conversions completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis_template = \"This text is about {}\" # Based on this interview, does the patient have {} sign or symptom\n",
    "inputs = list(srsly.read_jsonl(\"new-dataset.jsonl\"))\n",
    "texts = [d[\"text\"] for d in inputs]\n",
    "labels = [d[\"cats\"][\"new-dataset\"] for d in inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 1000 entries of texts and labels\n",
    "for i, (text, label) in enumerate(zip(texts[:1000], labels[:1000]), start=1):\n",
    "    print(f\"Entry {i}:\")\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Label: {label}\")\n",
    "    print(\"-\" * 50)  # Separator for better readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroshot_classifier = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/ModernBERT-large-zeroshot-v2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "outputs = zeroshot_classifier(texts, [\"new dataset\"], hypothesis_template=hypothesis_template, multi_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_zs = np.array([o[\"scores\"] for o in outputs]).reshape(-1) > 0.5\n",
    "np.mean(pred_zs == labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostly AI synthetic data engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from mostlyai import engine\n",
    "\n",
    "# set up workspace\n",
    "ws = PATHS.archive / \"legacy_external\" / \"ws-tabular-flat\"\n",
    "\n",
    "# load original data\n",
    "url = \"https://github.com/mostly-ai/public-demo-data/raw/refs/heads/dev/census\"\n",
    "trn_df = pd.read_csv(f\"{url}/census.csv.gz\")\n",
    "\n",
    "# execute the engine steps\n",
    "engine.split(                         # split data as PQT files for `trn` + `val` to `{ws}/OriginalData/tgt-data`\n",
    "  workspace_dir=ws,\n",
    "  tgt_data=trn_df,\n",
    "  model_type=\"TABULAR\",\n",
    ")\n",
    "engine.analyze(workspace_dir=ws)      # generate column-level statistics to `{ws}/ModelData/tgt-stats/stats.json`\n",
    "engine.encode(workspace_dir=ws)       # encode training data to `{ws}/OriginalData/encoded-data`\n",
    "engine.train(                         # train model and store to `{ws}/ModelStore/model-data`\n",
    "    workspace_dir=ws,\n",
    "    max_training_time=1,              # limit TRAIN to 1 minute for demo purposes\n",
    ")\n",
    "engine.generate(workspace_dir=ws)     # use model to generate synthetic samples to `{ws}/SyntheticData`\n",
    "pd.read_parquet(ws / \"SyntheticData\") # load synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mostlyai.sdk import MostlyAI\n",
    "\n",
    "# 1) Initialize the SDK in local or client mode\n",
    "mostly = MostlyAI(local=True)\n",
    "# mostly = MostlyAI(base_url='https://app.mostly.ai', api_key='YOUR_API_KEY')\n",
    "\n",
    "# 2) Load your original data\n",
    "trn_df = pd.read_csv('https://github.com/mostly-ai/public-demo-data/raw/dev/census/census.csv.gz')\n",
    "\n",
    "# 3) Train a synthetic data generator\n",
    "g = mostly.train(name='census', data=trn_df)  # shorthand syntax for 1-table config\n",
    "\n",
    "# 4) Live probe small synthetic samples\n",
    "df_samples = mostly.probe(g, size=10)\n",
    "\n",
    "# 5) Generate a full synthetic dataset\n",
    "sd = mostly.generate(g, size=100_000)\n",
    "syn_df = sd.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plan - use markdown; explore querying OpenAI, then explore modernBERT\n",
    "\n",
    "\n",
    "# [ ] TODO:  figure out why this \n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "model_id = \"answerdotai/ModernBERT-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_id)\n",
    "\n",
    "text = \"The capital of France is [MASK].\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# To get predictions for the mask:\n",
    "masked_index = inputs[\"input_ids\"][0].tolist().index(tokenizer.mask_token_id)\n",
    "predicted_token_id = outputs.logits[0, masked_index].argmax(axis=-1)\n",
    "predicted_token = tokenizer.decode(predicted_token_id)\n",
    "print(\"Predicted token:\", predicted_token)\n",
    "# Predicted token:  Paris\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anthropic Calls - \n",
    "\n",
    "[ ] todo: still need to troubleshoot the API key and buy credits if going to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "import os \n",
    "\n",
    "# Access the API key\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "if not anthropic_api_key:\n",
    "    raise ValueError(\"ANTHROPIC_API_KEY environment variable not set!\")\n",
    "\n",
    "client = anthropic.Anthropic(api_key=anthropic_api_key)\n",
    "\n",
    "response = client.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20241022\",\n",
    "    max_tokens=1024,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"document\",\n",
    "                    \"source\": {\n",
    "                        \"type\": \"text\",\n",
    "                        \"media_type\": \"text/plain\",\n",
    "                        \"data\": \"The grass is green. The sky is blue.\"\n",
    "                    },\n",
    "                    \"title\": \"My Document\",\n",
    "                    \"context\": \"This is a trustworthy document.\",\n",
    "                    \"citations\": {\"enabled\": True}\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"What color is the grass and sky?\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code from Allen Downey giving an introduction to various regressions and displays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import basename, exists\n",
    "\n",
    "\"\"\"Supporting code for Elements of Data Science\n",
    "\n",
    "by Allen Downey\n",
    "\n",
    "MIT License\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import contextlib\n",
    "import gzip\n",
    "import io\n",
    "import re\n",
    "import textwrap\n",
    "import os\n",
    "\n",
    "from IPython.core.magic import register_cell_magic\n",
    "from IPython.core.magic_arguments import argument, magic_arguments, parse_argstring\n",
    "\n",
    "from os.path import basename, exists\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "\n",
    "\n",
    "# Make the figures smaller to save some screen real estate.\n",
    "# The figures generated for the book have DPI 400, so scaling\n",
    "# them by a factor of 4 restores them to the size in the notebooks.\n",
    "plt.rcParams[\"figure.dpi\"] = 75\n",
    "plt.rcParams[\"figure.figsize\"] = [6, 3.5]\n",
    "\n",
    "\n",
    "def wrap(obj):\n",
    "    for line in textwrap.wrap(str(obj), subsequent_indent=\"    \"):\n",
    "        print(line)\n",
    "\n",
    "\n",
    "class FixedWidthVariables(object):\n",
    "    \"\"\"Represents a set of variables in a fixed width file.\"\"\"\n",
    "\n",
    "    def __init__(self, variables, index_base=0):\n",
    "        \"\"\"Initializes.\n",
    "\n",
    "        variables: DataFrame\n",
    "        index_base: are the indices 0 or 1 based?\n",
    "\n",
    "        Attributes:\n",
    "        colspecs: list of (start, end) index tuples\n",
    "        names: list of string variable names\n",
    "        \"\"\"\n",
    "        self.variables = variables\n",
    "        self.colspecs = variables[[\"start\", \"end\"]] - index_base\n",
    "\n",
    "        # convert colspecs to a list of pair of int\n",
    "        self.colspecs = self.colspecs.astype(np.int).values.tolist()\n",
    "        self.names = variables[\"name\"]\n",
    "\n",
    "    def read_fixed_width(self, filename, **options):\n",
    "        \"\"\"Reads a fixed width ASCII file.\n",
    "\n",
    "        filename: string filename\n",
    "\n",
    "        returns: DataFrame\n",
    "        \"\"\"\n",
    "        df = pd.read_fwf(filename, colspecs=self.colspecs, names=self.names, **options)\n",
    "        return df\n",
    "\n",
    "\n",
    "def read_stata_dict(dct_file, **options):\n",
    "    \"\"\"Reads a Stata dictionary file.\n",
    "\n",
    "    dct_file: string filename\n",
    "    options: dict of options passed to open()\n",
    "\n",
    "    returns: FixedWidthVariables object\n",
    "    \"\"\"\n",
    "    type_map = dict(\n",
    "        byte=int, int=int, long=int, float=float, double=float, numeric=float\n",
    "    )\n",
    "\n",
    "    var_info = []\n",
    "    with open(dct_file, **options) as f:\n",
    "        for line in f:\n",
    "            match = re.search(r\"_column\\(([^)]*)\\)\", line)\n",
    "            if not match:\n",
    "                continue\n",
    "            start = int(match.group(1))\n",
    "            t = line.split()\n",
    "            vtype, name, fstring = t[1:4]\n",
    "            name = name.lower()\n",
    "            if vtype.startswith(\"str\"):\n",
    "                vtype = str\n",
    "            else:\n",
    "                vtype = type_map[vtype]\n",
    "            long_desc = \" \".join(t[4:]).strip('\"')\n",
    "            var_info.append((start, vtype, name, fstring, long_desc))\n",
    "\n",
    "    columns = [\"start\", \"type\", \"name\", \"fstring\", \"desc\"]\n",
    "    variables = pd.DataFrame(var_info, columns=columns)\n",
    "\n",
    "    # fill in the end column by shifting the start column\n",
    "    variables[\"end\"] = variables.start.shift(-1)\n",
    "    variables.loc[len(variables) - 1, \"end\"] = 0\n",
    "\n",
    "    dct = FixedWidthVariables(variables, index_base=1)\n",
    "    return dct\n",
    "\n",
    "\n",
    "def read_stata(dct_name, dat_name, **options):\n",
    "    \"\"\"Reads Stata files from the given directory.\n",
    "\n",
    "    dirname: string\n",
    "\n",
    "    returns: DataFrame\n",
    "    \"\"\"\n",
    "    dct = read_stata_dict(dct_name)\n",
    "    df = dct.read_fixed_width(dat_name, **options)\n",
    "    return df\n",
    "\n",
    "\n",
    "def sample_rows(df, nrows, replace=False):\n",
    "    \"\"\"Choose a sample of rows from a DataFrame.\n",
    "\n",
    "    df: DataFrame\n",
    "    nrows: number of rows\n",
    "    replace: whether to sample with replacement\n",
    "\n",
    "    returns: DataDf\n",
    "    \"\"\"\n",
    "    indices = np.random.choice(df.index, nrows, replace=replace)\n",
    "    sample = df.loc[indices]\n",
    "    return sample\n",
    "\n",
    "\n",
    "def resample_rows(df):\n",
    "    \"\"\"Resamples rows from a DataFrame.\n",
    "\n",
    "    df: DataFrame\n",
    "\n",
    "    returns: DataFrame\n",
    "    \"\"\"\n",
    "    return sample_rows(df, len(df), replace=True)\n",
    "\n",
    "\n",
    "def resample_rows_weighted(df, column=\"finalwgt\"):\n",
    "    \"\"\"Resamples a DataFrame using probabilities proportional to given column.\n",
    "\n",
    "    df: DataFrame\n",
    "    column: string column name to use as weights\n",
    "\n",
    "    returns: DataFrame\n",
    "    \"\"\"\n",
    "    weights = df[column].copy()\n",
    "    weights /= sum(weights)\n",
    "    indices = np.random.choice(df.index, len(df), replace=True, p=weights)\n",
    "    sample = df.loc[indices]\n",
    "    return sample\n",
    "\n",
    "\n",
    "def resample_by_year(df, column=\"wtssall\"):\n",
    "    \"\"\"Resample rows within each year.\n",
    "\n",
    "    df: DataFrame\n",
    "    column: string name of weight variable\n",
    "\n",
    "    returns DataFrame\n",
    "    \"\"\"\n",
    "    grouped = df.groupby(\"year\")\n",
    "    samples = [resample_rows_weighted(group, column) for _, group in grouped]\n",
    "    sample = pd.concat(samples, ignore_index=True)\n",
    "    return sample\n",
    "\n",
    "\n",
    "def values(series):\n",
    "    \"\"\"Count the values and sort.\n",
    "\n",
    "    series: pd.Series\n",
    "\n",
    "    returns: series mapping from values to frequencies\n",
    "    \"\"\"\n",
    "    return series.value_counts(dropna=False).sort_index()\n",
    "\n",
    "\n",
    "def count_by_year(gss, varname):\n",
    "    \"\"\"Groups by category and year and counts.\n",
    "\n",
    "    gss: DataFrame\n",
    "    varname: string variable to group by\n",
    "\n",
    "    returns: DataFrame with one row per year, one column per category.\n",
    "    \"\"\"\n",
    "    grouped = gss.groupby([varname, \"year\"])\n",
    "    count = grouped[varname].count().unstack(level=0)\n",
    "\n",
    "    # note: the following is not ideal, because it does not\n",
    "    # distinguish 0 from NA, but in this dataset the only\n",
    "    # zeros are during years when the question was not asked.\n",
    "    count = count.replace(0, np.nan).dropna()\n",
    "    return count\n",
    "\n",
    "\n",
    "def fill_missing(df, varname, badvals=[98, 99]):\n",
    "    \"\"\"Fill missing data with random values.\n",
    "\n",
    "    df: DataFrame\n",
    "    varname: string column name\n",
    "    badvals: list of values to be replaced\n",
    "    \"\"\"\n",
    "    # replace badvals with NaN\n",
    "    df[varname].replace(badvals, np.nan, inplace=True)\n",
    "\n",
    "    # get the index of rows missing varname\n",
    "    null = df[varname].isnull()\n",
    "    n_missing = sum(null)\n",
    "\n",
    "    # choose a random sample from the non-missing values\n",
    "    fill = np.random.choice(df[varname].dropna(), n_missing, replace=True)\n",
    "\n",
    "    # replace missing data with the samples\n",
    "    df.loc[null, varname] = fill\n",
    "\n",
    "    # return the number of missing values replaced\n",
    "    return n_missing\n",
    "\n",
    "\n",
    "def round_into_bins(df, var, bin_width, high=None, low=0):\n",
    "    \"\"\"Rounds values down to the bin they belong in.\n",
    "\n",
    "    df: DataFrame\n",
    "    var: string variable name\n",
    "    bin_width: number, width of the bins\n",
    "\n",
    "    returns: array of bin values\n",
    "    \"\"\"\n",
    "    if high is None:\n",
    "        high = df[var].max()\n",
    "\n",
    "    bins = np.arange(low, high + bin_width, bin_width)\n",
    "    indices = np.digitize(df[var], bins)\n",
    "    return bins[indices - 1]\n",
    "\n",
    "\n",
    "def underride(d, **options):\n",
    "    \"\"\"Add key-value pairs to d only if key is not in d.\n",
    "\n",
    "    d: dictionary\n",
    "    options: keyword args to add to d\n",
    "    \"\"\"\n",
    "    for key, val in options.items():\n",
    "        d.setdefault(key, val)\n",
    "\n",
    "    return d\n",
    "\n",
    "\n",
    "def decorate(**options):\n",
    "    \"\"\"Decorate the current axes.\n",
    "    Call decorate with keyword arguments like\n",
    "    decorate(title='Title',\n",
    "             xlabel='x',\n",
    "             ylabel='y')\n",
    "    The keyword arguments can be any of the axis properties\n",
    "    https://matplotlib.org/api/axes_api.html\n",
    "    In addition, you can use `legend=False` to suppress the legend.\n",
    "    And you can use `loc` to indicate the location of the legend\n",
    "    (the default value is 'best')\n",
    "    \"\"\"\n",
    "    loc = options.pop(\"loc\", \"best\")\n",
    "    if options.pop(\"legend\", True):\n",
    "        legend(loc=loc)\n",
    "\n",
    "    plt.gca().set(**options)\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "def legend(**options):\n",
    "    \"\"\"Draws a legend only if there is at least one labeled item.\n",
    "    options are passed to plt.legend()\n",
    "    https://matplotlib.org/api/_as_gen/matplotlib.pyplot.legend.html\n",
    "    \"\"\"\n",
    "    underride(options, loc=\"best\")\n",
    "\n",
    "    ax = plt.gca()\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    if handles:\n",
    "        ax.legend(handles, labels, **options)\n",
    "\n",
    "\n",
    "def make_lowess(series, frac=2 / 3):\n",
    "    \"\"\"Use LOWESS to compute a smooth line.\n",
    "\n",
    "    series: pd.Series\n",
    "\n",
    "    returns: pd.Series\n",
    "    \"\"\"\n",
    "    y = series.values\n",
    "    x = series.index.values\n",
    "\n",
    "    smooth = lowess(y, x, frac=frac)\n",
    "    index, data = np.transpose(smooth)\n",
    "    return pd.Series(data, index=index)\n",
    "\n",
    "\n",
    "def plot_lowess(series, color, frac=0.7, **options):\n",
    "    \"\"\"Plot a smooth line.\n",
    "\n",
    "    series: pd.Series\n",
    "    color: string or tuple\n",
    "    \"\"\"\n",
    "    if \"label\" not in options:\n",
    "        options[\"label\"] = series.name\n",
    "\n",
    "    smooth = make_lowess(series, frac=frac)\n",
    "    smooth.plot(color=color, **options)\n",
    "\n",
    "\n",
    "def plot_series_lowess(series, color, frac=0.7, **options):\n",
    "    \"\"\"Plots a series of data points and a smooth line.\n",
    "\n",
    "    series: pd.Series\n",
    "    color: string or tuple\n",
    "    \"\"\"\n",
    "    if \"label\" not in options:\n",
    "        options[\"label\"] = series.name\n",
    "\n",
    "    x = series.index\n",
    "    y = series.values\n",
    "\n",
    "    if len(series) == 1:\n",
    "        # just plot the point\n",
    "        plt.plot(x, y, \"o\", color=color, alpha=0.5, label=options[\"label\"])\n",
    "    else:\n",
    "        # plot the points and line\n",
    "        plt.plot(x, y, \"o\", color=color, alpha=0.5, label=\"_\")\n",
    "        plot_lowess(series, color, frac, **options)\n",
    "\n",
    "\n",
    "def plot_columns_lowess(df, columns, colors):\n",
    "    \"\"\"Plot the columns in a DataFrame.\n",
    "\n",
    "    df: pd.DataFrame\n",
    "    columns: list of column names, in the desired order\n",
    "    colors: mapping from column names to colors\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        series = df[col]\n",
    "        plot_series_lowess(series, colors[col])\n",
    "\n",
    "\n",
    "def anchor_legend(x, y):\n",
    "    \"\"\"Put the legend at the given locationself.\n",
    "\n",
    "    x: axis coordinate\n",
    "    y: axis coordinate\n",
    "    \"\"\"\n",
    "    plt.legend(bbox_to_anchor=(x, y), loc=\"upper left\", ncol=1)\n",
    "\n",
    "\n",
    "def download(url):\n",
    "    filename = basename(url)\n",
    "    if not exists(filename):\n",
    "        from urllib.request import urlretrieve\n",
    "\n",
    "        local, _ = urlretrieve(url, filename)\n",
    "        print(\"Downloaded \" + local)\n",
    "\n",
    "\n",
    "def read_gss(dict_file=\"GSS.dct\", data_file=\"GSS.dat.gz\"):\n",
    "    from statadict import parse_stata_dict\n",
    "\n",
    "    download(\n",
    "        \"https://github.com/AllenDowney/\"\n",
    "        + \"ElementsOfDataScience/raw/master/data/\"\n",
    "        + dict_file\n",
    "    )\n",
    "\n",
    "    download(\n",
    "        \"https://github.com/AllenDowney/\"\n",
    "        + \"ElementsOfDataScience/raw/master/data/\"\n",
    "        + data_file\n",
    "    )\n",
    "\n",
    "    stata_dict = parse_stata_dict(dict_file)\n",
    "    fp = gzip.open(data_file)\n",
    "    gss = pd.read_fwf(fp, names=stata_dict.names, colspecs=stata_dict.colspecs)\n",
    "    return gss\n",
    "\n",
    "\n",
    "def traceback(mode):\n",
    "    \"\"\"Set the traceback mode.\n",
    "\n",
    "    mode: string\n",
    "    \"\"\"\n",
    "    # this context suppresses the output\n",
    "    with contextlib.redirect_stdout(io.StringIO()):\n",
    "        get_ipython().run_cell(f\"%xmode {mode}\")\n",
    "\n",
    "\n",
    "traceback(\"Minimal\")\n",
    "\n",
    "\n",
    "def extract_function_name(text):\n",
    "    \"\"\"Find a function definition and return its name.\n",
    "\n",
    "    text: String\n",
    "\n",
    "    returns: String or None\n",
    "    \"\"\"\n",
    "    pattern = r\"def\\s+(\\w+)\\s*\\(\"\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        func_name = match.group(1)\n",
    "        return func_name\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "@register_cell_magic\n",
    "def expect_error(line, cell):\n",
    "    try:\n",
    "        get_ipython().run_cell(cell)\n",
    "    except Exception as e:\n",
    "        get_ipython().run_cell(\"%tb\")\n",
    "\n",
    "\n",
    "@magic_arguments()\n",
    "@argument(\"exception\", help=\"Type of exception to catch\")\n",
    "@register_cell_magic\n",
    "def expect(line, cell):\n",
    "    args = parse_argstring(expect, line)\n",
    "    exception = eval(args.exception)\n",
    "    try:\n",
    "        get_ipython().run_cell(cell)\n",
    "    except exception as e:\n",
    "        get_ipython().run_cell(\"%tb\")\n",
    "\n",
    "\n",
    "\n",
    "def download(url):\n",
    "    filename = basename(url)\n",
    "    if not exists(filename):\n",
    "        from urllib.request import urlretrieve\n",
    "\n",
    "        local, _ = urlretrieve(url, filename)\n",
    "        print(\"Downloaded \" + str(local))\n",
    "    return filename\n",
    "\n",
    "download('https://raw.githubusercontent.com/AllenDowney/ElementsOfDataScience/v1/utils.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from skimpy import skim\n",
    "\n",
    "download('https://github.com/AllenDowney/ElementsOfDataScience/' +\n",
    "         'raw/v1/data/gss_extract_2022.hdf');\n",
    "gss = pd.read_hdf('gss_extract_2022.hdf', 'gss')\n",
    "skim(gss)\n",
    "gss.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Regression w scipy\n",
    "from scipy.stats import linregress\n",
    "\n",
    "data = gss.dropna(subset=['realinc', 'educ'])\n",
    "xs = data['educ']\n",
    "ys = data['realinc']\n",
    "\n",
    "res = linregress(xs, ys)\n",
    "res._asdict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple regression with statsmodels\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "results = smf.ols('realinc ~ educ', data=data).fit()\n",
    "type(results)\n",
    "results.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple regress statsmodel\n",
    "results = smf.ols('realinc ~ educ + age', data=gss).fit()\n",
    "results.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by age\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "grouped = gss.groupby('age')\n",
    "type(grouped)\n",
    "mean_income_by_age = grouped['realinc'].mean()\n",
    "\n",
    "plt.plot(mean_income_by_age, 'o', alpha=0.5)\n",
    "plt.xlabel('Age (years)')\n",
    "plt.ylabel('Income (1986 $)')\n",
    "plt.title('Average income, grouped by age');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quadratic age and ed\n",
    "import numpy as np\n",
    "\n",
    "gss['age2'] = gss['age']**2\n",
    "gss['educ2'] = gss['educ']**2\n",
    "model = smf.ols('realinc ~ educ + educ2 + age + age2', data=gss)\n",
    "results = model.fit()\n",
    "results.params\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['age'] = np.linspace(18, 89)\n",
    "df['age2'] = df['age']**2\n",
    "df['educ'] = 12\n",
    "df['educ2'] = df['educ']**2\n",
    "\n",
    "pred12 = results.predict(df)\n",
    "\n",
    "plt.plot(mean_income_by_age, 'o', alpha=0.5)\n",
    "plt.plot(df['age'], pred12, label='High school', color='C4')\n",
    "\n",
    "df['educ'] = 16\n",
    "df['educ2'] = df['educ']**2\n",
    "pred16 = results.predict(df)\n",
    "\n",
    "df['educ'] = 14\n",
    "df['educ2'] = df['educ']**2\n",
    "pred14 = results.predict(df)\n",
    "\n",
    "plt.plot(mean_income_by_age, 'o', alpha=0.5)\n",
    "plt.plot(df['age'], pred16, ':', label='Bachelor')\n",
    "plt.plot(df['age'], pred14, '--', label='Associate')\n",
    "plt.plot(df['age'], pred12, label='High school', color='C4')\n",
    "\n",
    "plt.xlabel('Age (years)')\n",
    "plt.ylabel('Income (1986 $)')\n",
    "plt.title('Income versus age, grouped by education level')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorical - sex\n",
    "\n",
    "formula = 'realinc ~ educ + educ2 + age + age2 + C(sex)'\n",
    "results = smf.ols(formula, data=gss).fit()\n",
    "results.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "gss['gunlaw'] = gss['gunlaw'].replace([2], [0]) # make binary 0 = no, 1= yes\n",
    "gss['gunlaw'].value_counts()\n",
    "formula = 'gunlaw ~ age + age2 + educ + educ2 + C(sex)'\n",
    "results = smf.logit(formula, data=gss).fit()\n",
    "results.params\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['age'] = np.linspace(18, 89)\n",
    "df['educ'] = 12\n",
    "df['age2'] = df['age']**2\n",
    "df['educ2'] = df['educ']**2\n",
    "\n",
    "df['sex'] = 1\n",
    "pred_male = results.predict(df)\n",
    "df['sex'] = 2\n",
    "pred_female = results.predict(df)\n",
    "\n",
    "grouped = gss.groupby('age')\n",
    "favor_by_age = grouped['gunlaw'].mean()\n",
    "\n",
    "plt.plot(favor_by_age, 'o', alpha=0.5)\n",
    "plt.plot(df['age'], pred_female, label='Female')\n",
    "plt.plot(df['age'], pred_male, '--', label='Male')\n",
    "\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Probability of favoring gun law')\n",
    "plt.title('Support for gun law versus age, grouped by sex')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3b1b solver for Wordle - which relies on entropy calculations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/woctezuma/3b1b-wordle-solver.git\n",
    "%cd 3b1b-wordle-solver\n",
    "%pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://github.com/woctezuma/3b1b-wordle-solver/releases/download/wordle/pattern_matrix.npy\"\n",
    "!wget {url} -P data/wordle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://github.com/woctezuma/3b1b-wordle-solver/releases/download/dungleon/pattern_matrix.npy\"\n",
    "!wget {url} -P data/dungleon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python simulations.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python simulations.py --game-name wordle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python simulations.py --game-name dungleon --first-guess ZBCFS --test-answer WSGCO --hard-mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cp data/dungleon/possible_words.seen.txt data/dungleon/possible_words.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python simulations.py --game-name dungleon --first-guess AWOCS --test-answer VMIFI --hard-mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cp data/dungleon/possible_words.seen.txt data/dungleon/possible_words.txt\n",
    "%cp data/dungleon/possible_words.seen.txt data/dungleon/allowed_words.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_NAME = \"simulations\"\n",
    "%mkdir -p {FOLDER_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "for l in Path('data/dungleon/possible_words.seen.txt').read_text().split('\\n'):\n",
    "  if l:\n",
    "    !python simulations.py --game-name dungleon --hard-mode --first-guess {l} | tail > {FOLDER_NAME}/{l}.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = \"Total\"\n",
    "\n",
    "!grep {keyword} {FOLDER_NAME}/*.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Med SSS Clinical reasoning - THUS FAR, NOT SUCCESFUL IN GETTING THIS GOING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import list_repo_files\n",
    "\n",
    "# List all files in the repository\n",
    "files = list_repo_files(\"pixas/MedSSS_Policy\")\n",
    "print(\"Files in repository:\")\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Download the adapter weights\n",
    "adapter_path = hf_hub_download(repo_id=\"pixas/MedSSS_Policy\", filename=\"adapter_model.safetensors\")\n",
    "adapter_state_dict = load_file(adapter_path)\n",
    "\n",
    "# Adjust keys in the adapter state_dict\n",
    "from collections import OrderedDict\n",
    "\n",
    "adjusted_state_dict = OrderedDict()\n",
    "for key, value in adapter_state_dict.items():\n",
    "    new_key = key.replace(\"model.model.model\", \"model\")\n",
    "    adjusted_state_dict[new_key] = value\n",
    "\n",
    "# Load the adjusted adapter state_dict into the base_model\n",
    "base_model.load_state_dict(adjusted_state_dict, strict=False)\n",
    "\n",
    "peft_config = PeftConfig(\n",
    "    base_model_name_or_path=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    task_type=\"CAUSAL_LM\",  # Task type (e.g., causal language modeling)\n",
    "    peft_type=\"LORA\",       # Type of PEFT (LoRA in this case)\n",
    "    inference_mode=True     # Set to True for inference\n",
    ")\n",
    "\n",
    "# Wrap the base_model with PeftModel\n",
    "model = PeftModel(base_model, peft_config=peft_config)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "\n",
    "# Test the model\n",
    "input_text = \"How to stop a cough?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\",  # Correct base model\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "for name, param in base_model.named_parameters():\n",
    "    print(name)\n",
    "\n",
    "# Load the LoRA adapter state_dict\n",
    "lora_state_dict = torch.load(\"path/to/lora/checkpoint.bin\", map_location=\"cpu\")\n",
    "\n",
    "# Create a new state_dict with adjusted keys\n",
    "new_state_dict = OrderedDict()\n",
    "for key, value in lora_state_dict[\"model\"].items():\n",
    "    # Adjust keys by replacing the extra 'model.model.model' with just 'model'\n",
    "    new_key = key.replace(\"model.model.model\", \"model\")\n",
    "    new_state_dict[new_key] = value\n",
    "\n",
    "# Load the adjusted state_dict into the model\n",
    "model.load_state_dict(new_state_dict, strict=False)\n",
    "\n",
    "\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model, \n",
    "    \"pixas/MedSSS_Policy\", \n",
    "    torch_dtype=\"auto\", \n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "model.embed_tokens.weight\n",
    "base_model.model.embed_tokens.weight\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"pixas/MedSSS_Policy\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "\n",
    "input_text = \"How to stop a cough?\"\n",
    "messages = [{\"role\": \"user\", \"content\": input_text}]\n",
    "inputs = tokenizer(tokenizer.apply_chat_template(messages, tokenize=False,add_generation_prompt=True\n",
    "), return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=2048)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check model specification for troubleshooting\n",
    "\n",
    "from huggingface_hub import hf_hub_download, model_info\n",
    "\n",
    "model_card = hf_hub_download(repo_id=\"pixas/MedSSS_Policy\", filename=\"README.md\")\n",
    "with open(model_card, \"r\") as f:\n",
    "    print(f.read())\n",
    "\n",
    "# Fetch model information\n",
    "info = model_info(\"pixas/MedSSS_Policy\")\n",
    "\n",
    "# Print information\n",
    "print(f\"Model ID: {info.modelId}\")\n",
    "print(f\"Tags: {info.tags}\")\n",
    "print(f\"Files: {info.siblings}\")\n",
    "print(f\"Card Data: {info.cardData}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftConfig\n",
    "\n",
    "# Load adapter config\n",
    "adapter_config = PeftConfig.from_pretrained(\"pixas/MedSSS_Policy\")\n",
    "print(adapter_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Ollama Version - requires ollama to be installed (Mac only) and ~>16gb ram, 8gb of disk space.\n",
    "# Local - too verbose and doesn't quite get the instructions right (openAI has better prompt engineering)\n",
    "\n",
    "model = llm.get_model(\"hf.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF:Q8_0\") \n",
    "\n",
    "# Create a new column to store the responses\n",
    "transcripts_df[\"responses\"] = None\n",
    "\n",
    "# Iterate through each transcript in the DataFrame\n",
    "for i, row in transcripts_df.iterrows():\n",
    "    full_prompts = row[\"full_prompts\"]  # Get the full_prompts dictionary for this transcript\n",
    "    transcript_responses = {}  # Dictionary to store responses for this transcript\n",
    "    \n",
    "    # Iterate through each diagnosis and its associated prompt\n",
    "    for diagnosis, prompt in full_prompts.items():\n",
    "        response = model.prompt(prompt)  # Get the model's response\n",
    "        transcript_responses[diagnosis] = response.text()  # Store the response text\n",
    "    \n",
    "    # Save the responses back into the DataFrame\n",
    "    transcripts_df.at[i, \"responses\"] = transcript_responses\n",
    "\n",
    "display(transcripts_df)\n",
    "#print(tabulate(transcripts_df, headers = 'keys', tablefmt = 'fancy_grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bambi and PyMC\n",
    "\n",
    "https://bambinos.github.io/bambi/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import bambi as bmb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in a dataset from the package content\n",
    "data = bmb.load_data(\"sleepstudy\")\n",
    "\n",
    "# See first rows\n",
    "data.head()\n",
    "skimpy.skim(data)\n",
    " \n",
    "# Initialize the fixed effects only model\n",
    "model = bmb.Model('Reaction ~ Days', data)\n",
    "\n",
    "# Get model description\n",
    "print(model)\n",
    "\n",
    "# Fit the model using 1000 on each chain\n",
    "results = model.fit(draws=1000)\n",
    "\n",
    "# Key summary and diagnostic info on the model parameters\n",
    "az.summary(results)\n",
    "\n",
    "# Use ArviZ to plot the results\n",
    "az.plot_trace(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\n",
    "    \"g\": np.random.choice([\"Yes\", \"No\"], size=50),\n",
    "    \"x1\": np.random.normal(size=50),\n",
    "    \"x2\": np.random.normal(size=50)\n",
    "})\n",
    "skimpy.skim(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
