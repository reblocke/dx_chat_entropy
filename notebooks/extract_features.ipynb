{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Repository setup for portable, repo-relative paths\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "def _find_repo_root(start: Path | None = None) -> Path:\n",
    "    start = (start or Path.cwd()).resolve()\n",
    "    for candidate in [start, *start.parents]:\n",
    "        if (candidate / \"pyproject.toml\").exists():\n",
    "            return candidate\n",
    "    return start\n",
    "\n",
    "REPO_ROOT = _find_repo_root()\n",
    "if str(REPO_ROOT / \"src\") not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_ROOT / \"src\"))\n",
    "\n",
    "from dx_chat_entropy.paths import get_paths\n",
    "PATHS = get_paths(REPO_ROOT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for extracting whether features are present from chatbot transcripts\n",
    "\n",
    "\n",
    "### How it works: \n",
    "\n",
    "This takes: \n",
    "- a specification 'assessment_template.xlsx' that has a column identifying each piece of information you'd like to assess for in the transcript. This file must be in an ASSESSMENT_DIR, which is where the output will be written. Each sheet in the assessment_template should have information features pertinent to a single diagnosis (e.g. Cardiac chest pain, esophageal dysphagia, etc.)\n",
    "- a folder that contains all the transcripts (TRANSCRIPT_DIR)\n",
    "\n",
    "The script then extracts the text from each transcripts and feeds it to an LLM, asking it to evaluate whether each piece of information was assessed by the doctor, and if so whether they patient responded that it was present or not. \n",
    "\n",
    "It then feeds the response text to an LLM, reformats it, and outputs it back to a new excel spreadsheet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from markitdown import MarkItDown\n",
    "import llm\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "import os\n",
    "from typing import List, Optional, Literal\n",
    "from tabulate import tabulate\n",
    "from IPython.display import display\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # looks for a .env file in the current dir by default\n",
    "#print(os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script will read in all transcripts in this directory to be analyzed. \n",
    "TRANSCRIPT_DIR = str(PATHS.raw / 'chatbot_transcripts')\n",
    "\n",
    "# The script will pull in the features from this template (which should be in the assessment dir), \n",
    "# and output the resulting assessments here. \n",
    "ASSESSMENT_DIR = str(PATHS.processed / 'assessments')\n",
    "ASSESSMENT_TEMPLATE = str(PATHS.raw / 'assessment_templates' / 'asssessment_template_new.xlsx')\n",
    "\n",
    "# Note, you also need an OpenAI API key that should be saved using the LLM package. \n",
    "# Create a file \".env\" in the working folder that contains OPENAI_API_KEY=your-secret-key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make array of all transcript file names to be ingested\n",
    "pdf_filepaths = [\n",
    "    os.path.join(TRANSCRIPT_DIR, f)\n",
    "    for f in os.listdir(TRANSCRIPT_DIR)\n",
    "    if f.endswith(\".pdf\")\n",
    "]\n",
    "print(\"PDF Filepaths:\")\n",
    "for filepath in pdf_filepaths:\n",
    "    print(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for ingesting transcript PDFs\n",
    "md = MarkItDown()\n",
    "transcripts = []\n",
    "\n",
    "for filepath in pdf_filepaths:\n",
    "    # Convert the PDF to text using MarkItDown\n",
    "    result = md.convert(filepath)\n",
    "    extracted_text = result.text_content\n",
    "    transcripts.append({\n",
    "        'filename': os.path.basename(filepath),\n",
    "        'text_content': extracted_text\n",
    "    })\n",
    "transcripts_df = pd.DataFrame(transcripts)\n",
    "\n",
    "display(transcripts_df)\n",
    "#print(tabulate(transcripts_df, headers = 'keys', tablefmt = 'fancy_grid'))\n",
    "# If desired, you can save it to a CSV\n",
    "# df.to_csv(os.path.join(TRANSCRIPT_DIR, \"transcripts.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instruction part of the prompt\n",
    "instruction_prompt = \"\"\"You are a research assistant who is meticulously reviewing transcripts of interview for a research project. \n",
    "\n",
    "Task: You will be given a list of pieces of information that a doctor might ask a patient about. \n",
    "Your goal is to read the transcript and score it by whether the doctor collected each piece of information. \n",
    "To do this, you must read the transcript carefully, understanding what each question and response meant. \n",
    "If an answer is asked obliquely - but a reasonable person would understand what was intended - this should count.\n",
    "\n",
    "Return format: for each piece of information, you should answer in the following way - \n",
    "\n",
    "a. If the doctor asked about the piece of information, and the patient responded that the feature is present, answer \"<Information>, YES\". \n",
    "For example, if the information is: Chest Pain? and the doctor asked \"Do you have chest pain?\" and the patient answered \"I do\", you should respond 'Chest Pain?, YES'\n",
    "If the information is: Nausea? doctor asked \"Did you have nausea?\" and the patient answers \"I did\", you should respond 'Nausea?, YES'.\n",
    "\n",
    "b. If the doctor asked about the piece of information, and the patient responded that the feature was not present, answer '<Information>, NO' \n",
    "For example, if the question is Shortness of Breath? and the doctor asked \"Are you dyspneic?\" and the patient answers they are not, the answer should be 'Shortness of Breath?,NO'\n",
    "\n",
    "c. If the doctor did not ask about the piece of information, you should return '<Information>, MISSING'\n",
    "\n",
    "Warning: There are ONLY three ways you should ever answer for each piece of information: '<Information>, YES', '<Information>, NO', and '<Information>, NOT ASKED'. \n",
    "Never answer in other ways. \n",
    "\n",
    "Here are some examples:\n",
    "1.\n",
    "Information: 'Pain not worse with exertion (requires they clarify exercise 1hr after meal)'\n",
    "Doctor at some point asks: Does the pain worsen after a meal? \n",
    "Patient: yes, it's worse\n",
    "Response: \"'Pain not worse with exertion (requires they clarify exercise 1hr after meal)', YES\", because this is close enough for a reasonable person. \n",
    "\n",
    "2. \n",
    "Information: 'no prior CAD'\n",
    "Doctor at some point asks: Have you ever had a heart attack? \n",
    "Patient: never\n",
    "Response: \"'no prior CAD', NO\", because CAD stands for coronary artery disease and a heart attack is the most common manifestation.\n",
    "\n",
    "3. \n",
    "Information: 'no diaphoresis'\n",
    "Doctor never asks anything that clarifies if the patient was sweaty and never assessed it on examination\n",
    "Response: \"'no diaphoresis', MISSING\" , because exam findings that are discussed should also count.  \n",
    "\n",
    "Putting it all together, the response should follow this format: \n",
    "1. Pain not worse with exertion (requires they clarify exercise 1hr after meal), YES\n",
    "2. \"Do you have any PMHx?\" (counts as 2 independent minor features), MISSING\n",
    "3. no tobacco, NO\n",
    "4. no associated shortness of breath, YES\n",
    "5. no radiation to the neck, arm, or jaw?, MISSING\n",
    "... and so on, through the entire list.\n",
    "\n",
    "Remember, NOTHING ELSE should be in the final output. Just the information, and YES/NO/MISSING \n",
    "\n",
    "Here is the list of pieces of information I would like you to look for: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing of the specification for what assessments we want the LLM to look for\n",
    "\n",
    "def process_sheet(sheet_data):\n",
    "    \"\"\"\n",
    "    Processes a sheet to extract the 'Information' and associated 'LR' values.\n",
    "    Ignores the 'Y/N' column.\n",
    "    Returns a list of tuples (information_str, lr_value).\n",
    "    \"\"\"\n",
    "    # Drop Y/N column if it exists\n",
    "    sheet_data = sheet_data.drop(columns=[\"Y/N\"], errors=\"ignore\")\n",
    "\n",
    "    info_list = []\n",
    "\n",
    "    # Iterate through each row and capture the single 'Information' + 'LR' from that row\n",
    "    for _, row in sheet_data.iterrows():\n",
    "        info_val = row.get(\"Information\", None)  # Safely get 'Information' column\n",
    "        lr_val = row.get(\"LR\", None)             # Safely get 'LR' column\n",
    "\n",
    "        # If the information cell is not empty/NaN, we record it.\n",
    "        # If LR is NaN or missing, we'll store it as None.\n",
    "        if pd.notnull(info_val):\n",
    "            # Normalize LR to None if it's NaN\n",
    "            if pd.isnull(lr_val):\n",
    "                lr_val = None\n",
    "\n",
    "            info_list.append((info_val, lr_val))\n",
    "\n",
    "    return info_list\n",
    "\n",
    "diagnosis_info = {}\n",
    "with pd.ExcelFile(ASSESSMENT_TEMPLATE) as spreadsheet_data:\n",
    "    for sheet_name in spreadsheet_data.sheet_names:\n",
    "        try:\n",
    "            sheet_data = pd.read_excel(ASSESSMENT_TEMPLATE, sheet_name=sheet_name)\n",
    "\n",
    "            if sheet_data.empty:\n",
    "                print(f\"Skipping empty sheet: {sheet_name}\")\n",
    "                continue\n",
    "\n",
    "            # Process the sheet to get [(info, LR), ...]\n",
    "            diagnosis_info[sheet_name] = process_sheet(sheet_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sheet '{sheet_name}': {e}\")\n",
    "\n",
    "# Print out the collected data\n",
    "for diagnosis, info_pairs in diagnosis_info.items():\n",
    "    print(f\"Diagnosis: {diagnosis}\")\n",
    "    for info_val, lr_val in info_pairs:\n",
    "        print(f\"  Information: {info_val}, LR: {lr_val}\")\n",
    "\n",
    "# NOTE: may not actually need to bother with LRs at this point? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# ESTIMATE LRS FOR ALL THAT HAVE UNKNOWN LRS\n",
    "# TODO: Note, in the real workflow - should do this using o1 and only do it once, rather than over and over.\n",
    "\n",
    "class LRResponse(BaseModel):\n",
    "    \"\"\"\n",
    "    A structured schema ensuring the model returns exactly one of the five LR labels.\n",
    "    \"\"\"\n",
    "    label: Literal[\"STRONG NEGATIVE\", \n",
    "                   \"WEAK NEGATIVE\", \n",
    "                   \"NEUTRAL\", \n",
    "                   \"WEAK POSITIVE\", \n",
    "                   \"STRONG POSITIVE\"]\n",
    "\n",
    "\n",
    "def estimate_lr(diagnosis, info_val, client):\n",
    "    \"\"\"\n",
    "    Returns one of the five LR categories (STRONG NEGATIVE, WEAK NEGATIVE,\n",
    "    NEUTRAL, WEAK POSITIVE, STRONG POSITIVE) for a given diagnosis and info_val.\n",
    "    Uses OpenAI's structured output parsing to ensure the response is valid.\n",
    "    \"\"\"\n",
    "\n",
    "    lr_prompt = \"\"\"You are an expert diagnostician who is explaining to a trainee which pieces of information they should pay attention to during the diagnostic process. Your task is to summarize how strong of evidence the presence or absence of a particular new finding is for whether a patient has a disease. For example, if a patient has chest pain and the EKG show ST segment elevations, this is STRONG evidence that the chest pain is due to a heart attack. If the patient has t-wave inversions, this is WEAKER evidence in favor - because t-wave changes are not as specific for cardiac causes of chest pain. If they have known heartburn, this is WEAK absence against (because it’s an explanation, but it IS possible to have a history of heartburn but have a heart attack). Lastly, if they are a young female without an inherited condition, this is STRONG evidence against a cardiac cause because that demographic almost never has heart attacks. Lastly, if the piece of information is unhelpful, it would be called neutral. For example, if the patient has blue eyes irrelevant to the cause of chest pain, thus it would be NEUTRAL. \n",
    "\n",
    "    I’d like you to follow the following steps:\n",
    "        1.\tConsider, what does the finding mean about what is going on with the patient?\n",
    "        2.\tdoes the presence of the new information make the disease more or less likely? Or no difference?\n",
    "        3.\tDoes the finding make another cause of the same symptom more common? If so, then by definition it makes the target condition a less likely explanation.\n",
    "        4.\tOnce you’ve decided whether the finding makes the diagnosis more or less likely, use the following scale to come up with a response:\n",
    "\n",
    "        •\tIf knowing the piece of information makes the odds of the diagnosis more than 1.95x higher than it was before, it is a STRONG POSITIVE finding\n",
    "        •\tIf knowing the piece of information makes the odds of the diagnosis 1.18x to 1.95x higher than it was before, it is a WEAK POSITIVE finding\n",
    "        •\tIf knowing the piece of information makes changes the odds only 0.92x to 1.18x as likely as it was before, then it is a NEUTRAL finding\n",
    "        •\tIf knowing the piece of information makes the odds of the diagnosis 0.72x to 0.92x times as likely as it was before, then it is a WEAK NEGATIVE finding.\n",
    "        •\tIf knowing the piece of information makes the odds of the diagnosis less than 0.72x higher than it was before, it is a STRONG POSITIVE\n",
    "\n",
    "    As another example, say I’m wondering whether a patient with GI bleeding has a lower GI bleed (below the ligament of Treitz) or an upper GI bleed. The presence of clots in the blood is a very strong predictor of lower GI bleeding, because bleeding from the stomach cannot form clots due to the stomach acid. You should use all physiologic clues to whether a piece of information is a STRONG, WEAK, or NEUTRAL predictor. \n",
    "\n",
    "    You will receive inputs in the following format; Target condition: <Condition, e.g. Cardiac chest pain>. Finding: <piece of information, e.g. ‘No radiation to the neck, arm, or jaw’>.\n",
    "\n",
    "    You must respond with EXACTLY ONE of the following categories (no extra text):\n",
    "    STRONG POSITIVE, WEAK POSITIVE, NEUTRAL, WEAK NEGATIVE, STRONG NEGATIVE.\n",
    "\n",
    "    You must respond with EXACTLY one of these categories in valid JSON\n",
    "    Your output must match the Pydantic schema: { 'label': '<one of the five strings>' }\n",
    "\n",
    "    Here are some examples:\n",
    "    Prompt = Target condition: Cardiac Chest Pain. Finding: Pain not worse with exertion (requires they clarify exercise 1hr after meal).\n",
    "    You would reason that because cardiac chest pain is usually worse with exertion because exertion worsens cardiac demand for oxygen, and thus worsens ischemia.\n",
    "    Response = {\n",
    "        \"label\": \"STRONG NEGATIVE\"\n",
    "    }\n",
    "\n",
    "    Prompt =  Target condition: Cardiac Chest Pain. Finding: No tobacco.\n",
    "    You would reason that because being someone who smokes increases your risk of coronary artery disease, and thus being a never smoker means you’re at less risk… but many people who have heart attacks still smoke, so it’s only a weak predictor. \n",
    "    Response = {\n",
    "        \"label\": \"WEAK NEGATIVE\"\n",
    "    }\n",
    "\n",
    "    Prompt = Target condition: Cardaic Chest Pain. Finding = enjoys playing chess.\n",
    "    You would reason that because enjoying chest has no relationship to having a heart attack.\n",
    "    Response = {\n",
    "        \"label\": \"NEUTRAL\"\n",
    "    }\n",
    "\n",
    "    Prompt = Target condition: Cardiac Chest Pain. Finding = pain located behind the sternum\n",
    "    You would reason that because cardiac chest pain is often experienced behind the sternum (thus, more likely), but so are many other causes of chest pain - like GERD.\n",
    "    Response = {\n",
    "        \"label\": \"WEAK POSITIVE\"\n",
    "    }\n",
    "\n",
    "    Prompt = Condition: Cardiac Chest Pain. Finding = pain worse with exertion.\n",
    "    You would reason that because the increased myocardial oxygen consumption worsens the pain if oxygen delivery to the myocardium is the cause, as it is in heart attacks.\n",
    "    Response = {\n",
    "        \"label\": \"STRONG NEGATIVE\"\n",
    "    }\n",
    "\n",
    "    OK: here’s the prompt…. \"\"\"\n",
    "        \n",
    "    # Create your conversation messages\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": lr_prompt},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Condition: {diagnosis}\\nFinding: {info_val}\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Make the structured call to the model\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        response_format=LRResponse,  # Our Pydantic model\n",
    "    )\n",
    "    \n",
    "    # Extract the parsed LRResponse from the completion\n",
    "    lr_response = completion.choices[0].message.parsed  # This will be an LRResponse instance\n",
    "    # The label is guaranteed to be one of the enumerated strings by Pydantic\n",
    "    return lr_response.label\n",
    "\n",
    "\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "for diagnosis, info_pairs in diagnosis_info.items():\n",
    "    # info_pairs is a list of (info_val, lr_val) tuples\n",
    "    for idx, (info_val, lr_val) in enumerate(info_pairs):\n",
    "        if lr_val is None:  # Missing LR\n",
    "            estimated_label = estimate_lr(diagnosis, info_val, client)\n",
    "            # Update the tuple\n",
    "            info_pairs[idx] = (info_val, estimated_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through all the pieces of info to make prompts for each disease (LLM called separately)\n",
    "info_prompts = {}\n",
    "for diagnosis, info_list in diagnosis_info.items():\n",
    "    # Create the long string with the specified format\n",
    "    info_prompt = \"\\n\".join([f\"Information: {info[0]}\" for info in info_list]) # info[0] = info, info[1] = lr\n",
    "    info_prompts[diagnosis] = info_prompt\n",
    "\n",
    "for key, prompt in info_prompts.items():\n",
    "    print(f\"Diagnosis: {key}. Prompt:\\n{prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the full prompts for each disease for each transcript\n",
    "\n",
    "# Initialize a new column in transcripts_df to store the prompts\n",
    "transcripts_df[\"full_prompts\"] = None\n",
    "\n",
    "# Iterate through each transcript in transcripts_df\n",
    "for i, transcript in enumerate(transcripts_df['text_content']):\n",
    "    # Create a dictionary for the current transcript's prompts\n",
    "    transcript_prompts = {}\n",
    "    \n",
    "    # Iterate through each diagnosis and its associated info list\n",
    "    for diagnosis, info_list in diagnosis_info.items():\n",
    "        # Create the long string with the specified format\n",
    "        info_prompt = \"\\n\".join([f\"Information: {info}\" for info in info_list])\n",
    "        \n",
    "        # Prefix the instruction_prompt to the disease-specific prompt, and add the transcript to the end.\n",
    "        full_prompt = (\n",
    "            f\"{instruction_prompt}\\n{info_prompt}\\n\\n and here is the transcript to assess:\\n{transcript}\"\n",
    "        )\n",
    "        \n",
    "        # Store the prompt for the current diagnosis\n",
    "        transcript_prompts[diagnosis] = full_prompt\n",
    "    \n",
    "    # Assign the dictionary of prompts to the new column for this transcript\n",
    "    transcripts_df.at[i, \"full_prompts\"] = transcript_prompts\n",
    "\n",
    "display(transcripts_df)\n",
    "#print(tabulate(transcripts_df, headers = 'keys', tablefmt = 'fancy_grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Available models to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in llm.get_models():\n",
    "    print(model.model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Ollama Version - requires ollama to be installed (Mac only) and ~>16gb ram, 8gb of disk space.\n",
    "# Local - too verbose and doesn't quite get the instructions right (openAI has better prompt engineering)\n",
    "\"\"\"\n",
    "model = llm.get_model(\"hf.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF:Q8_0\") \n",
    "\n",
    "# Create a new column to store the responses\n",
    "transcripts_df[\"responses\"] = None\n",
    "\n",
    "# Iterate through each transcript in the DataFrame\n",
    "for i, row in transcripts_df.iterrows():\n",
    "    full_prompts = row[\"full_prompts\"]  # Get the full_prompts dictionary for this transcript\n",
    "    transcript_responses = {}  # Dictionary to store responses for this transcript\n",
    "    \n",
    "    # Iterate through each diagnosis and its associated prompt\n",
    "    for diagnosis, prompt in full_prompts.items():\n",
    "        response = model.prompt(prompt)  # Get the model's response\n",
    "        transcript_responses[diagnosis] = response.text()  # Store the response text\n",
    "    \n",
    "    # Save the responses back into the DataFrame\n",
    "    transcripts_df.at[i, \"responses\"] = transcript_responses\n",
    "\n",
    "display(transcripts_df)\n",
    "#print(tabulate(transcripts_df, headers = 'keys', tablefmt = 'fancy_grid'))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Initialize the model\n",
    "#model = llm.get_model(\"gpt-4o\") # costs a bit more - \n",
    "model = llm.get_model(\"gpt-4o-mini\")\n",
    "model.key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# Create a new column to store the responses\n",
    "transcripts_df[\"responses\"] = None\n",
    "\n",
    "# Iterate through each transcript in the DataFrame\n",
    "for i, row in transcripts_df.iterrows():\n",
    "    full_prompts = row[\"full_prompts\"]  # Get the full_prompts dictionary for this transcript\n",
    "    transcript_responses = {}  # Dictionary to store responses for this transcript\n",
    "    \n",
    "    # Iterate through each diagnosis and its associated prompt\n",
    "    for diagnosis, prompt in full_prompts.items():\n",
    "        response = model.prompt(prompt)  # Get the model's response\n",
    "        transcript_responses[diagnosis] = response.text()  # Store the response text\n",
    "    \n",
    "    # Save the responses back into the DataFrame\n",
    "    transcripts_df.at[i, \"responses\"] = transcript_responses\n",
    "\n",
    "display(transcripts_df)\n",
    "#print(tabulate(transcripts_df, headers = 'keys', tablefmt = 'fancy_grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Separate Call to OpenAI with structured outputs to parse into JSON format\n",
    "\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "# Define a class for a single question-answer pair\n",
    "class InfoAnswer(BaseModel):\n",
    "    info: str\n",
    "    answer: Optional[bool]\n",
    "\n",
    "# Define a container class that holds a list of question-answer pairs\n",
    "class InfoAnswerList(BaseModel):\n",
    "    pairs: List[InfoAnswer]\n",
    "\n",
    "# Add a new column for storing info_answers\n",
    "transcripts_df[\"info_answers\"] = None\n",
    "\n",
    "# Iterate through each transcript's responses\n",
    "for i, row in transcripts_df.iterrows():\n",
    "    # Get the responses for this transcript\n",
    "    transcript_responses = row[\"responses\"]  # Assumes \"responses\" is already populated as a dictionary\n",
    "    \n",
    "    # Create a dictionary to hold the parsed info answers for all diagnoses\n",
    "    transcript_info_answers = {}\n",
    "    \n",
    "    # Iterate through each diagnosis and its response\n",
    "    for diagnosis, response_text in transcript_responses.items():\n",
    "        # Generate messages for each diagnosis\n",
    "        completion = client.beta.chat.completions.parse(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"Extract a list of pieces of information (info) and whether or not that piece of information was present (answer) as a boolean (YES = True, NO = False, MISSING = None).\",\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": response_text},\n",
    "            ],\n",
    "            response_format=InfoAnswerList,\n",
    "        )\n",
    "        \n",
    "        # Parse the response into the structured InfoAnswerList\n",
    "        info_answer_list = completion.choices[0].message.parsed\n",
    "        \n",
    "        # Store the parsed result for this diagnosis\n",
    "        transcript_info_answers[diagnosis] = info_answer_list\n",
    "    \n",
    "    # Store the parsed info answers for this transcript in the DataFrame\n",
    "    transcripts_df.at[i, \"info_answers\"] = transcript_info_answers\n",
    "\n",
    "display(transcripts_df)\n",
    "#print(tabulate(transcripts_df, headers = 'keys', tablefmt = 'fancy_grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each row in transcripts_df\n",
    "for i, row in transcripts_df.iterrows():\n",
    "    # Get the info_answers dictionary for this transcript\n",
    "    info_answers = row[\"info_answers\"]\n",
    "    transcript_filename = row[\"filename\"]  # Get the original filename\n",
    "    \n",
    "    # Construct the output file name\n",
    "    output_file = os.path.join(\n",
    "        ASSESSMENT_DIR,\n",
    "        f\"answers_{transcript_filename.replace('.pdf', '.xlsx')}\"\n",
    "    )\n",
    "    \n",
    "    # Create a writer object to handle multiple sheets\n",
    "    with pd.ExcelWriter(output_file, engine=\"openpyxl\") as writer:\n",
    "        # Iterate through each diagnosis in info_answers\n",
    "        for diagnosis, info_answer_list in info_answers.items():\n",
    "            # Create a DataFrame for the current diagnosis\n",
    "            data = [{\"answer\": pair.answer, \"info\": pair.info} for pair in info_answer_list.pairs]\n",
    "            df = pd.DataFrame(data)\n",
    "            display(df)\n",
    "            # Write the DataFrame to a sheet named after the diagnosis\n",
    "            df.to_excel(writer, sheet_name=diagnosis[:31], index=False)  # Sheet name max length is 31 characters\n",
    "    \n",
    "    print(f\"Info-Answer pairs have been written to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to go back and match the assessments (from GPT) to the data-frame with the LRs - might be able to specify this in the call. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End\n",
    "\n",
    "\n",
    "# TODO: would it be advisable to use some retrieval-augmented generation to improve this?\n",
    "#  (or perhaps just embed some knowledge)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
