{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Repository setup for portable, repo-relative paths\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "def _find_repo_root(start: Path | None = None) -> Path:\n",
    "    start = (start or Path.cwd()).resolve()\n",
    "    for candidate in [start, *start.parents]:\n",
    "        if (candidate / \"pyproject.toml\").exists():\n",
    "            return candidate\n",
    "    return start\n",
    "\n",
    "REPO_ROOT = _find_repo_root()\n",
    "if str(REPO_ROOT / \"src\") not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_ROOT / \"src\"))\n",
    "\n",
    "from dx_chat_entropy.paths import get_paths\n",
    "PATHS = get_paths(REPO_ROOT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Data from the NNT for input into the Auto LR generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "def get_specialty_links():\n",
    "    \"\"\"\n",
    "    Extracts specialties and their corresponding article links from the webpage.\n",
    "    Returns a list of dictionaries with specialty names and associated links.\n",
    "    \"\"\"\n",
    "\n",
    "    url = 'https://thennt.com/home-lr/'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Locate the section with \"Diagnosis (LR) Reviews by Specialty\"\n",
    "    specialty_section = soup.find('div', class_='well subdisplay accordion_caption', id='lr-byspecialty')\n",
    "\n",
    "    if not specialty_section:\n",
    "        print(\"Could not find the 'Diagnosis (LR) Reviews by Specialty' section on the webpage.\")\n",
    "        return []\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Find all specialty headings (e.g., h3)\n",
    "    subheadings = specialty_section.find_all('h3')\n",
    "\n",
    "    for subheading in subheadings:\n",
    "        subheading_text = subheading.get_text(strip=True)  # Get specialty name\n",
    "        links = []\n",
    "\n",
    "        # Find the next unordered list (ul) which contains links\n",
    "        next_ul = subheading.find_next_sibling('ul')\n",
    "\n",
    "        if next_ul:\n",
    "            for a_tag in next_ul.find_all('a', href=True):\n",
    "                link_text = a_tag.get_text(strip=True)  # Link display name\n",
    "                link_href = a_tag['href']  # Actual URL\n",
    "                links.append({'display_name': link_text, 'url': link_href})\n",
    "\n",
    "        results.append({'specialty': subheading_text, 'links': links})\n",
    "\n",
    "    return results\n",
    "\n",
    "def extract_likelihood_ratios(page_content):\n",
    "    \"\"\"\n",
    "    Parses all likelihood ratio tables within <article class=\"lr_cards_details\">.\n",
    "    Extracts findings and their likelihood ratios, ensuring sequential <td> pairs are handled correctly.\n",
    "    Returns a list of tuples: (finding, likelihood ratio).\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(page_content, 'html.parser')\n",
    "    results = []\n",
    "\n",
    "    # Locate the section containing likelihood ratio tables\n",
    "    lr_section = soup.find('article', class_='lr_cards_details')\n",
    "    if not lr_section:\n",
    "        return results  # Return empty if no section found\n",
    "\n",
    "    # Find all tables inside the LR card\n",
    "    tables = lr_section.find_all('table', class_='lrtable')\n",
    "\n",
    "    for table in tables:\n",
    "        # Grab all <tr> elements\n",
    "        all_rows = table.find_all('tr')\n",
    "\n",
    "        # Filter out any row that only has <th> (i.e., a header row)\n",
    "        data_rows = []\n",
    "        for row in all_rows:\n",
    "            # If there's at least one <td> in this row, treat it as a data row\n",
    "            if row.find_all('td'):\n",
    "                data_rows.append(row)\n",
    "\n",
    "        # If we have real data rows, parse them\n",
    "        if data_rows:\n",
    "            for row in data_rows:\n",
    "                cols = row.find_all('td')\n",
    "                # If the row has exactly 2 <td>, treat them as (finding, LR)\n",
    "                if len(cols) == 2:\n",
    "                    finding = cols[0].get_text(strip=True)\n",
    "                    lr_value = cols[1].get_text(strip=True)\n",
    "                    # If there's an <a> inside the LR cell, grab its text\n",
    "                    link = cols[1].find('a')\n",
    "                    if link:\n",
    "                        lr_value = link.get_text(strip=True) or lr_value\n",
    "                    if not lr_value:\n",
    "                        lr_value = \"Not reported\"\n",
    "\n",
    "                    results.append((finding, lr_value))\n",
    "\n",
    "        else:\n",
    "            # Fallback: if there are no valid data rows, we process all <td> in pairs\n",
    "            cols = table.find_all('td')\n",
    "            for i in range(0, len(cols) - 1, 2):\n",
    "                finding = cols[i].get_text(strip=True)\n",
    "                lr_value_element = cols[i + 1]\n",
    "\n",
    "                # Extract the likelihood ratio, handling nested <a> and <br/>\n",
    "                link = lr_value_element.find('a')\n",
    "                if link:\n",
    "                    lr_value = link.get_text(strip=True)\n",
    "                else:\n",
    "                    lr_value = lr_value_element.get_text(strip=True)\n",
    "\n",
    "                if not lr_value:\n",
    "                    lr_value = \"Not reported\"\n",
    "\n",
    "                results.append((finding, lr_value))\n",
    "\n",
    "    return results\n",
    "\n",
    "def fetch_webpages(specialty_links):\n",
    "    \"\"\"\n",
    "    Iterates through all the extracted links, fetches the webpage content, \n",
    "    and extracts likelihood ratio findings.\n",
    "    \"\"\"\n",
    "    findings_by_display_name = {}\n",
    "\n",
    "    for item in specialty_links:\n",
    "        print(f\"Fetching pages for Specialty: {item['specialty']}\")\n",
    "\n",
    "        for link in item['links']:\n",
    "            display_name = link['display_name']\n",
    "            url = link['url']\n",
    "\n",
    "            try:\n",
    "                print(f\"  - Fetching: {display_name} ({url})\")\n",
    "                response = requests.get(url)\n",
    "\n",
    "                if response.status_code == 200:\n",
    "                    print(f\"    Success: {display_name} page fetched.\")\n",
    "                    \n",
    "                    # Extract likelihood ratio findings\n",
    "                    findings = extract_likelihood_ratios(response.text)\n",
    "                    \n",
    "                    # Store the extracted data\n",
    "                    findings_by_display_name[display_name] = findings\n",
    "\n",
    "                else:\n",
    "                    print(f\"    Failed to fetch {display_name} - Status Code: {response.status_code}\")\n",
    "\n",
    "                time.sleep(1)  # Optional: Add a delay to avoid overwhelming the server\n",
    "\n",
    "            except requests.RequestException as e:\n",
    "                print(f\"    Error fetching {display_name}: {e}\")\n",
    "\n",
    "        print(\"\\n\")  # Add space between specialties for readability\n",
    "\n",
    "    return findings_by_display_name\n",
    "\n",
    "def save_to_excel(findings_data, filename=\"data/processed/nnt_lrs/nnt_lrs.xlsx\", blank_values=False):\n",
    "    \"\"\"\n",
    "    Saves likelihood ratios to an Excel file with each display_name as a separate sheet.\n",
    "    If blank_values is True, the Likelihood Ratio column is left blank.\n",
    "    The first row contains the full display_name, and column headers start from the second row.\n",
    "    \"\"\"\n",
    "    with pd.ExcelWriter(filename, engine=\"openpyxl\") as writer:\n",
    "        for display_name, findings in findings_data.items():\n",
    "            if findings:\n",
    "                # Prepare DataFrame\n",
    "                df = pd.DataFrame(findings, columns=[\"Finding\", \"Likelihood Ratio\"])\n",
    "\n",
    "                if blank_values:\n",
    "                    df[\"Likelihood Ratio\"] = \"\"  # Clear likelihood ratio values\n",
    "\n",
    "                # Insert full display_name as the first row\n",
    "                full_name_row = pd.DataFrame({df.columns[0]: [display_name], df.columns[1]: [\"\"]})\n",
    "                df = pd.concat([full_name_row, df], ignore_index=True)\n",
    "\n",
    "                # Save to Excel with sheet name as the **last** 31 characters\n",
    "                sheet_name = display_name[-31:]\n",
    "                df.to_excel(writer, sheet_name=sheet_name, index=False, header=False)  # No default header\n",
    "\n",
    "            else:\n",
    "                print(f\"Skipping {display_name} (No data found).\")\n",
    "\n",
    "    print(f\"\\nLikelihood ratios saved to {filename}\")\n",
    "\n",
    "# Fetch specialties and links\n",
    "specialty_links = get_specialty_links()\n",
    "findings_data = fetch_webpages(specialty_links)\n",
    "\n",
    "# Save normal file\n",
    "save_to_excel(findings_data, \"data/processed/nnt_lrs/nnt_lrs.xlsx\", blank_values=False)\n",
    "\n",
    "# Save version with blank likelihood ratios\n",
    "save_to_excel(findings_data, \"data/processed/nnt_lrs/nnt_lrs_sans_number.xlsx\", blank_values=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for each individual webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL\n",
    "url = 'https://thennt.com/lr/lung-ultrasound-diagnosis-pneumonia-children/'\n",
    "\n",
    "# Fetch the webpage content\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    page_content = response.text\n",
    "else:\n",
    "    print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "    exit()\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(page_content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
